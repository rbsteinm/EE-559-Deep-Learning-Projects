{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor, LongTensor, Tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Superclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward(self, *input_):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Module\n",
    "\n",
    "ReLU function: \n",
    "\\begin{equation}\n",
    "f(x) = max(0, x)\n",
    "\\end{equation}\n",
    "\n",
    "the derivative of ReLU is\n",
    "\n",
    "\\begin{equation} \n",
    "f'(x)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module represents the ReLU activation function\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    # input_: the tensor outputed by the current layer\n",
    "    def forward(self, input_):\n",
    "        self.z = input_.clone()\n",
    "        input_[input_ < 0] = 0\n",
    "        return input_\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        da = gradwrtoutput\n",
    "        tensor = self.z.clone()\n",
    "        # g'(z)\n",
    "        tensor[tensor > 0] = 1\n",
    "        tensor[tensor < 0] = 0\n",
    "        # dz[l]\n",
    "        return da.mul(tensor)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):   \n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    # input_: the tensor outputed by the current layer\n",
    "    def forward(self, input_):\n",
    "        self.z = input_\n",
    "        return input_.tanh()\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        da = gradwrtoutput\n",
    "        # g'(z)\n",
    "        g_prime = (1 - self.z.tanh().pow(2))\n",
    "        # dz[l]\n",
    "        return da.mul(g_prime)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Module\n",
    "fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):   \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        # keep track of the weigths, the biases and the output of the previous layer's activation function\n",
    "        self.w = Tensor(out_dim,in_dim).normal_(0)\n",
    "        self.b = Tensor(out_dim,1).normal_(0)\n",
    "        self.x_previous_layer = None\n",
    "        # init the gradient of the loss wrt w / b\n",
    "        self.grad_w_sum = Tensor(self.w.size()).zero_()\n",
    "        self.grad_b_sum = Tensor(self.b.size()).zero_()\n",
    "    \n",
    "    # input_: the output of the previous layer's activation function\n",
    "    def forward(self, input_):\n",
    "        self.x_previous_layer = input_\n",
    "        return (self.w.mm(input_.t()) + self.b).t()\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        dz = gradwrtoutput.t()\n",
    "        dw = dz.mm(self.x_previous_layer)\n",
    "        db = dz\n",
    "        # sum the gradients for the weights and biases\n",
    "        self.grad_w_sum += dw\n",
    "        self.grad_b_sum += db.sum(1).unsqueeze(1)\n",
    "        return (self.w.t().mm(dz)).t()\n",
    "        \n",
    "    # returns a list of pairs, each composed of a parameter tensor and a gradient tensor\n",
    "    # parameters: weights and biases\n",
    "    def param(self):\n",
    "        return [ (self.w, self.grad_w_sum), (self.b, self.grad_b_sum) ]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad_w_sum.zero_()\n",
    "        self.grad_b_sum.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Module\n",
    "to combine several modules in basic sequential structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module allows to combine several modules (layers, activation functions) in a basic sequential structure\n",
    "class Sequential(Module):    \n",
    "    def __init__(self, *layers_):\n",
    "        self.modules = layers_\n",
    "        \n",
    "    # input_: the input data is a minibatch whose columns are features and lines are samples\n",
    "    def forward(self, input_):\n",
    "        x = input_\n",
    "        for module in self.modules:\n",
    "            x = module.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        x = gradwrtoutput\n",
    "        for module in reversed(self.modules):\n",
    "            x = module.backward(x)\n",
    "        return x\n",
    "        \n",
    "    # returns a flatened list of each module's parameters\n",
    "    # each parameter in the list is represented as a tuple containing the parameter tensor (e.g. w)\n",
    "    # and the gradient tensor (e.g. dl/dw)\n",
    "    def param(self):\n",
    "        return [ p for module in self.modules for p in module.param() ]\n",
    "    \n",
    "    # s,\n",
    "    def zero_grad(self):\n",
    "        for module in self.modules:\n",
    "            module.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(Module): \n",
    "    def __init__(self):\n",
    "        self.error = None\n",
    "        \n",
    "    def forward(self, preds, labels):\n",
    "        self.error = preds - labels\n",
    "        return self.error.pow(2).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        return 2 * self.error\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-entropy Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossCrossEntropy(Module): \n",
    "    def __init__(self):\n",
    "        self.p = None\n",
    "        self.c = None\n",
    "        \n",
    "    # stabilized version of softmax\n",
    "    def softmax(self, t):\n",
    "        stable_exp = (t - t.max(1)[0].unsqueeze(1)).exp()\n",
    "        #print(stable_exp)\n",
    "        return stable_exp / (stable_exp.sum(1).unsqueeze(1))\n",
    "    \n",
    "    def convert_to_one_hot(self, target):\n",
    "        tmp = FloatTensor(target.size(0), 2).fill_(0)\n",
    "        for k in range(0, target.size(0)):\n",
    "            tmp[k, train_target[k]] = 1\n",
    "        return tmp\n",
    "        \n",
    "    def forward(self, preds, labels):\n",
    "        self.p = self.softmax(preds)\n",
    "        self.c = labels\n",
    "        return -(self.c * self.p.log()).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        return(self.p-self.c)\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.11772183, -0.78549684],\n",
       "       [-0.86989726,  0.19409049],\n",
       "       [-0.89348088,  0.23706266]])"
      ]
     },
     "execution_count": 156,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "X = np.array([[0.2, 0.8], [0.3, 0.7],[0.1, 0.9]])\n",
    "y = np.array([1, 0, 0])\n",
    "\n",
    "def softmax_np(X):\n",
    "    exps = np.exp(X - np.max(X))\n",
    "    return exps / np.sum(exps)\n",
    "\n",
    "def cross_entropy(X,y):\n",
    "    \"\"\"\n",
    "    X is the output from fully connected layer (num_examples x num_classes)\n",
    "    y is labels (num_examples x 1)\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    p = softmax_np(X)\n",
    "    log_likelihood = -np.log(p[range(m),y])\n",
    "    #print(y)\n",
    "    #print(np.log(p))\n",
    "    #print(log_likelihood)\n",
    "    loss = np.sum(log_likelihood)\n",
    "    return loss\n",
    "\n",
    "#cross_entropy(X, y)\n",
    "\n",
    "def delta_cross_entropy(X,y):\n",
    "    \"\"\"\n",
    "    X is the output from fully connected layer (num_examples x num_classes)\n",
    "    y is labels (num_examples x 1)\n",
    "    \"\"\"\n",
    "    m = y.shape[0]\n",
    "    grad = softmax_np(X)\n",
    "    #print(grad)\n",
    "    #print(y)\n",
    "    #print(grad[range(m),y])\n",
    "    grad[range(m),y] -= 1\n",
    "    grad = grad\n",
    "    return grad\n",
    "#cross_entropy(X, y)\n",
    "delta_cross_entropy(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " 0  1\n",
      " 1  0\n",
      " 1  0\n",
      "[torch.FloatTensor of size 3x2]\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.1177 -0.7855\n",
       "-0.8699  0.1941\n",
       "-0.8935  0.2371\n",
       "[torch.FloatTensor of size 3x2]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t1 = FloatTensor([[0.2, 0.8], [0.3, 0.7],[0.1, 0.9]])\n",
    "t2 = FloatTensor([[0, 1], [1, 0], [1, 0]])\n",
    "test = LossCrossEntropy()\n",
    "test.forward(t1, t2)\n",
    "test.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-2.7725887298583984"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "t = Tensor(2, 2).fill_(0.5)\n",
    "-(t.exp() / t.exp().sum()).log()\n",
    "\n",
    "c = Tensor(2, 2).fill_(0.5)\n",
    "p = Tensor(2, 2).fill_(0.5)\n",
    "\n",
    "(c * p.log() + (1 - c) * (1 - p).log()).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "t2 = Tensor([[87.2280, 5.0081], \n",
    " [87.2212, 5.0043],\n",
    " [87.2355, 5.0180],\n",
    " [87.2333, 5.0175],\n",
    " [87.2464, 5.0163]]).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(t):\n",
    "    stable_exp = (t - t.max(1)[0].unsqueeze(1)).exp()\n",
    "    #print(stable_exp)\n",
    "    return stable_exp / (stable_exp.sum(1).unsqueeze(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optim_SGD(Module):\n",
    "    # parameters: the parameters of the Sequential module\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.param = parameters #[ p.shallow() for tup in parameters for p in tup ]\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    # performs a gradient step (SGD) for all parameters\n",
    "    def step(self):\n",
    "        for (p, grad_p) in self.param:\n",
    "            p.sub_(self.lr*grad_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO\n",
    "\n",
    "zero_grad() method: should we put it in the Module class, so that we can remove it from ReLU and Tanh?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    a = Tensor(nb, 2).uniform_(0, 1)\n",
    "    target = ((a.pow(2).sum(1)).sqrt() < math.sqrt(1/(2*math.pi))).long()\n",
    "    return a, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts 'target' Tensor to one hot labels\n",
    "def convert_to_one_hot(target):\n",
    "    tmp = FloatTensor(target.size(0), 2).fill_(0)\n",
    "    for k in range(0, target.size(0)):\n",
    "        tmp[k, target[k]] = 1\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, input_, target):\n",
    "    nb_data_errors = 0\n",
    "    output = model.forward(input_)\n",
    "\n",
    "    _, predicted_classes = output.max(1)\n",
    "    for k in range(input_.size(0)):\n",
    "        if target[k] != predicted_classes[k]:\n",
    "            nb_data_errors = nb_data_errors + 1\n",
    "    return 100 - (100*(nb_data_errors / input_.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_input, train_target, model, criterion, optimizer, nb_epochs=100, mini_batch_size=5, verbose=False):\n",
    "    for e in range(0, nb_epochs):\n",
    "        loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            # sum the loss for each batch to get the current epoch's loss\n",
    "            loss += criterion.forward(output, train_target.narrow(0, b, mini_batch_size))\n",
    "            # set the gradients of all layers to zero before the next batch can go through the network\n",
    "            model.zero_grad()\n",
    "            model.backward(criterion.backward())\n",
    "            optimizer.step() # performs a gradient step to optimize the parameters\n",
    "        if verbose:\n",
    "            print(\"Epoch\", e+1, \":\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training and testing data\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "# convert targets to one hot labels\n",
    "train_one_hot_target = convert_to_one_hot(train_target)\n",
    "test_one_hot_target = convert_to_one_hot(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : 3728.389531050951\n",
      "Epoch 2 : 475.303110988432\n",
      "Epoch 3 : 149.40726788389804\n",
      "Epoch 4 : 86.81020900421468\n",
      "Epoch 5 : 66.0208892639425\n",
      "Epoch 6 : 56.05730733516167\n",
      "Epoch 7 : 50.51417402525804\n",
      "Epoch 8 : 46.5406000247431\n",
      "Epoch 9 : 43.34990138436868\n",
      "Epoch 10 : 40.60817901119374\n",
      "Epoch 11 : 38.33505391183765\n",
      "Epoch 12 : 36.23055983475794\n",
      "Epoch 13 : 34.672501992010595\n",
      "Epoch 14 : 33.5911391616108\n",
      "Epoch 15 : 32.71745194949898\n",
      "Epoch 16 : 31.976181271218962\n",
      "Epoch 17 : 31.319781526540226\n",
      "Epoch 18 : 30.73460244662766\n",
      "Epoch 19 : 30.190827656998053\n",
      "Epoch 20 : 29.726908699046845\n",
      "Epoch 21 : 29.32559981207048\n",
      "Epoch 22 : 28.94649081767426\n",
      "Epoch 23 : 28.55521514381512\n",
      "Epoch 24 : 28.139718375530038\n",
      "Epoch 25 : 27.731912205304425\n",
      "Epoch 26 : 27.305642992706026\n",
      "Epoch 27 : 26.902617903024606\n",
      "Epoch 28 : 26.488906365273195\n",
      "Epoch 29 : 26.089493944448748\n",
      "Epoch 30 : 25.72099828871292\n",
      "Epoch 31 : 25.390738496052535\n",
      "Epoch 32 : 25.084070425544937\n",
      "Epoch 33 : 24.80801242524791\n",
      "Epoch 34 : 24.548359452136992\n",
      "Epoch 35 : 24.301722198136275\n",
      "Epoch 36 : 24.06153085021215\n",
      "Epoch 37 : 23.85023793089681\n",
      "Epoch 38 : 23.645191496882475\n",
      "Epoch 39 : 23.455113151318557\n",
      "Epoch 40 : 23.27264974877812\n",
      "Epoch 41 : 23.024102348410807\n",
      "Epoch 42 : 22.574506612897565\n",
      "Epoch 43 : 22.17091980408395\n",
      "Epoch 44 : 21.91505350022051\n",
      "Epoch 45 : 21.552380667559177\n",
      "Epoch 46 : 21.284876105133606\n",
      "Epoch 47 : 21.0845578775335\n",
      "Epoch 48 : 20.895612765645367\n",
      "Epoch 49 : 20.749928716687066\n",
      "Epoch 50 : 20.46928655018999\n",
      "Epoch 51 : 20.184093217488353\n",
      "Epoch 52 : 20.002052923340585\n",
      "Epoch 53 : 19.80048736009202\n",
      "Epoch 54 : 19.65048984722091\n",
      "Epoch 55 : 19.501430765607054\n",
      "Epoch 56 : 19.316825447346503\n",
      "Epoch 57 : 19.16382390437927\n",
      "Epoch 58 : 18.988981464748136\n",
      "Epoch 59 : 18.815066169341065\n",
      "Epoch 60 : 18.64216695258837\n",
      "Epoch 61 : 18.475300379940677\n",
      "Epoch 62 : 18.337248761154115\n",
      "Epoch 63 : 18.236206681909508\n",
      "Epoch 64 : 18.139184443507432\n",
      "Epoch 65 : 17.902747804590888\n",
      "Epoch 66 : 17.843201260397407\n",
      "Epoch 67 : 17.56785193362792\n",
      "Epoch 68 : 17.39209886957144\n",
      "Epoch 69 : 17.275370625064696\n",
      "Epoch 70 : 17.114703001646433\n",
      "Epoch 71 : 16.97569561816077\n",
      "Epoch 72 : 16.86120586303754\n",
      "Epoch 73 : 16.77183116711089\n",
      "Epoch 74 : 16.65739143101716\n",
      "Epoch 75 : 16.604886450482027\n",
      "Epoch 76 : 16.522890651287696\n",
      "Epoch 77 : 16.37717609095803\n",
      "Epoch 78 : 16.277302338902373\n",
      "Epoch 79 : 16.21709870014773\n",
      "Epoch 80 : 16.145503386775133\n",
      "Epoch 81 : 15.97730994530042\n",
      "Epoch 82 : 15.945277157080525\n",
      "Epoch 83 : 15.879831471183977\n",
      "Epoch 84 : 15.80019477180926\n",
      "Epoch 85 : 15.699859551054303\n",
      "Epoch 86 : 15.649244008064443\n",
      "Epoch 87 : 15.558121410944352\n",
      "Epoch 88 : 15.407687244895797\n",
      "Epoch 89 : 15.38341816353973\n",
      "Epoch 90 : 15.335866241039156\n",
      "Epoch 91 : 15.23959665772746\n",
      "Epoch 92 : 15.135272623748975\n",
      "Epoch 93 : 15.067865214951652\n",
      "Epoch 94 : 14.976115184828132\n",
      "Epoch 95 : 14.925031878609701\n",
      "Epoch 96 : 14.848623608422542\n",
      "Epoch 97 : 14.760829279194368\n",
      "Epoch 98 : 14.642349386325591\n",
      "Epoch 99 : 14.69736225302647\n",
      "Epoch 100 : 14.549594591232818\n",
      "Train accuracy : 100.0 %\n",
      "Test accuracy : 99.9 %\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 100\n",
    "mini_batch_size = 5\n",
    "model = Sequential(Linear(2,25), ReLU(), Linear(25,25), ReLU(), Linear(25, 25), ReLU(), Linear(25,2))\n",
    "#criterion = LossMSE()\n",
    "criterion = LossCrossEntropy()\n",
    "optimizer = optim_SGD(model.param(), 1e-4)\n",
    "\n",
    "train_model(train_input, train_one_hot_target, model, criterion, optimizer, nb_epochs, mini_batch_size, verbose=True)\n",
    "\n",
    "print(\"Train accuracy :\", compute_accuracy(model, train_input, train_target), \"%\")\n",
    "print(\"Test accuracy :\", compute_accuracy(model, test_input, test_target), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n",
      "Train accuracy : 99.2 %\n",
      "Test accuracy : 99.7 %\n",
      "iter 2\n",
      "Train accuracy : 99.6 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 3\n",
      "Train accuracy : 99.8 %\n",
      "Test accuracy : 99.9 %\n",
      "iter 4\n",
      "Train accuracy : 99.8 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 5\n",
      "Train accuracy : 99.7 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 6\n",
      "Train accuracy : 99.6 %\n",
      "Test accuracy : 99.6 %\n",
      "iter 7\n",
      "Train accuracy : 99.5 %\n",
      "Test accuracy : 99.9 %\n",
      "iter 8\n",
      "Train accuracy : 99.6 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 9\n",
      "Train accuracy : 99.8 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 10\n",
      "Train accuracy : 99.6 %\n",
      "Test accuracy : 99.9 %\n",
      "iter 11\n",
      "Train accuracy : 99.2 %\n",
      "Test accuracy : 99.7 %\n",
      "iter 12\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 99.8 %\n",
      "iter 13\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 99.6 %\n",
      "iter 14\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 15\n",
      "Train accuracy : 99.7 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 16\n",
      "Train accuracy : 99.3 %\n",
      "Test accuracy : 99.7 %\n",
      "iter 17\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 99.8 %\n",
      "iter 18\n",
      "Train accuracy : 99.7 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 19\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 99.8 %\n",
      "iter 20\n",
      "Train accuracy : 99.5 %\n",
      "Test accuracy : 99.9 %\n",
      "iter 21\n",
      "Train accuracy : 99.5 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 22\n",
      "Train accuracy : 99.6 %\n",
      "Test accuracy : 99.8 %\n",
      "iter 23\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 99.8 %\n",
      "iter 24\n",
      "Train accuracy : 99.3 %\n",
      "Test accuracy : 99.8 %\n",
      "iter 25\n",
      "Train accuracy : 99.3 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 26\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 99.7 %\n",
      "iter 27\n",
      "Train accuracy : 99.6 %\n",
      "Test accuracy : 99.8 %\n",
      "iter 28\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 99.6 %\n",
      "iter 29\n",
      "Train accuracy : 99.3 %\n",
      "Test accuracy : 99.9 %\n",
      "iter 30\n",
      "Train accuracy : 99.2 %\n",
      "Test accuracy : 99.8 %\n",
      "iter 31\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 32\n",
      "Train accuracy : 99.7 %\n",
      "Test accuracy : 99.8 %\n",
      "iter 33\n",
      "Train accuracy : 99.6 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 34\n",
      "Train accuracy : 99.7 %\n",
      "Test accuracy : 99.8 %\n",
      "iter 35\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 99.9 %\n",
      "iter 36\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 99.9 %\n",
      "iter 37\n",
      "Train accuracy : 99.5 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 38\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 99.8 %\n",
      "iter 39\n",
      "Train accuracy : 99.7 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 40\n",
      "Train accuracy : 99.6 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 41\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 99.9 %\n",
      "iter 42\n",
      "Train accuracy : 99.5 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 43\n",
      "Train accuracy : 99.8 %\n",
      "Test accuracy : 99.9 %\n",
      "iter 44\n",
      "Train accuracy : 99.6 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 45\n",
      "Train accuracy : 99.7 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 46\n",
      "Train accuracy : 99.5 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 47\n",
      "Train accuracy : 99.4 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 48\n",
      "Train accuracy : 99.3 %\n",
      "Test accuracy : 99.9 %\n",
      "iter 49\n",
      "Train accuracy : 99.5 %\n",
      "Test accuracy : 100.0 %\n",
      "iter 50\n",
      "Train accuracy : 99.7 %\n",
      "Test accuracy : 99.8 %\n"
     ]
    }
   ],
   "source": [
    "n_iters = 50\n",
    "test_acc=[]\n",
    "train_acc=[]\n",
    "for i in range(n_iters):\n",
    "    print('iter', i+1)\n",
    "    model = Sequential(Linear(2,25), Tanh(), Linear(25,25), Tanh(), Linear(25, 25), Tanh(), Linear(25,2))\n",
    "    #criterion = LossMSE()\n",
    "    criterion = LossCrossEntropy()\n",
    "    optimizer = optim_SGD(model.param(), 1e-3)\n",
    "    \n",
    "    train_model(train_input, train_one_hot_target, model, criterion, optimizer, nb_epochs, mini_batch_size)\n",
    "\n",
    "    train_a = compute_accuracy(model, train_input, train_target)\n",
    "    test_a = compute_accuracy(model, test_input, test_target)\n",
    "    print(\"Train accuracy :\", train_a, \"%\")\n",
    "    print(\"Test accuracy :\", test_a, \"%\")\n",
    "    train_acc.append(train_a)\n",
    "    test_acc.append(test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train accuracy 99.50800000000001\n",
      "test accuracy 99.88400000000003\n"
     ]
    }
   ],
   "source": [
    "print('train accuracy', sum(train_acc)/len(train_acc))\n",
    "print('test accuracy', sum(test_acc)/len(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
