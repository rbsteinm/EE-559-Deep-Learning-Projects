{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor, LongTensor, Tensor\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Module Superclass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    def forward(self, *input_):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Module\n",
    "\n",
    "ReLU function: \n",
    "\\begin{equation}\n",
    "f(x) = max(0, x)\n",
    "\\end{equation}\n",
    "\n",
    "the derivative of ReLU is\n",
    "\n",
    "\\begin{equation} \n",
    "f'(x)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module represents the ReLU activation function\n",
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    # input_: the tensor outputed by the current layer\n",
    "    def forward(self, input_):\n",
    "        self.z = input_.clone()\n",
    "        input_[input_ < 0] = 0\n",
    "        return input_\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        da = gradwrtoutput\n",
    "        tensor = self.z.clone()\n",
    "        # g'(z)\n",
    "        tensor[tensor > 0] = 1\n",
    "        tensor[tensor < 0] = 0\n",
    "        # dz[l]\n",
    "        return da.mul(tensor)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Tanh(Module):   \n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    # input_: the tensor outputed by the current layer\n",
    "    def forward(self, input_):\n",
    "        self.z = input_\n",
    "        return input_.tanh()\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        da = gradwrtoutput\n",
    "        # g'(z)\n",
    "        g_prime = (1 - self.z.tanh().pow(2))\n",
    "        # dz[l]\n",
    "        return da.mul(g_prime)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Module\n",
    "fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Module):   \n",
    "    def __init__(self, in_dim, out_dim):\n",
    "        # keep track of the weigths, the biases and the output of the previous layer's activation function\n",
    "        self.w = Tensor(out_dim,in_dim).normal_(0)\n",
    "        self.b = Tensor(out_dim,1).normal_(0)\n",
    "        self.x_previous_layer = None\n",
    "        # init the gradient of the loss wrt w / b\n",
    "        self.grad_w_sum = Tensor(self.w.size()).zero_()\n",
    "        self.grad_b_sum = Tensor(self.b.size()).zero_()\n",
    "    \n",
    "    # input_: the output of the previous layer's activation function\n",
    "    def forward(self, input_):\n",
    "        self.x_previous_layer = input_\n",
    "        return (self.w.mm(input_.t()) + self.b).t()\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        dz = gradwrtoutput.t()\n",
    "        dw = dz.mm(self.x_previous_layer)\n",
    "        db = dz\n",
    "        # sum the gradients for the weights and biases\n",
    "        self.grad_w_sum += dw\n",
    "        self.grad_b_sum += db.sum(1).unsqueeze(1)\n",
    "        return (self.w.t().mm(dz)).t()\n",
    "        \n",
    "    # returns a list of pairs, each composed of a parameter tensor and a gradient tensor\n",
    "    # parameters: weights and biases\n",
    "    def param(self):\n",
    "        return [ (self.w, self.grad_w_sum), (self.b, self.grad_b_sum) ]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad_w_sum.zero_()\n",
    "        self.grad_b_sum.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Module\n",
    "to combine several modules in basic sequential structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This module allows to combine several modules (layers, activation functions) in a basic sequential structure\n",
    "class Sequential(Module):    \n",
    "    def __init__(self, *layers_):\n",
    "        self.modules = layers_\n",
    "        \n",
    "    # input_: the input data is a minibatch whose columns are features and lines are samples\n",
    "    def forward(self, input_):\n",
    "        x = input_\n",
    "        for module in self.modules:\n",
    "            x = module.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        x = gradwrtoutput\n",
    "        for module in reversed(self.modules):\n",
    "            x = module.backward(x)\n",
    "        return x\n",
    "        \n",
    "    # returns a flatened list of each module's parameters\n",
    "    # each parameter in the list is represented as a tuple containing the parameter tensor (e.g. w)\n",
    "    # and the gradient tensor (e.g. dl/dw)\n",
    "    def param(self):\n",
    "        return [ p for module in self.modules for p in module.param() ]\n",
    "    \n",
    "    # s,\n",
    "    def zero_grad(self):\n",
    "        for module in self.modules:\n",
    "            module.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LossMSE(Module): \n",
    "    def __init__(self):\n",
    "        self.error = None\n",
    "        \n",
    "    def forward(self, preds, labels):\n",
    "        self.error = preds - labels\n",
    "        return self.error.pow(2).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        return 2 * self.error\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class optim_SGD(Module):\n",
    "    # parameters: the parameters of the Sequential module\n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.param = parameters #[ p.shallow() for tup in parameters for p in tup ]\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    # performs a gradient step (SGD) for all parameters\n",
    "    def step(self):\n",
    "        for (p, grad_p) in self.param:\n",
    "            p.sub_(self.lr*grad_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Helpers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    a = Tensor(nb, 2).uniform_(0, 1)\n",
    "    target = ((a.pow(2).sum(1)).sqrt() < math.sqrt(1/(2*math.pi))).long()\n",
    "    return a, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converts 'target' Tensor to one hot labels\n",
    "def convert_to_one_hot(target):\n",
    "    tmp = FloatTensor(target.size(0), 2).fill_(0)\n",
    "    for k in range(0, target.size(0)):\n",
    "        tmp[k, train_target[k]] = 1\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(model, input_, target):\n",
    "    nb_data_errors = 0\n",
    "    output = model.forward(input_)\n",
    "\n",
    "    _, predicted_classes = output.max(1)\n",
    "    for k in range(input_.size(0)):\n",
    "        if target[k] != predicted_classes[k]:\n",
    "            nb_data_errors = nb_data_errors + 1\n",
    "    return 100 - (100*(nb_data_errors / input_.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def train_model(train_input, train_one_hot_target, model, criterion, optimizer, nb_epochs=100, mini_batch_size=5, verbose=False):\n",
    "    for e in range(0, nb_epochs):\n",
    "        loss = 0\n",
    "        for b in range(0, train_input.size(0), mini_batch_size):\n",
    "            output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "            # sum the loss for each batch to get the current epoch's loss\n",
    "            loss += criterion.forward(output, train_one_hot_target.narrow(0, b, mini_batch_size))\n",
    "            # set the gradients of all layers to zero before the next batch can go through the network\n",
    "            model.zero_grad()\n",
    "            model.backward(criterion.backward())\n",
    "            optimizer.step() # performs a gradient step to optimize the parameters\n",
    "        if verbose:\n",
    "            print(\"Epoch\", e+1, \":\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate training and testing data\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)\n",
    "\n",
    "# convert targets to one hot labels\n",
    "train_one_hot_target = convert_to_one_hot(train_target)\n",
    "test_one_hot_target = convert_to_one_hot(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : 832.9720829972371\n",
      "Epoch 2 : 154.90995721241262\n",
      "Epoch 3 : 129.86513044108133\n",
      "Epoch 4 : 110.14496682553451\n",
      "Epoch 5 : 96.83185583888873\n",
      "Epoch 6 : 88.59321982804663\n",
      "Epoch 7 : 82.43121548430342\n",
      "Epoch 8 : 77.32139528462334\n",
      "Epoch 9 : 72.71635719806248\n",
      "Epoch 10 : 68.59061387278207\n",
      "Epoch 11 : 64.51636541251641\n",
      "Epoch 12 : 60.313300845997006\n",
      "Epoch 13 : 56.39257784141226\n",
      "Epoch 14 : 53.68016271212343\n",
      "Epoch 15 : 50.6784058541575\n",
      "Epoch 16 : 48.16249963307973\n",
      "Epoch 17 : 46.09611943953415\n",
      "Epoch 18 : 44.31350497681193\n",
      "Epoch 19 : 42.750062040166156\n",
      "Epoch 20 : 41.37042179608008\n",
      "Epoch 21 : 40.14095954670577\n",
      "Epoch 22 : 39.036341381068965\n",
      "Epoch 23 : 38.04109333754259\n",
      "Epoch 24 : 37.1443383343493\n",
      "Epoch 25 : 36.335787241992634\n",
      "Epoch 26 : 35.60435188836871\n",
      "Epoch 27 : 34.93895581631742\n",
      "Epoch 28 : 34.329268978727555\n",
      "Epoch 29 : 33.766434263849646\n",
      "Epoch 30 : 33.243015057384156\n",
      "Epoch 31 : 32.75315915758428\n",
      "Epoch 32 : 32.29219931610962\n",
      "Epoch 33 : 31.856454960266916\n",
      "Epoch 34 : 31.442975512965376\n",
      "Epoch 35 : 31.049298258628028\n",
      "Epoch 36 : 30.673198348436742\n",
      "Epoch 37 : 30.31271263567386\n",
      "Epoch 38 : 29.965901529554856\n",
      "Epoch 39 : 29.631062757086752\n",
      "Epoch 40 : 29.30657955992551\n",
      "Epoch 41 : 28.991055825813504\n",
      "Epoch 42 : 28.683314457531367\n",
      "Epoch 43 : 28.382338948684563\n",
      "Epoch 44 : 28.087392294746294\n",
      "Epoch 45 : 27.797796437343123\n",
      "Epoch 46 : 27.513045439027977\n",
      "Epoch 47 : 27.23279081990013\n",
      "Epoch 48 : 26.956690148668216\n",
      "Epoch 49 : 26.684543499820094\n",
      "Epoch 50 : 26.416152355126588\n",
      "Epoch 51 : 26.151409889799936\n",
      "Epoch 52 : 25.890279126578406\n",
      "Epoch 53 : 25.632713836341367\n",
      "Epoch 54 : 25.378709925546403\n",
      "Epoch 55 : 25.12837681759909\n",
      "Epoch 56 : 24.881721014209376\n",
      "Epoch 57 : 24.638930654211137\n",
      "Epoch 58 : 24.400075039325287\n",
      "Epoch 59 : 24.165354732375505\n",
      "Epoch 60 : 23.934947942728144\n",
      "Epoch 61 : 23.708964538752156\n",
      "Epoch 62 : 23.487606674192577\n",
      "Epoch 63 : 23.271033662831456\n",
      "Epoch 64 : 23.059416498413853\n",
      "Epoch 65 : 22.852872352642095\n",
      "Epoch 66 : 22.651549857671895\n",
      "Epoch 67 : 22.455520998068224\n",
      "Epoch 68 : 22.264913498084823\n",
      "Epoch 69 : 22.079740186411357\n",
      "Epoch 70 : 21.900030175352185\n",
      "Epoch 71 : 21.72581021513369\n",
      "Epoch 72 : 21.557074525440953\n",
      "Epoch 73 : 21.393716677398828\n",
      "Epoch 74 : 21.235703324674333\n",
      "Epoch 75 : 21.08292489191865\n",
      "Epoch 76 : 20.93526907307303\n",
      "Epoch 77 : 20.792632249625285\n",
      "Epoch 78 : 20.654821684696387\n",
      "Epoch 79 : 20.52172649905144\n",
      "Epoch 80 : 20.393177136839498\n",
      "Epoch 81 : 20.268982001642996\n",
      "Epoch 82 : 20.148980565643498\n",
      "Epoch 83 : 20.03299621499706\n",
      "Epoch 84 : 19.920858098422485\n",
      "Epoch 85 : 19.812408757483098\n",
      "Epoch 86 : 19.70743721779344\n",
      "Epoch 87 : 19.605824444852573\n",
      "Epoch 88 : 19.507386148759792\n",
      "Epoch 89 : 19.411978018487844\n",
      "Epoch 90 : 19.31945035445665\n",
      "Epoch 91 : 19.22965429981457\n",
      "Epoch 92 : 19.142460186916367\n",
      "Epoch 93 : 19.057729790849677\n",
      "Epoch 94 : 18.975352617910474\n",
      "Epoch 95 : 18.895203467628335\n",
      "Epoch 96 : 18.817172522204093\n",
      "Epoch 97 : 18.741151410920825\n",
      "Epoch 98 : 18.6670799411999\n",
      "Epoch 99 : 18.594801501148027\n",
      "Epoch 100 : 18.52428424374314\n",
      "Train accuracy : 99.1 %\n",
      "Test accuracy : 98.8 %\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 100\n",
    "mini_batch_size = 5\n",
    "model = Sequential(Linear(2,25), Tanh(), Linear(25,25), Tanh(), Linear(25, 25), Tanh(), Linear(25,2))\n",
    "criterion = LossMSE()\n",
    "optimizer = optim_SGD(model.param(), 1e-3)\n",
    "\n",
    "train_model(train_input, train_one_hot_target, model, criterion, optimizer, nb_epochs, mini_batch_size, verbose=True)\n",
    "\n",
    "print(\"Train accuracy :\", compute_accuracy(model, train_input, train_target), \"%\")\n",
    "print(\"Test accuracy :\", compute_accuracy(model, test_input, test_target), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iter 1\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "sub received an invalid combination of arguments - got (torch.LongTensor), but expected one of:\n * (float value)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n * (torch.FloatTensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n * (float value, torch.FloatTensor other)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-41-b9e5a0b0bc86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim_SGD\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1e-3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[0mtrain_a\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompute_accuracy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-37-6ab593426a63>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(train_input, train_one_hot_target, model, criterion, optimizer, nb_epochs, mini_batch_size, verbose)\u001b[0m\n\u001b[1;32m      5\u001b[0m             \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0;31m# sum the loss for each batch to get the current epoch's loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_one_hot_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0;31m# set the gradients of all layers to zero before the next batch can go through the network\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-66fb7a6f1394>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, preds, labels)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/tensor.py\u001b[0m in \u001b[0;36m__sub__\u001b[0;34m(self, other)\u001b[0m\n\u001b[1;32m    310\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__sub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 312\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    314\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__rsub__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mother\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: sub received an invalid combination of arguments - got (torch.LongTensor), but expected one of:\n * (float value)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n * (torch.FloatTensor other)\n      didn't match because some of the arguments have invalid types: (\u001b[31;1mtorch.LongTensor\u001b[0m)\n * (float value, torch.FloatTensor other)\n"
     ]
    }
   ],
   "source": [
    "n_iters = 50\n",
    "test_acc=[]\n",
    "train_acc=[]\n",
    "for i in range(n_iters):\n",
    "    print('iter', i+1)\n",
    "    model = Sequential(Linear(2,25), Tanh(), Linear(25,25), Tanh(), Linear(25, 25), Tanh(), Linear(25,2))\n",
    "    criterion = LossMSE()\n",
    "    optimizer = optim_SGD(model.param(), 1e-3)\n",
    "    \n",
    "    train_model(train_input, train_one_hot_target, model, criterion, optimizer, nb_epochs, mini_batch_size)\n",
    "\n",
    "    train_a = compute_accuracy(model, train_input, train_target)\n",
    "    test_a = compute_accuracy(model, test_input, test_target)\n",
    "    print(\"Train accuracy :\", train_a, \"%\")\n",
    "    print(\"Test accuracy :\", test_a, \"%\")\n",
    "    train_acc.append(train_a)\n",
    "    test_acc.append(test_a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('train accuracy', sum(train_acc)/len(train_acc))\n",
    "print('test accuracy', sum(test_acc)/len(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
