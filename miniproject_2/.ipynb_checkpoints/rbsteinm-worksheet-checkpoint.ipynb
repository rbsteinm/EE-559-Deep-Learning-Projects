{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor, LongTensor, Tensor\n",
    "import math\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "    def forward(self, *input_):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Module\n",
    "\n",
    "ReLU function: \n",
    "\\begin{equation}\n",
    "f(x) = max(0, x)\n",
    "\\end{equation}\n",
    "\n",
    "the derivative of ReLU is\n",
    "\n",
    "\\begin{equation} \n",
    "f'(x)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.z = input_.clone()\n",
    "        input_[input_ < 0] = 0\n",
    "        return input_\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        da = gradwrtoutput\n",
    "        tensor = self.z.clone()\n",
    "        # g'(z)\n",
    "        tensor[tensor > 0] = 1\n",
    "        tensor[tensor < 0] = 0\n",
    "        # dz[l]\n",
    "        return da.mul(tensor)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):   \n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.z = input_\n",
    "        return input_.tanh()\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        da = gradwrtoutput\n",
    "        # g'(z)\n",
    "        g_prime = (1 - self.z.tanh().pow(2))\n",
    "        # dz[l]\n",
    "        return da.mul(g_prime)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Module\n",
    "fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 479,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Module):   \n",
    "    def __init__(self, in_dim, out_dim):        \n",
    "        self.w = Tensor(out_dim,in_dim).normal_(0)\n",
    "        self.b = Tensor(out_dim,1).normal_(0)\n",
    "        self.x_previous_layer = None\n",
    "        # sum the gradient wrt w / b for each batch in these variables\n",
    "        self.grad_w_sum = Tensor(self.w.size()).zero_()\n",
    "        self.grad_b_sum = Tensor(self.b.size()).zero_()\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.x_previous_layer = input_\n",
    "        return (self.w.mm(input_.t()) + self.b).t()\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        dz = gradwrtoutput.t()\n",
    "        dw = dz.mm(self.x_previous_layer)\n",
    "        db = dz\n",
    "        # sum the gradients for the weights and biases\n",
    "        print(self.grad_w_sum.size())\n",
    "        print(dw.size())\n",
    "        print('-'*20)\n",
    "        print(self.grad_b_sum.size())\n",
    "        print(db.sum(1).unsqueeze(1).size())\n",
    "        self.grad_w_sum += dw\n",
    "        self.grad_b_sum += db.sum(1)\n",
    "        return (self.w.t().mm(dz)).t()\n",
    "        \n",
    "    def param(self):\n",
    "        return [ (self.w, self.grad_w_sum), (self.b, self.grad_b_sum) ]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad_w_sum.zero_()\n",
    "        self.grad_b_sum.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Module\n",
    "to combine several modules in basic sequential structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):    \n",
    "    def __init__(self, *layers_):\n",
    "        self.modules = layers_\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        x = input_\n",
    "        for module in self.modules:\n",
    "            x = module.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        x = gradwrtoutput\n",
    "        for module in reversed(self.modules):\n",
    "            x = module.backward(x)\n",
    "        return x\n",
    "        \n",
    "    def param(self):\n",
    "        return [ p for module in self.modules for p in module.param() ]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for module in self.modules:\n",
    "            module.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossMSE(Module): \n",
    "    def __init__(self):\n",
    "        self.error = None\n",
    "        \n",
    "    def forward(self, preds, labels):\n",
    "        self.error = preds - labels\n",
    "        return self.error.pow(2).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        return 2 * self.error\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class optim_SGD(Module): \n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.param = parameters #[ p.shallow() for tup in parameters for p in tup ]\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    def step(self):\n",
    "        for (p, grad_p) in self.param:\n",
    "            p.sub_(self.lr*grad_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  5.9373   7.2770\n",
       " -7.0436   2.5153\n",
       "  7.2012   3.9569\n",
       " 18.7668  -2.0585\n",
       "  3.5095   5.1504\n",
       "[torch.FloatTensor of size 5x2]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = FloatTensor(5, 2)\n",
    "a.normal_(0, 5)\n",
    "b = FloatTensor(5, 2)\n",
    "b.normal_(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -9.4639 -11.7929\n",
      " -3.3858 -14.1017\n",
      "  3.7349   1.5512\n",
      "  4.7306  -0.6805\n",
      "  1.5711  -0.4694\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n",
      "\n",
      "-3.9147  5.9485\n",
      "-4.1990 -1.6456\n",
      " 8.7344 -7.4869\n",
      " 2.4498 -8.1163\n",
      "-0.4472  5.3993\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  2.4001\n",
       " -7.2299\n",
       "  5.4902\n",
       "  6.9134\n",
       " 10.6199\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260.3018182516098"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = FloatTensor(5, 4).normal_(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 480,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10])\n",
      "torch.Size([4, 10])\n",
      "--------------------\n",
      "torch.Size([4, 1])\n",
      "torch.Size([4, 1])\n",
      "torch.Size([10, 2])\n",
      "torch.Size([10, 2])\n",
      "--------------------\n",
      "torch.Size([10, 1])\n",
      "torch.Size([10, 1])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/site-packages/torch/tensor.py:309: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return self.add_(other)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\n",
       "1.00000e+05 *\n",
       " -0.0107  0.0179\n",
       " -4.0004 -0.3849\n",
       "  0.0003  0.0006\n",
       "  0.0013  0.0031\n",
       "  0.0021  0.0051\n",
       "[torch.FloatTensor of size 5x2]"
      ]
     },
     "execution_count": 480,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model = Sequential(Linear(2,10), ReLU(), Linear(10,4), ReLU())\n",
    "\n",
    "our_loss = LossMSE()\n",
    "\n",
    "our_optim = optim_SGD(our_model.param(), 0.01)\n",
    "\n",
    "output = our_model.forward(a)\n",
    "\n",
    "our_loss.forward(output, target)\n",
    "\n",
    "our_model.backward(our_loss.backward())\n",
    "\n",
    "#our_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 461,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1.0126  4.5310\n",
       "-7.2013  4.0270\n",
       " 3.9680  1.7036\n",
       " 1.6381 -7.2932\n",
       "-0.5733  1.6145\n",
       " 3.9516  5.6916\n",
       " 2.2626 -1.8835\n",
       " 7.8173 -0.8127\n",
       "-4.2478  7.7019\n",
       "-1.4106 -3.9694\n",
       "[torch.FloatTensor of size 10x2]"
      ]
     },
     "execution_count": 461,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model.param()[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "our_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 394,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "    0     0\n",
       "[torch.FloatTensor of size 10x2]"
      ]
     },
     "execution_count": 394,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model.param()[0][0].zero_()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 378,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\n",
      " 0.1647  1.5122\n",
      " 0.5307  0.1592\n",
      " 1.9198 -2.5079\n",
      "-1.2089  0.4031\n",
      "-1.0443 -0.5580\n",
      " 0.6487 -0.1277\n",
      " 0.0070 -0.1935\n",
      " 0.1507 -0.2277\n",
      " 0.7951 -1.3963\n",
      " 1.2994  0.1784\n",
      "[torch.FloatTensor of size 10x2]\n",
      ", \n",
      "  731.1170   153.0622\n",
      " 4487.7734  -821.4435\n",
      "  631.4024  -193.0466\n",
      "  182.6864    30.8173\n",
      " -487.6267   -82.2576\n",
      " 1381.4794    24.7660\n",
      "-1999.9968   783.3753\n",
      " -713.5247   -88.8846\n",
      " 2720.7378  -558.8494\n",
      " 2034.0952  -178.8320\n",
      "[torch.FloatTensor of size 10x2]\n",
      "), (\n",
      "-0.9272\n",
      "-1.3761\n",
      "-0.7448\n",
      "-0.2707\n",
      " 0.1445\n",
      "-0.6812\n",
      " 0.2777\n",
      " 1.0100\n",
      "-0.8755\n",
      " 0.7084\n",
      "[torch.FloatTensor of size 10x1]\n",
      ", \n",
      "  84.6115\n",
      " 506.7211\n",
      "  68.2542\n",
      " -29.5306\n",
      "  78.8230\n",
      " 166.8729\n",
      "-217.7150\n",
      "-121.7368\n",
      " 304.8163\n",
      " 239.9527\n",
      "[torch.FloatTensor of size 10]\n",
      "), (\n",
      "-1.5164  1.0109 -1.3242 -1.4095 -0.7296  0.1188  0.0954  0.6558  2.2024  0.1759\n",
      " 1.1016  2.6712  0.4880  1.3274 -1.2304  0.4205 -1.8041 -0.1396  1.7075  0.9373\n",
      " 0.0361 -0.3390 -0.6613 -0.8050  1.3170  2.0945  1.6856 -1.7166 -0.6874  1.3346\n",
      "-1.8927  0.6111 -0.7577 -0.4711  1.6690  0.2081 -0.8378  0.4869  1.2908 -1.7653\n",
      "[torch.FloatTensor of size 4x10]\n",
      ", \n",
      "\n",
      "Columns 0 to 6 \n",
      "    0.0000     0.0000     0.0000     0.0000     0.0000     0.0000     0.0000\n",
      "  243.7387   597.1524  3905.2263     0.0000     0.0000  1021.1020   125.5413\n",
      "  135.6838   122.4091   393.2934   114.0565   120.7831   167.0964     7.6863\n",
      "    0.0000     0.0000     0.0000   230.5369   244.1330     0.0000    14.8299\n",
      "\n",
      "Columns 7 to 9 \n",
      "    0.0000     0.0000     0.0000\n",
      "  517.8618  1613.6556  2303.4727\n",
      "   83.3908   114.2427   466.0905\n",
      "   10.7128     0.0000     0.0000\n",
      "[torch.FloatTensor of size 4x10]\n",
      "), (\n",
      " 0.8489\n",
      "-0.5453\n",
      " 0.2424\n",
      " 0.7869\n",
      "[torch.FloatTensor of size 4x1]\n",
      ", \n",
      "   0.0000\n",
      " 186.8765\n",
      "  65.3517\n",
      "  33.9665\n",
      "[torch.FloatTensor of size 4]\n",
      ")]\n",
      "==================================================\n",
      "[(\n",
      " 0.1647  1.5122\n",
      " 0.5307  0.1592\n",
      " 1.9198 -2.5079\n",
      "-1.2089  0.4031\n",
      "-1.0443 -0.5580\n",
      " 0.6487 -0.1277\n",
      " 0.0070 -0.1935\n",
      " 0.1507 -0.2277\n",
      " 0.7951 -1.3963\n",
      " 1.2994  0.1784\n",
      "[torch.FloatTensor of size 10x2]\n",
      ", \n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "[torch.FloatTensor of size 10x2]\n",
      "), (\n",
      "-0.9272\n",
      "-1.3761\n",
      "-0.7448\n",
      "-0.2707\n",
      " 0.1445\n",
      "-0.6812\n",
      " 0.2777\n",
      " 1.0100\n",
      "-0.8755\n",
      " 0.7084\n",
      "[torch.FloatTensor of size 10x1]\n",
      ", \n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "), (\n",
      "-1.5164  1.0109 -1.3242 -1.4095 -0.7296  0.1188  0.0954  0.6558  2.2024  0.1759\n",
      " 1.1016  2.6712  0.4880  1.3274 -1.2304  0.4205 -1.8041 -0.1396  1.7075  0.9373\n",
      " 0.0361 -0.3390 -0.6613 -0.8050  1.3170  2.0945  1.6856 -1.7166 -0.6874  1.3346\n",
      "-1.8927  0.6111 -0.7577 -0.4711  1.6690  0.2081 -0.8378  0.4869  1.2908 -1.7653\n",
      "[torch.FloatTensor of size 4x10]\n",
      ", \n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 4x10]\n",
      "), (\n",
      " 0.8489\n",
      "-0.5453\n",
      " 0.2424\n",
      " 0.7869\n",
      "[torch.FloatTensor of size 4x1]\n",
      ", \n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 4x1]\n",
      ")]\n",
      "[(\n",
      " 0.1647  1.5122\n",
      " 0.5307  0.1592\n",
      " 1.9198 -2.5079\n",
      "-1.2089  0.4031\n",
      "-1.0443 -0.5580\n",
      " 0.6487 -0.1277\n",
      " 0.0070 -0.1935\n",
      " 0.1507 -0.2277\n",
      " 0.7951 -1.3963\n",
      " 1.2994  0.1784\n",
      "[torch.FloatTensor of size 10x2]\n",
      ", \n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "[torch.FloatTensor of size 10x2]\n",
      "), (\n",
      "-0.9272\n",
      "-1.3761\n",
      "-0.7448\n",
      "-0.2707\n",
      " 0.1445\n",
      "-0.6812\n",
      " 0.2777\n",
      " 1.0100\n",
      "-0.8755\n",
      " 0.7084\n",
      "[torch.FloatTensor of size 10x1]\n",
      ", \n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "), (\n",
      "-1.5164  1.0109 -1.3242 -1.4095 -0.7296  0.1188  0.0954  0.6558  2.2024  0.1759\n",
      " 1.1016  2.6712  0.4880  1.3274 -1.2304  0.4205 -1.8041 -0.1396  1.7075  0.9373\n",
      " 0.0361 -0.3390 -0.6613 -0.8050  1.3170  2.0945  1.6856 -1.7166 -0.6874  1.3346\n",
      "-1.8927  0.6111 -0.7577 -0.4711  1.6690  0.2081 -0.8378  0.4869  1.2908 -1.7653\n",
      "[torch.FloatTensor of size 4x10]\n",
      ", \n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 4x10]\n",
      "), (\n",
      " 0.8489\n",
      "-0.5453\n",
      " 0.2424\n",
      " 0.7869\n",
      "[torch.FloatTensor of size 4x1]\n",
      ", \n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 4x1]\n",
      ")]\n",
      "==================================================\n",
      "[(\n",
      " 0.1647  1.5122\n",
      " 0.5307  0.1592\n",
      " 1.9198 -2.5079\n",
      "-1.2089  0.4031\n",
      "-1.0443 -0.5580\n",
      " 0.6487 -0.1277\n",
      " 0.0070 -0.1935\n",
      " 0.1507 -0.2277\n",
      " 0.7951 -1.3963\n",
      " 1.2994  0.1784\n",
      "[torch.FloatTensor of size 10x2]\n",
      ", \n",
      "  731.1170   153.0622\n",
      " 4487.7734  -821.4435\n",
      "  631.4024  -193.0466\n",
      "  182.6864    30.8173\n",
      " -487.6267   -82.2576\n",
      " 1381.4794    24.7660\n",
      "-1999.9968   783.3753\n",
      " -713.5247   -88.8846\n",
      " 2720.7378  -558.8494\n",
      " 2034.0952  -178.8320\n",
      "[torch.FloatTensor of size 10x2]\n",
      "), (\n",
      "-0.9272\n",
      "-1.3761\n",
      "-0.7448\n",
      "-0.2707\n",
      " 0.1445\n",
      "-0.6812\n",
      " 0.2777\n",
      " 1.0100\n",
      "-0.8755\n",
      " 0.7084\n",
      "[torch.FloatTensor of size 10x1]\n",
      ", \n",
      "  84.6115\n",
      " 506.7211\n",
      "  68.2542\n",
      " -29.5306\n",
      "  78.8230\n",
      " 166.8729\n",
      "-217.7150\n",
      "-121.7368\n",
      " 304.8163\n",
      " 239.9527\n",
      "[torch.FloatTensor of size 10]\n",
      "), (\n",
      "-1.5164  1.0109 -1.3242 -1.4095 -0.7296  0.1188  0.0954  0.6558  2.2024  0.1759\n",
      " 1.1016  2.6712  0.4880  1.3274 -1.2304  0.4205 -1.8041 -0.1396  1.7075  0.9373\n",
      " 0.0361 -0.3390 -0.6613 -0.8050  1.3170  2.0945  1.6856 -1.7166 -0.6874  1.3346\n",
      "-1.8927  0.6111 -0.7577 -0.4711  1.6690  0.2081 -0.8378  0.4869  1.2908 -1.7653\n",
      "[torch.FloatTensor of size 4x10]\n",
      ", \n",
      "\n",
      "Columns 0 to 6 \n",
      "    0.0000     0.0000     0.0000     0.0000     0.0000     0.0000     0.0000\n",
      "  243.7387   597.1524  3905.2263     0.0000     0.0000  1021.1020   125.5413\n",
      "  135.6838   122.4091   393.2934   114.0565   120.7831   167.0964     7.6863\n",
      "    0.0000     0.0000     0.0000   230.5369   244.1330     0.0000    14.8299\n",
      "\n",
      "Columns 7 to 9 \n",
      "    0.0000     0.0000     0.0000\n",
      "  517.8618  1613.6556  2303.4727\n",
      "   83.3908   114.2427   466.0905\n",
      "   10.7128     0.0000     0.0000\n",
      "[torch.FloatTensor of size 4x10]\n",
      "), (\n",
      " 0.8489\n",
      "-0.5453\n",
      " 0.2424\n",
      " 0.7869\n",
      "[torch.FloatTensor of size 4x1]\n",
      ", \n",
      "   0.0000\n",
      " 186.8765\n",
      "  65.3517\n",
      "  33.9665\n",
      "[torch.FloatTensor of size 4]\n",
      ")]\n"
     ]
    }
   ],
   "source": [
    "print(our_model.param())\n",
    "print(\"=\"*50)\n",
    "our_optim.step()\n",
    "print(\"=\"*50)\n",
    "print(our_model.param())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    a = Tensor(nb, 2).uniform_(0, 1)\n",
    "    target = (a.pow(2).sum(1) < (2/math.pi)).long()\n",
    "    return a, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(target):\n",
    "    tmp = FloatTensor(target.size(0), 2).fill_(0)\n",
    "    for k in range(0, target.size(0)):\n",
    "        tmp[k, train_target[k]] = 1\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_hot_target = convert_to_one_hot(train_target)\n",
    "test_one_hot_target = convert_to_one_hot(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 466,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/usr/local/lib/python3.6/site-packages/torch/tensor.py:309: UserWarning: other is not broadcastable to self, but they have the same number of elements.  Falling back to deprecated pointwise behavior.\n",
      "  return self.add_(other)\n",
      " 90%|█████████ | 45/50 [00:00<00:00, 222.15it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : 2990.264766845929\n",
      "Epoch 2 : 2963.9999977350235\n",
      "Epoch 3 : 2963.9999977350235\n",
      "Epoch 4 : 2963.9999977350235\n",
      "Epoch 5 : 2963.9999977350235\n",
      "Epoch 6 : 2963.9999977350235\n",
      "Epoch 7 : 2963.9999977350235\n",
      "Epoch 8 : 2963.9999977350235\n",
      "Epoch 9 : 2963.9999977350235\n",
      "Epoch 10 : 2963.9999977350235\n",
      "Epoch 11 : 2963.9999977350235\n",
      "Epoch 12 : 2963.9999977350235\n",
      "Epoch 13 : 2963.9999977350235\n",
      "Epoch 14 : 2963.9999977350235\n",
      "Epoch 15 : 2963.9999977350235\n",
      "Epoch 16 : 2963.9999977350235\n",
      "Epoch 17 : 2963.9999977350235\n",
      "Epoch 18 : 2963.9999977350235\n",
      "Epoch 19 : 2963.9999977350235\n",
      "Epoch 20 : 2963.9999977350235\n",
      "Epoch 21 : 2963.9999977350235\n",
      "Epoch 22 : 2963.9999977350235\n",
      "Epoch 23 : 2963.9999977350235\n",
      "Epoch 24 : 2963.9999977350235\n",
      "Epoch 25 : 2963.9999977350235\n",
      "Epoch 26 : 2963.999997615814\n",
      "Epoch 27 : 2963.999997615814\n",
      "Epoch 28 : 2963.999997615814\n",
      "Epoch 29 : 2963.999997615814\n",
      "Epoch 30 : 2963.999997615814\n",
      "Epoch 31 : 2963.999997615814\n",
      "Epoch 32 : 2963.999997615814\n",
      "Epoch 33 : 2963.999997615814\n",
      "Epoch 34 : 2963.999997615814\n",
      "Epoch 35 : 2963.999997615814\n",
      "Epoch 36 : 2963.999997615814\n",
      "Epoch 37 : 2963.999997615814\n",
      "Epoch 38 : 2963.999997615814\n",
      "Epoch 39 : 2963.999997615814\n",
      "Epoch 40 : 2963.999997615814\n",
      "Epoch 41 : 2963.999997615814\n",
      "Epoch 42 : 2963.999997615814\n",
      "Epoch 43 : 2963.999997615814\n",
      "Epoch 44 : 2963.999997615814\n",
      "Epoch 45 : 2963.999997615814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "100%|██████████| 50/50 [00:00<00:00, 220.97it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 46 : 2963.999997615814\n",
      "Epoch 47 : 2963.999997615814\n",
      "Epoch 48 : 2963.999997615814\n",
      "Epoch 49 : 2963.999997496605\n",
      "Epoch 50 : 2963.999997496605\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(Linear(2,25), Tanh(), Linear(25,25), Tanh(), Linear(25,2), Tanh())\n",
    "\n",
    "criterion = LossMSE()\n",
    "optimizer = optim_SGD(model.param(), 1e-1)\n",
    "nb_epochs = 50\n",
    "mini_batch_size = 100\n",
    "\n",
    "for e in tqdm(range(0, nb_epochs)):\n",
    "    loss = 0\n",
    "    for b in range(0, train_input.size(0), mini_batch_size):\n",
    "        output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "        loss += criterion.forward(output, train_one_hot_target.narrow(0, b, mini_batch_size))\n",
    "        model.zero_grad()\n",
    "        model.backward(criterion.backward())\n",
    "        optimizer.step()\n",
    "    print(\"Epoch\", e+1, \":\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 483,
   "metadata": {},
   "outputs": [],
   "source": [
    " x = Tensor(4,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 484,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0000e+00\n",
       " 1.0842e-19\n",
       " 1.4138e+10\n",
       " 4.6577e-10\n",
       "[torch.FloatTensor of size 4x1]"
      ]
     },
     "execution_count": 484,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Tensor(x.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = Tensor(10,1).normal_(0)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
