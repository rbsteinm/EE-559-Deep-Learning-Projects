{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from torch import FloatTensor, LongTensor, Tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "    def forward(self, *input_):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Module\n",
    "\n",
    "ReLU function: \n",
    "\\begin{equation}\n",
    "f(x) = max(0, x)\n",
    "\\end{equation}\n",
    "\n",
    "the derivative of ReLU is\n",
    "\n",
    "\\begin{equation} \n",
    "f'(x)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.z = input_.clone()\n",
    "        input_[input_ < 0] = 0\n",
    "        return input_\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        da = gradwrtoutput\n",
    "        tensor = self.z.clone()\n",
    "        # g'(z)\n",
    "        tensor[tensor > 0] = 1\n",
    "        tensor[tensor < 0] = 0\n",
    "        # dz[l]\n",
    "        return da.mul(tensor)\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):   \n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.z = input_\n",
    "        return input_.tanh()\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        da = gradwrtoutput\n",
    "        # g'(z)\n",
    "        g_prime = (1 - self.z.tanh().pow(2))\n",
    "        # dz[l]\n",
    "        return da.mul(tensor)\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Module\n",
    "fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Module):   \n",
    "    def __init__(self, in_dim, out_dim):        \n",
    "        self.w = Tensor(out_dim,in_dim).normal_(0)\n",
    "        self.b = Tensor(out_dim,1).normal_(0)\n",
    "        self.x_previous_layer = None\n",
    "        # sum the gradient wrt w / b for each batch in these variables\n",
    "        self.grad_w_sum = Tensor(self.w.size()).zero_()\n",
    "        self.grad_b_sum = Tensor(self.b.size()).zero_()\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.x_previous_layer = input_\n",
    "        return (self.w.mm(input_.t()) + self.b).t()\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        dz = gradwrtoutput.t()\n",
    "        dw = dz.mm(self.x_previous_layer)\n",
    "        db = dz\n",
    "        print(dw.size())\n",
    "        print(db.size())\n",
    "        # sum the gradients for the weights and biases\n",
    "        self.grad_w_sum.add(dw)\n",
    "        self.grad_b_sum.add(db)\n",
    "        \n",
    "        return self.w.t().mm(dz)\n",
    "        \n",
    "    def param(self):\n",
    "        # TODO\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad_w_sum.zero_()\n",
    "        self.grad_b_sum.zero_()\n",
    "        \n",
    "    def SGD_step(self, lr):\n",
    "        self.w -= lr*self.grad_w_sum\n",
    "        self.b -= lr*self.grad_b_sum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Module\n",
    "to combine several modules in basic sequential structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):    \n",
    "    def __init__(self, *layers_):\n",
    "        self.modules = layers_\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        x = input_\n",
    "        for module in self.modules:\n",
    "            x = module.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        x = gradwrtoutput\n",
    "        for module in self.modules:\n",
    "            x = module.backward(x)\n",
    "        return x\n",
    "        \n",
    "    def param(self):\n",
    "        # TODO\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Loss Function TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossMSE(Module): \n",
    "    def __init__(self):\n",
    "        self.error = None\n",
    "        \n",
    "    def forward(self, preds, labels):\n",
    "        self.error = preds - labels\n",
    "        return self.error.pow(2).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        return 2 * self.error\n",
    "        \n",
    "    def param(self):\n",
    "        # TODO\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "-3.9147  5.9485\n",
       "-4.1990 -1.6456\n",
       " 8.7344 -7.4869\n",
       " 2.4498 -8.1163\n",
       "-0.4472  5.3993\n",
       "[torch.FloatTensor of size 5x2]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = FloatTensor(5, 2)\n",
    "a.normal_(0, 5)\n",
    "b = FloatTensor(5, 2)\n",
    "b.normal_(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -9.4639 -11.7929\n",
      " -3.3858 -14.1017\n",
      "  3.7349   1.5512\n",
      "  4.7306  -0.6805\n",
      "  1.5711  -0.4694\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n",
      "\n",
      "-3.9147  5.9485\n",
      "-4.1990 -1.6456\n",
      " 8.7344 -7.4869\n",
      " 2.4498 -8.1163\n",
      "-0.4472  5.3993\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.FloatTensor"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = ReLU().forward(a)\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = FloatTensor(5, 10).normal_(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2242.38652664423"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model = Sequential(Linear(2,10),ReLU())\n",
    "\n",
    "our_loss = LossMSE()\n",
    "\n",
    "output = our_model.forward(a)\n",
    "\n",
    "our_loss.forward(output, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 2])\n",
      "torch.Size([10, 5])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "inconsistent tensor size, expected r_ [2 x 5], t [2 x 5] and src [5 x 10] to have the same number of elements, but got 10, 10 and 50 elements respectively at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1036",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-124-fa0b92a8023c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mour_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mour_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-116-2aa08074782d>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradwrtoutput)\u001b[0m\n\u001b[1;32m     12\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgradwrtoutput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodules\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m             \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-001be55347a2>\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradwrtoutput)\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mtensor\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtensor\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m         \u001b[0;31m# dz[l]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmul\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mparam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: inconsistent tensor size, expected r_ [2 x 5], t [2 x 5] and src [5 x 10] to have the same number of elements, but got 10, 10 and 50 elements respectively at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/TH/generic/THTensorMath.c:1036"
     ]
    }
   ],
   "source": [
    "our_model.backward(our_loss.backward())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "\n",
       "Columns 0 to 7 \n",
       " -5.2156   8.9197 -20.2387   9.5524  12.1348   7.4901   2.8185  28.8327\n",
       " 12.3700  -1.9795  11.1196  16.2809  -0.5563  -1.1511 -22.5246  -2.9792\n",
       " 12.2615  11.4033   7.3916  21.8851   2.4965  -1.7212  -3.4069  -2.0032\n",
       "  3.0220  20.7735   8.1369   5.1801   3.6803  14.6389   3.6854 -22.3885\n",
       "  6.3670  23.5802  -3.6112  -3.5011   7.9782 -15.0273  -7.8661 -21.9332\n",
       "\n",
       "Columns 8 to 9 \n",
       "-10.9242  26.9693\n",
       "  9.2024 -11.0887\n",
       " 16.4431  18.5549\n",
       " 17.0725  18.1277\n",
       " 10.1746   7.5075\n",
       "[torch.FloatTensor of size 5x10]"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_loss.backward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0.0931  0.0000  0.0000  0.8406  0.0000  0.0111  0.0000  0.0000  0.0000  0.3615\n",
       " 0.0931  0.0000  0.0000  0.8406  0.0000  0.0111  0.0000  0.0000  0.0000  0.3615\n",
       " 3.4060  0.1470  0.0000  1.3889  5.7810  4.9087  4.7950  0.0000  5.7277  0.0000\n",
       " 1.5007  0.6573  0.0000  2.5456  6.2245  3.5140  4.2874  0.0000  6.7593  0.0000\n",
       " 0.5606  0.0000  0.0000  1.4068  1.4057  1.1744  1.1125  0.0000  1.8837  0.0000\n",
       "[torch.FloatTensor of size 5x10]"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
