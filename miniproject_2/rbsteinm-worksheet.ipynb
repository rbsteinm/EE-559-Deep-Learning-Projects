{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor, LongTensor, Tensor\n",
    "import math\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "    def forward(self, *input_):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLU Module\n",
    "\n",
    "ReLU function: \n",
    "\\begin{equation}\n",
    "f(x) = max(0, x)\n",
    "\\end{equation}\n",
    "\n",
    "the derivative of ReLU is\n",
    "\n",
    "\\begin{equation} \n",
    "f'(x)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.z = input_.clone()\n",
    "        input_[input_ < 0] = 0\n",
    "        return input_\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        da = gradwrtoutput\n",
    "        tensor = self.z.clone()\n",
    "        # g'(z)\n",
    "        tensor[tensor > 0] = 1\n",
    "        tensor[tensor < 0] = 0\n",
    "        # dz[l]\n",
    "        return da.mul(tensor)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tanh Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):   \n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.z = input_\n",
    "        return input_.tanh()\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        da = gradwrtoutput\n",
    "        # g'(z)\n",
    "        g_prime = (1 - self.z.tanh().pow(2))\n",
    "        # dz[l]\n",
    "        return da.mul(g_prime)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Module\n",
    "fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Module):   \n",
    "    def __init__(self, in_dim, out_dim):        \n",
    "        self.w = Tensor(out_dim,in_dim).normal_(0)\n",
    "        self.b = Tensor(out_dim,1).normal_(0)\n",
    "        self.x_previous_layer = None\n",
    "        # sum the gradient wrt w / b for each batch in these variables\n",
    "        self.grad_w_sum = Tensor(self.w.size()).zero_()\n",
    "        self.grad_b_sum = Tensor(self.b.size()).zero_()\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.x_previous_layer = input_\n",
    "        return (self.w.mm(input_.t()) + self.b).t()\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        dz = gradwrtoutput.t()\n",
    "        dw = dz.mm(self.x_previous_layer)\n",
    "        db = dz\n",
    "        # sum the gradients for the weights and biases\n",
    "        self.grad_w_sum = dw\n",
    "        self.grad_b_sum = db.sum(1)\n",
    "        return (self.w.t().mm(dz)).t()\n",
    "        \n",
    "    def param(self):\n",
    "        return [ (self.w, self.grad_w_sum), (self.b, self.grad_b_sum) ]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad_w_sum.zero_()\n",
    "        self.grad_b_sum.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sequential Module\n",
    "to combine several modules in basic sequential structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):    \n",
    "    def __init__(self, *layers_):\n",
    "        self.modules = layers_\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        x = input_\n",
    "        for module in self.modules:\n",
    "            x = module.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        x = gradwrtoutput\n",
    "        for module in reversed(self.modules):\n",
    "            x = module.backward(x)\n",
    "        return x\n",
    "        \n",
    "    def param(self):\n",
    "        return [ p for module in self.modules for p in module.param() ]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for module in self.modules:\n",
    "            module.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MSE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossMSE(Module): \n",
    "    def __init__(self):\n",
    "        self.error = None\n",
    "        \n",
    "    def forward(self, preds, labels):\n",
    "        self.error = preds - labels\n",
    "        return self.error.pow(2).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        return 2 * self.error\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class optim_SGD(Module): \n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.param = parameters\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    def step(self):\n",
    "        #print(self.param)\n",
    "        for (p, grad_p) in self.param:\n",
    "            p -= self.lr*grad_p\n",
    "        #print(self.param)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  5.9373   7.2770\n",
       " -7.0436   2.5153\n",
       "  7.2012   3.9569\n",
       " 18.7668  -2.0585\n",
       "  3.5095   5.1504\n",
       "[torch.FloatTensor of size 5x2]"
      ]
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = FloatTensor(5, 2)\n",
    "a.normal_(0, 5)\n",
    "b = FloatTensor(5, 2)\n",
    "b.normal_(0, 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " -9.4639 -11.7929\n",
      " -3.3858 -14.1017\n",
      "  3.7349   1.5512\n",
      "  4.7306  -0.6805\n",
      "  1.5711  -0.4694\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n",
      "\n",
      "-3.9147  5.9485\n",
      "-4.1990 -1.6456\n",
      " 8.7344 -7.4869\n",
      " 2.4498 -8.1163\n",
      "-0.4472  5.3993\n",
      "[torch.FloatTensor of size 5x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(a)\n",
    "print(b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  2.4001\n",
       " -7.2299\n",
       "  5.4902\n",
       "  6.9134\n",
       " 10.6199\n",
       "[torch.FloatTensor of size 5]"
      ]
     },
     "execution_count": 291,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.sum(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "260.3018182516098"
      ]
     },
     "execution_count": 318,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.pow(2).sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 324,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = FloatTensor(5, 4).normal_(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(\n",
      "-0.9394  0.2829\n",
      "-0.9570 -0.9508\n",
      " 0.0185 -0.0531\n",
      "-0.5339 -0.9712\n",
      " 1.0330 -0.9913\n",
      " 0.5648  1.1601\n",
      " 2.0306  0.6977\n",
      "-1.5693  1.2258\n",
      "-0.1933 -0.0247\n",
      " 0.3024  1.1010\n",
      "[torch.FloatTensor of size 10x2]\n",
      ", \n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "    0     0\n",
      "[torch.FloatTensor of size 10x2]\n",
      "), (\n",
      " 0.1473\n",
      "-0.9681\n",
      "-2.4477\n",
      "-1.2904\n",
      " 0.2527\n",
      " 0.1553\n",
      " 0.4330\n",
      "-0.5547\n",
      "-0.7481\n",
      "-0.1981\n",
      "[torch.FloatTensor of size 10x1]\n",
      ", \n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "    0\n",
      "[torch.FloatTensor of size 10x1]\n",
      "), (\n",
      " 0.0213 -0.3914  1.2891  0.2215  0.4615  0.7060 -0.7218  1.5412  0.9724  1.3990\n",
      " 0.5743 -0.1761  0.3649  0.7016 -0.6176  0.2110 -0.5071  1.8456  1.7542 -0.0271\n",
      "-0.1300 -0.9722 -0.2770 -0.0351 -0.2841 -0.5783 -1.8363 -0.5716  1.4358 -1.8119\n",
      " 0.2769  0.7367  0.2189  1.1120  1.2765 -1.4958 -0.4148 -0.9925 -0.2688 -0.9220\n",
      "[torch.FloatTensor of size 4x10]\n",
      ", \n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "    0     0     0     0     0     0     0     0     0     0\n",
      "[torch.FloatTensor of size 4x10]\n",
      "), (\n",
      "-1.0338\n",
      " 0.3525\n",
      " 0.1426\n",
      " 0.0580\n",
      "[torch.FloatTensor of size 4x1]\n",
      ", \n",
      " 0\n",
      " 0\n",
      " 0\n",
      " 0\n",
      "[torch.FloatTensor of size 4x1]\n",
      ")]\n",
      "\n",
      "\n",
      "Columns 0 to 7 \n",
      "  97.5150  102.3557    0.0000   52.0979    0.0000   -3.9664   -5.0847  135.3083\n",
      " 268.1971  281.5105    0.0000  143.2856    0.0000    0.0000    0.0000  372.9021\n",
      "   0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000    0.0000\n",
      " 102.3646  107.4460    0.0000   54.6888   45.4540    2.8213   54.8043  142.3281\n",
      "\n",
      "Columns 8 to 9 \n",
      "   8.1508   -2.9114\n",
      "  22.4172    0.0000\n",
      "   0.0000    0.0000\n",
      "   8.5561    0.0000\n",
      "[torch.FloatTensor of size 4x10]\n",
      "\n",
      "\n",
      "-201.4544  -33.9833\n",
      "  10.9085    1.8402\n",
      "   0.0000    0.0000\n",
      "-353.4664  -59.6262\n",
      "  39.2602  -16.5639\n",
      " -46.9046   17.5856\n",
      " -11.8384    7.2473\n",
      "-595.8243 -104.1604\n",
      "-587.4060  -99.0894\n",
      "  -1.7818   -3.6148\n",
      "[torch.FloatTensor of size 10x2]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "our_model = Sequential(Linear(2,10), ReLU(), Linear(10,4), ReLU())\n",
    "\n",
    "our_loss = LossMSE()\n",
    "\n",
    "our_optim = optim_SGD(our_model.param(), 0.01)\n",
    "\n",
    "output = our_model.forward(a)\n",
    "\n",
    "our_loss.forward(output, target)\n",
    "\n",
    "our_model.backward(our_loss.backward())\n",
    "\n",
    "our_optim.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    a = Tensor(nb, 2).uniform_(0, 1)\n",
    "    target = (a.pow(2).sum(1) < (2/math.pi)).long()\n",
    "    return a, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(target):\n",
    "    tmp = FloatTensor(target.size(0), 2).fill_(0)\n",
    "    for k in range(0, target.size(0)):\n",
    "        tmp[k, train_target[k]] = 1\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_hot_target = convert_to_one_hot(train_target)\n",
    "test_one_hot_target = convert_to_one_hot(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 259.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : 1208.3341082928696\n",
      "Epoch 2 : 1208.3341082928696\n",
      "Epoch 3 : 1208.3341082928696\n",
      "Epoch 4 : 1208.3341082928696\n",
      "Epoch 5 : 1208.3341082928696\n",
      "Epoch 6 : 1208.3341082928696\n",
      "Epoch 7 : 1208.3341082928696\n",
      "Epoch 8 : 1208.3341082928696\n",
      "Epoch 9 : 1208.3341082928696\n",
      "Epoch 10 : 1208.3341082928696\n",
      "Epoch 11 : 1208.3341082928696\n",
      "Epoch 12 : 1208.3341082928696\n",
      "Epoch 13 : 1208.3341082928696\n",
      "Epoch 14 : 1208.3341082928696\n",
      "Epoch 15 : 1208.3341082928696\n",
      "Epoch 16 : 1208.3341082928696\n",
      "Epoch 17 : 1208.3341082928696\n",
      "Epoch 18 : 1208.3341082928696\n",
      "Epoch 19 : 1208.3341082928696\n",
      "Epoch 20 : 1208.3341082928696\n",
      "Epoch 21 : 1208.3341082928696\n",
      "Epoch 22 : 1208.3341082928696\n",
      "Epoch 23 : 1208.3341082928696\n",
      "Epoch 24 : 1208.3341082928696\n",
      "Epoch 25 : 1208.3341082928696\n",
      "Epoch 26 : 1208.3341082928696\n",
      "Epoch 27 : 1208.3341082928696\n",
      "Epoch 28 : 1208.3341082928696\n",
      "Epoch 29 : 1208.3341082928696\n",
      "Epoch 30 : 1208.3341082928696\n",
      "Epoch 31 : 1208.3341082928696\n",
      "Epoch 32 : 1208.3341082928696\n",
      "Epoch 33 : 1208.3341082928696\n",
      "Epoch 34 : 1208.3341082928696\n",
      "Epoch 35 : 1208.3341082928696\n",
      "Epoch 36 : 1208.3341082928696\n",
      "Epoch 37 : 1208.3341082928696\n",
      "Epoch 38 : 1208.3341082928696\n",
      "Epoch 39 : 1208.3341082928696\n",
      "Epoch 40 : 1208.3341082928696\n",
      "Epoch 41 : 1208.3341082928696\n",
      "Epoch 42 : 1208.3341082928696\n",
      "Epoch 43 : 1208.3341082928696\n",
      "Epoch 44 : 1208.3341082928696\n",
      "Epoch 45 : 1208.3341082928696\n",
      "Epoch 46 : 1208.3341082928696\n",
      "Epoch 47 : 1208.3341082928696\n",
      "Epoch 48 : 1208.3341082928696\n",
      "Epoch 49 : 1208.3341082928696\n",
      "Epoch 50 : 1208.3341082928696\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(Linear(2,25), Tanh(), Linear(25,25), Tanh(), Linear(25,2), Tanh())\n",
    "\n",
    "criterion = LossMSE()\n",
    "optimizer = optim_SGD(model.param(), 1e-1)\n",
    "nb_epochs = 50\n",
    "mini_batch_size = 100\n",
    "\n",
    "for e in tqdm(range(0, nb_epochs)):\n",
    "    loss = 0\n",
    "    for b in range(0, train_input.size(0), mini_batch_size):\n",
    "        output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "        loss += criterion.forward(output, train_one_hot_target.narrow(0, b, mini_batch_size))\n",
    "        model.zero_grad()\n",
    "        model.backward(criterion.backward())\n",
    "        optimizer.step()\n",
    "    print(\"Epoch\", e+1, \":\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
