{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import FloatTensor, LongTensor, Tensor\n",
    "import math\n",
    "from tqdm import tqdm, tqdm_notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Module(object):\n",
    "    \n",
    "    def forward(self, *input_):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def backward(self, *gradwrtoutput):\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ReLU Module\n",
    "\n",
    "ReLU function: \n",
    "\\begin{equation}\n",
    "f(x) = max(0, x)\n",
    "\\end{equation}\n",
    "\n",
    "the derivative of ReLU is\n",
    "\n",
    "\\begin{equation} \n",
    "f'(x)=\n",
    "    \\begin{cases}\n",
    "      1, & \\text{if}\\ x>0 \\\\\n",
    "      0, & \\text{otherwise}\n",
    "    \\end{cases}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ReLU(Module):\n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.z = input_.clone()\n",
    "        input_[input_ < 0] = 0\n",
    "        return input_\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        da = gradwrtoutput\n",
    "        tensor = self.z.clone()\n",
    "        # g'(z)\n",
    "        tensor[tensor > 0] = 1\n",
    "        tensor[tensor < 0] = 0\n",
    "        # dz[l]\n",
    "        return da.mul(tensor)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tanh Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Tanh(Module):   \n",
    "    def __init__(self):\n",
    "        self.z = None\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.z = input_\n",
    "        return input_.tanh()\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        da = gradwrtoutput\n",
    "        # g'(z)\n",
    "        g_prime = (1 - self.z.tanh().pow(2))\n",
    "        # dz[l]\n",
    "        return da.mul(g_prime)\n",
    "        \n",
    "    def param(self):\n",
    "        return []\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Module\n",
    "fully connected layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 502,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Linear(Module):   \n",
    "    def __init__(self, in_dim, out_dim):        \n",
    "        self.w = Tensor(out_dim,in_dim).normal_(0)\n",
    "        self.b = Tensor(out_dim,1).normal_(0)\n",
    "        self.x_previous_layer = None\n",
    "        # sum the gradient wrt w / b for each batch in these variables\n",
    "        self.grad_w_sum = Tensor(self.w.size()).zero_()\n",
    "        self.grad_b_sum = Tensor(self.b.size()).zero_()\n",
    "    \n",
    "    def forward(self, input_):\n",
    "        self.x_previous_layer = input_\n",
    "        return (self.w.mm(input_.t()) + self.b).t()\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        dz = gradwrtoutput.t()\n",
    "        dw = dz.mm(self.x_previous_layer)\n",
    "        db = dz\n",
    "        # sum the gradients for the weights and biases\n",
    "        self.grad_w_sum += dw\n",
    "        self.grad_b_sum += db.sum(1).unsqueeze(1)\n",
    "        return (self.w.t().mm(dz)).t()\n",
    "        \n",
    "    def param(self):\n",
    "        return [ (self.w, self.grad_w_sum), (self.b, self.grad_b_sum) ]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        self.grad_w_sum.zero_()\n",
    "        self.grad_b_sum.zero_()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sequential Module\n",
    "to combine several modules in basic sequential structure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequential(Module):    \n",
    "    def __init__(self, *layers_):\n",
    "        self.modules = layers_\n",
    "        \n",
    "    def forward(self, input_):\n",
    "        x = input_\n",
    "        for module in self.modules:\n",
    "            x = module.forward(x)\n",
    "        return x\n",
    "        \n",
    "    def backward(self, gradwrtoutput):\n",
    "        x = gradwrtoutput\n",
    "        for module in reversed(self.modules):\n",
    "            x = module.backward(x)\n",
    "        return x\n",
    "        \n",
    "    def param(self):\n",
    "        return [ p for module in self.modules for p in module.param() ]\n",
    "    \n",
    "    def zero_grad(self):\n",
    "        for module in self.modules:\n",
    "            module.zero_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MSE Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class LossMSE(Module): \n",
    "    def __init__(self):\n",
    "        self.error = None\n",
    "        \n",
    "    def forward(self, preds, labels):\n",
    "        self.error = preds - labels\n",
    "        return self.error.pow(2).sum()\n",
    "        \n",
    "    def backward(self):\n",
    "        return 2 * self.error\n",
    "        \n",
    "    def param(self):\n",
    "        return []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## SGD optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 465,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class optim_SGD(Module): \n",
    "    def __init__(self, parameters, learning_rate):\n",
    "        self.param = parameters #[ p.shallow() for tup in parameters for p in tup ]\n",
    "        self.lr = learning_rate\n",
    "        \n",
    "    def step(self):\n",
    "        for (p, grad_p) in self.param:\n",
    "            p.sub_(self.lr*grad_p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 494,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "target = FloatTensor(5, 4).normal_(0,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 497,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "   1.8532   10.2142\n",
       "-141.7424  153.1749\n",
       " -64.6264   69.8389\n",
       "   0.0000    0.0000\n",
       "-164.5195   86.1878\n",
       "[torch.FloatTensor of size 5x2]"
      ]
     },
     "execution_count": 497,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "our_model = Sequential(Linear(2,10), ReLU(), Linear(10,4), ReLU())\n",
    "\n",
    "our_loss = LossMSE()\n",
    "\n",
    "our_optim = optim_SGD(our_model.param(), 0.01)\n",
    "\n",
    "output = our_model.forward(a)\n",
    "\n",
    "our_loss.forward(output, target)\n",
    "\n",
    "our_model.backward(our_loss.backward())\n",
    "\n",
    "#our_optim.step()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 534,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def generate_disc_set(nb):\n",
    "    a = Tensor(nb, 2).uniform_(0, 1)\n",
    "    target = (a.pow(2).sum(1) < (2/math.pi)).long()\n",
    "    return a, target\n",
    "\n",
    "train_input, train_target = generate_disc_set(1000)\n",
    "test_input, test_target = generate_disc_set(1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 535,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "520"
      ]
     },
     "execution_count": 535,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_to_one_hot(target):\n",
    "    tmp = FloatTensor(target.size(0), 2).fill_(0)\n",
    "    for k in range(0, target.size(0)):\n",
    "        tmp[k, train_target[k]] = 1\n",
    "    return tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 536,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_one_hot_target = convert_to_one_hot(train_target)\n",
    "test_one_hot_target = convert_to_one_hot(test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1 : 1232.1206424680252\n",
      "Epoch 2 : 395.00490848482826\n",
      "Epoch 3 : 284.54823530106984\n",
      "Epoch 4 : 247.88485170867702\n",
      "Epoch 5 : 214.811980666586\n",
      "Epoch 6 : 176.07135696391555\n",
      "Epoch 7 : 139.64829893080648\n",
      "Epoch 8 : 125.56326954292145\n",
      "Epoch 9 : 118.1748435563116\n",
      "Epoch 10 : 112.99839875409816\n",
      "Epoch 11 : 108.92823118393753\n",
      "Epoch 12 : 105.49558437747082\n",
      "Epoch 13 : 102.4633725938744\n",
      "Epoch 14 : 99.70230613186138\n",
      "Epoch 15 : 97.13978662415683\n",
      "Epoch 16 : 94.73573195138749\n",
      "Epoch 17 : 92.46955646511796\n",
      "Epoch 18 : 90.33204806339903\n",
      "Epoch 19 : 88.31973954808043\n",
      "Epoch 20 : 86.43102904268\n",
      "Epoch 21 : 84.6639106038726\n",
      "Epoch 22 : 83.01499246640608\n",
      "Epoch 23 : 81.47941062448085\n",
      "Epoch 24 : 80.05120698120467\n",
      "Epoch 25 : 78.72373668487606\n",
      "Epoch 26 : 77.49004642496656\n",
      "Epoch 27 : 76.34335888649491\n",
      "Epoch 28 : 75.27715828406097\n",
      "Epoch 29 : 74.285409241054\n",
      "Epoch 30 : 73.3625733547268\n",
      "Epoch 31 : 72.50355743472016\n",
      "Epoch 32 : 71.70369856836193\n",
      "Epoch 33 : 70.95859975262792\n",
      "Epoch 34 : 70.26412210405859\n",
      "Epoch 35 : 69.616332210966\n",
      "Epoch 36 : 69.01139709440889\n",
      "Epoch 37 : 68.44563630666252\n",
      "Epoch 38 : 67.91553202203002\n",
      "Epoch 39 : 67.41776940014216\n",
      "Epoch 40 : 66.94922828865924\n",
      "Epoch 41 : 66.50699831731724\n",
      "Epoch 42 : 66.08841758221338\n",
      "Epoch 43 : 65.69099989786088\n",
      "Epoch 44 : 65.31252812766787\n",
      "Epoch 45 : 64.95095137179663\n",
      "Epoch 46 : 64.60448076753491\n",
      "Epoch 47 : 64.27139906496213\n",
      "Epoch 48 : 63.95018401021798\n",
      "Epoch 49 : 63.639511566095074\n",
      "Epoch 50 : 63.33805099943147\n",
      "Epoch 51 : 63.04467738052891\n",
      "Epoch 52 : 62.75833909969789\n",
      "Epoch 53 : 62.478035064766246\n",
      "Epoch 54 : 62.20287610242627\n",
      "Epoch 55 : 61.93208464836083\n",
      "Epoch 56 : 61.66490833595461\n",
      "Epoch 57 : 61.40070458358108\n",
      "Epoch 58 : 61.13893709093966\n",
      "Epoch 59 : 60.87918639130298\n",
      "Epoch 60 : 60.62105704296111\n",
      "Epoch 61 : 60.364369849605524\n",
      "Epoch 62 : 60.10898873434581\n",
      "Epoch 63 : 59.85493680125902\n",
      "Epoch 64 : 59.60224273463045\n",
      "Epoch 65 : 59.35108240488316\n",
      "Epoch 66 : 59.10162243285387\n",
      "Epoch 67 : 58.8541025354878\n",
      "Epoch 68 : 58.60867532853336\n",
      "Epoch 69 : 58.36553681369142\n",
      "Epoch 70 : 58.124819407480295\n",
      "Epoch 71 : 57.88660542611426\n",
      "Epoch 72 : 57.65092561422502\n",
      "Epoch 73 : 57.41782169104948\n",
      "Epoch 74 : 57.18724092847152\n",
      "Epoch 75 : 56.95914406274083\n",
      "Epoch 76 : 56.73348899251465\n",
      "Epoch 77 : 56.51017253553402\n",
      "Epoch 78 : 56.289132619117275\n",
      "Epoch 79 : 56.07026987423966\n",
      "Epoch 80 : 55.85355254693607\n",
      "Epoch 81 : 55.63889010479933\n",
      "Epoch 82 : 55.426252813861325\n",
      "Epoch 83 : 55.215578817685156\n",
      "Epoch 84 : 55.006807471013644\n",
      "Epoch 85 : 54.799929259612526\n",
      "Epoch 86 : 54.59493109875805\n",
      "Epoch 87 : 54.39177394311925\n",
      "Epoch 88 : 54.19045076615224\n",
      "Epoch 89 : 53.99095987157032\n",
      "Epoch 90 : 53.79329212620456\n",
      "Epoch 91 : 53.59744656966949\n",
      "Epoch 92 : 53.40343308481491\n",
      "Epoch 93 : 53.21123323446135\n",
      "Epoch 94 : 53.02087295364241\n",
      "Epoch 95 : 52.83232637165261\n",
      "Epoch 96 : 52.645613495967275\n",
      "Epoch 97 : 52.460743281974146\n",
      "Epoch 98 : 52.277676099728296\n",
      "Epoch 99 : 52.096457665671714\n",
      "Epoch 100 : 51.91704806097346\n"
     ]
    }
   ],
   "source": [
    "model = Sequential(Linear(2,25), Tanh(), Linear(25,25), Tanh(), Linear(25,2), Tanh())\n",
    "\n",
    "criterion = LossMSE()\n",
    "optimizer = optim_SGD(model.param(), 1e-3)\n",
    "nb_epochs = 100\n",
    "mini_batch_size = 5\n",
    "\n",
    "for e in range(0, nb_epochs):\n",
    "    loss = 0\n",
    "    for b in range(0, train_input.size(0), mini_batch_size):\n",
    "        output = model.forward(train_input.narrow(0, b, mini_batch_size))\n",
    "        loss += criterion.forward(output, train_one_hot_target.narrow(0, b, mini_batch_size))\n",
    "        model.zero_grad()\n",
    "        model.backward(criterion.backward())\n",
    "        optimizer.step()\n",
    "    print(\"Epoch\", e+1, \":\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "nb_data_errors = 0\n",
    "output = model.forward(test_input)\n",
    "\n",
    "_, predicted_classes = output.max(1)\n",
    "for k in range(test_input.size(0)):\n",
    "    if test_target[k] != predicted_classes[k]:\n",
    "        nb_data_errors = nb_data_errors + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy : 97.7 %\n"
     ]
    }
   ],
   "source": [
    "print(\"Test accuracy :\", 100*(1-nb_data_errors/test_input.size(0)), \"%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
