{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n",
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "import dlc_bci as bci\n",
    "train_input , train_target = bci.load(root = './data_bci')\n",
    "print(str(type(train_input)), train_input.size()) \n",
    "print(str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(str(type(test_input)), test_input.size()) \n",
    "print(str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://documents.epfl.ch/users/f/fl/fleuret/www/data/bci/sp1s_aa_train_1000Hz.txt\n",
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 500])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n",
      "Downloading https://documents.epfl.ch/users/f/fl/fleuret/www/data/bci/sp1s_aa_test_1000Hz.txt\n",
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 500])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "import dlc_bci as bci\n",
    "train_input , train_target = bci.load(root = './data_bci', one_khz = True)\n",
    "print(str(type(train_input)), train_input.size()) \n",
    "print(str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False, one_khz = True)\n",
    "print(str(type(test_input)), test_input.size()) \n",
    "print(str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  41.8000   44.8000   47.1000  ...    69.8000   72.6000   76.1000\n",
       " -10.3000   -5.9000   -3.3000  ...    12.6000   24.0000   26.5000\n",
       "  38.1000   25.2000   46.0000  ...    45.1000   74.1000   64.8000\n",
       "             ...                â‹±                ...             \n",
       "   7.9000   11.2000   14.3000  ...    32.7000   43.4000   45.5000\n",
       "  19.2000   33.6000   33.8000  ...    46.7000   53.7000   43.4000\n",
       "  -0.4000   12.7000   12.0000  ...    30.7000   40.6000   33.1000\n",
       "[torch.FloatTensor of size 28x50]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.LongTensor of size 316]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = logistic_reg.predict(test_input.view(test_input.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(1-(torch.LongTensor(pred) - test_target).abs().sum()/test_input.size(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0 ,.,.) = \n",
       " -0.8203 -0.8865 -0.7052  ...  -0.4529 -0.5386 -0.4798\n",
       " -2.1847 -2.2606 -2.1063  ...  -1.8760 -1.9667 -1.8662\n",
       " -0.8914 -0.6268 -0.8203  ...  -0.4015 -0.8056 -0.3696\n",
       "           ...             â‹±             ...          \n",
       " -1.1094 -1.1534 -1.0359  ...  -1.0898 -1.2588 -1.2980\n",
       " -0.7076 -0.9942 -0.7566  ...  -0.5533 -0.7640 -0.9599\n",
       " -1.4106 -1.5650 -1.3837  ...  -1.1681 -1.4841 -1.8173\n",
       "\n",
       "( 1 ,.,.) = \n",
       "  1.2519  1.6708  1.3817  ...   1.4577  1.4381  1.1956\n",
       " -0.0120  0.7277  0.1987  ...   0.5244 -0.1614  0.2966\n",
       "  0.8821  1.8398  1.1197  ...   1.3940  0.8404  1.1442\n",
       "           ...             â‹±             ...          \n",
       "  1.2274  1.2029  0.9776  ...  -0.4896 -0.0683  0.2942\n",
       "  1.4283  1.2446  1.0682  ...  -0.3941  0.4607  1.1246\n",
       "  1.2642  1.1613  0.9188  ...  -1.1559 -0.1614  0.6616\n",
       "\n",
       "( 2 ,.,.) = \n",
       "  1.0486  1.2740  1.3303  ...   1.2470  1.0217  0.8355\n",
       " -0.7762 -0.2692 -0.4651  ...  -0.3500 -0.6807 -0.6905\n",
       "  0.5661  0.9849  0.8747  ...   0.9310  0.5122  0.6983\n",
       "           ...             â‹±             ...          \n",
       " -0.0953  0.0811  0.0762  ...  -0.0144 -0.2741 -0.3084\n",
       "  0.2329  0.1080 -0.0169  ...   0.5955  0.2476 -0.1394\n",
       "  0.0909 -0.0169 -0.2031  ...  -0.0585 -0.3794 -0.4847\n",
       "... \n",
       "\n",
       "(97 ,.,.) = \n",
       "  2.9764  2.7094  2.6359  ...   1.2813  1.1246  1.2005\n",
       "  0.2525  0.5955  0.0076  ...  -0.7860 -1.5674 -0.8644\n",
       "  0.7008  1.0168  0.5342  ...  -0.1957 -1.0016 -0.3500\n",
       "           ...             â‹±             ...          \n",
       " -0.3696 -0.6954 -0.3353  ...  -1.2367 -1.3004 -1.5601\n",
       " -0.0291 -0.3647  0.3383  ...  -1.0163 -0.8424 -0.9599\n",
       " -0.6489 -0.8840 -0.2839  ...  -1.3298 -1.2098 -1.3886\n",
       "\n",
       "(98 ,.,.) = \n",
       "  1.4797  1.5116  1.0192  ...   1.2985  1.0364  1.4283\n",
       " -0.2471 -0.1369 -0.7272  ...  -0.3892 -0.7591 -0.2153\n",
       "  0.8282  1.1172  0.4020  ...   0.6690  0.2183  0.8747\n",
       "           ...             â‹±             ...          \n",
       " -0.2006 -0.0561 -0.0071  ...  -0.7052 -0.3868 -0.4382\n",
       " -0.3892 -0.5705 -0.2716  ...  -0.2055  0.1350  0.1203\n",
       " -0.6489 -0.2839 -0.1124  ...  -0.6146 -0.3917 -0.3745\n",
       "\n",
       "(99 ,.,.) = \n",
       "  2.3958  2.5942  2.3860  ...   0.8061  0.4216  0.8894\n",
       "  0.3334  0.7498  0.2917  ...  -0.7713 -1.3837 -0.7223\n",
       "  1.1980  1.5802  1.2372  ...   0.2721 -0.2471  0.4216\n",
       "           ...             â‹±             ...          \n",
       "  0.0370 -0.1443  0.0737  ...  -0.6464 -0.7689 -0.4921\n",
       "  0.0296 -0.2643  0.0762  ...  -0.5901 -0.7101 -0.5509\n",
       " -0.1492 -0.4896 -0.2545  ...  -0.6832 -0.9256 -0.7517\n",
       "[torch.FloatTensor of size 100x28x50]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# normalize mean to 0\n",
    "train_input.sub_(train_input.mean())\n",
    "test_input.sub_(test_input.mean())\n",
    "\n",
    "# normalize variance to 1\n",
    "train_input.div_(train_input.std())\n",
    "test_input.div_(test_input.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basic_CNN(nn.Module):\n",
    "    def __init__(self, nb_hidden=64):\n",
    "        super(Basic_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        #self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(640, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"======\")\n",
    "        #print(x.size())\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #print(x.size())\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        #print(x.size())\n",
    "        #x = self.dropout(x)\n",
    "        x = F.tanh(self.fc1(x.view(-1, 640)))\n",
    "        #print(x.size())\n",
    "        x = self.fc2(x)\n",
    "        #print(\"======\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Basic_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(cnn, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_errors_cnn = compute_nb_errors(cnn, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "50.0 % accuracy\n"
     ]
    }
   ],
   "source": [
    "print(100*(1-(nb_errors_cnn/test_input.size(0))),'% accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.LongTensor of size 100]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = cnn(Variable(test_input.view(-1, 1, 28, 50)))\n",
    "_, predicted_classes = torch.max(output.data, 1)\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "50"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(predicted_classes-test_target).abs().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       " 0.9980  0.9978\n",
       " 0.9977  0.9974\n",
       " 0.9980  0.9977\n",
       " 0.9970  0.9967\n",
       " 0.9980  0.9978\n",
       " 0.9980  0.9978\n",
       " 0.9973  0.9970\n",
       " 0.9981  0.9979\n",
       " 0.9979  0.9977\n",
       " 0.9969  0.9967\n",
       " 0.9980  0.9978\n",
       " 0.9981  0.9979\n",
       " 0.9981  0.9980\n",
       " 0.9978  0.9976\n",
       " 0.9979  0.9976\n",
       " 0.9982  0.9980\n",
       " 0.9972  0.9969\n",
       " 0.9980  0.9978\n",
       " 0.9979  0.9976\n",
       " 0.9979  0.9977\n",
       " 0.9978  0.9974\n",
       " 0.9980  0.9977\n",
       " 0.9979  0.9976\n",
       " 0.9866  0.9870\n",
       " 0.9977  0.9975\n",
       " 0.9978  0.9976\n",
       " 0.9978  0.9976\n",
       " 0.9979  0.9976\n",
       " 0.9979  0.9976\n",
       " 0.9983  0.9981\n",
       " 0.9978  0.9975\n",
       " 0.9981  0.9979\n",
       " 0.9981  0.9979\n",
       " 0.9981  0.9979\n",
       " 0.9981  0.9979\n",
       " 0.9980  0.9978\n",
       " 0.9975  0.9974\n",
       " 0.9980  0.9978\n",
       " 0.9982  0.9980\n",
       " 0.9979  0.9977\n",
       " 0.9980  0.9978\n",
       " 0.9980  0.9977\n",
       " 0.9976  0.9973\n",
       " 0.9979  0.9977\n",
       " 0.9979  0.9978\n",
       " 0.9975  0.9973\n",
       " 0.9977  0.9974\n",
       " 0.9981  0.9979\n",
       " 0.9975  0.9972\n",
       " 0.9965  0.9957\n",
       " 0.9954  0.9950\n",
       " 0.9979  0.9977\n",
       " 0.9980  0.9977\n",
       " 0.9979  0.9977\n",
       " 0.9982  0.9979\n",
       " 0.9970  0.9967\n",
       " 0.9977  0.9975\n",
       " 0.9974  0.9972\n",
       " 0.9983  0.9980\n",
       " 0.9979  0.9976\n",
       " 0.9982  0.9979\n",
       " 0.9977  0.9974\n",
       " 0.9976  0.9974\n",
       " 0.9977  0.9975\n",
       " 0.9965  0.9962\n",
       " 0.9981  0.9978\n",
       " 0.9982  0.9979\n",
       " 0.9973  0.9970\n",
       " 0.9974  0.9971\n",
       " 0.9978  0.9976\n",
       " 0.9975  0.9972\n",
       " 0.9980  0.9977\n",
       " 0.9983  0.9982\n",
       " 0.9980  0.9977\n",
       " 0.9979  0.9978\n",
       " 0.9971  0.9967\n",
       " 0.9946  0.9944\n",
       " 0.9979  0.9976\n",
       " 0.9976  0.9974\n",
       " 0.9979  0.9976\n",
       " 0.9980  0.9978\n",
       " 0.9980  0.9978\n",
       " 0.9981  0.9979\n",
       " 0.9976  0.9973\n",
       " 0.9972  0.9970\n",
       " 0.9978  0.9977\n",
       " 0.9980  0.9977\n",
       " 0.9967  0.9963\n",
       " 0.9970  0.9968\n",
       " 0.9981  0.9978\n",
       " 0.9977  0.9974\n",
       " 0.9980  0.9978\n",
       " 0.9979  0.9978\n",
       " 0.9977  0.9974\n",
       " 0.9969  0.9968\n",
       " 0.9980  0.9978\n",
       " 0.9980  0.9978\n",
       " 0.9976  0.9974\n",
       " 0.9980  0.9977\n",
       " 0.9981  0.9979\n",
       "[torch.FloatTensor of size 100x2]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = nn.Sequential(\n",
    "        nn.Linear(1400, 2000),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(2000, 1000),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(1000, 500),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(500, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_model(MLP, Variable(train_input.view(train_input.size(0),-1)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nb_errors_MLP = compute_nb_errors(MLP, Variable(test_input.view(test_input.size(0),-1)), Variable(test_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "49.0 % accuracy\n"
     ]
    }
   ],
   "source": [
    "print(100*(1-(nb_errors_MLP/test_input.size(0))),'% accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
