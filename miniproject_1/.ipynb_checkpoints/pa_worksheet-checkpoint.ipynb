{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pickle \n",
    "import json\n",
    "\n",
    "import dlc_bci as bci\n",
    "from helpers import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (10,10)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input : <class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "Train target : <class 'torch.LongTensor'> torch.Size([316])\n",
      "Test input : <class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "Test target : <class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Subset of data sampled at 100Hz\n",
    "train_input , train_target = bci.load(root = './data_bci')\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input : <class 'torch.FloatTensor'> torch.Size([316, 28, 500])\n",
      "Train target : <class 'torch.LongTensor'> torch.Size([316])\n",
      "Test input : <class 'torch.FloatTensor'> torch.Size([100, 28, 500])\n",
      "Test target : <class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Full data sampled at 1Khz\n",
    "train_input , train_target = bci.load(root = './data_bci', one_khz = True)\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False, one_khz = True)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  41.8000   44.8000   47.1000  ...    69.8000   72.6000   76.1000\n",
       " -10.3000   -5.9000   -3.3000  ...    12.6000   24.0000   26.5000\n",
       "  38.1000   25.2000   46.0000  ...    45.1000   74.1000   64.8000\n",
       "             ...                â‹±                ...             \n",
       "   7.9000   11.2000   14.3000  ...    32.7000   43.4000   45.5000\n",
       "  19.2000   33.6000   33.8000  ...    46.7000   53.7000   43.4000\n",
       "  -0.4000   12.7000   12.0000  ...    30.7000   40.6000   33.1000\n",
       "[torch.FloatTensor of size 28x50]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.LongTensor of size 316]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# normalize mean to 0\n",
    "train_input.sub_(train_input.mean())\n",
    "test_input.sub_(test_input.mean())\n",
    "\n",
    "# normalize variance to 1\n",
    "train_input.div_(train_input.std())\n",
    "test_input.div_(test_input.std())\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq = []\n",
    "for i in range(10):\n",
    "    seq.append(train_input[:,:,i::10])\n",
    "train_input = torch.cat(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = torch.cat([train_target]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq = []\n",
    "for i in range(10):\n",
    "    seq.append(test_input[:,:,i::10])\n",
    "test_input = torch.cat(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_target = torch.cat([test_target]*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_pred_train = discrete_predictions(torch.FloatTensor(linear_reg.predict(train_input.view(train_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % train error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(linear_pred_train) - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_pred_test = discrete_predictions(torch.FloatTensor(linear_reg.predict(test_input.view(test_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0 % test error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(linear_pred_test) - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_pred_train = logistic_reg.predict(train_input.view(train_input.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % train error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(logistic_pred_train) - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_pred_test = logistic_reg.predict(test_input.view(test_input.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.000000000000004 % test error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(logistic_pred_test) - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert targets to one hot labels\n",
    "#train_target = convert_to_one_hot_labels(train_input, train_target)\n",
    "#test_target = convert_to_one_hot_labels(test_input, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP_big = nn.Sequential(\n",
    "        nn.Linear(14000, 1400),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(1400, 280),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(280, 28),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(28, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP = nn.Sequential(\n",
    "        nn.Linear(50, 280),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(280, 140),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(140, 28),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(28, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1 at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/THNN/generic/SpatialClassNLLCriterion.c:60",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-967537cad33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Cours/Deep Learning/DL_projects/miniproject_1/helpers.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_input, train_target, mini_batch_size, nb_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                         \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[0;32m--> 679\u001b[0;31m                                self.ignore_index, self.reduce)\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \"\"\"\n\u001b[0;32m-> 1161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1 at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/THNN/generic/SpatialClassNLLCriterion.c:60"
     ]
    }
   ],
   "source": [
    "train_model(MLP, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 55.861808478832245\n",
      "Epoch 1 loss = 54.59076404571533\n",
      "Epoch 2 loss = 53.797380566596985\n",
      "Epoch 3 loss = 53.191094636917114\n",
      "Epoch 4 loss = 52.66807159781456\n",
      "Epoch 5 loss = 52.18098449707031\n",
      "Epoch 6 loss = 51.69751679897308\n",
      "Epoch 7 loss = 51.194580405950546\n",
      "Epoch 8 loss = 50.65688365697861\n",
      "Epoch 9 loss = 50.074628949165344\n",
      "Epoch 10 loss = 49.44200122356415\n",
      "Epoch 11 loss = 48.756370306015015\n",
      "Epoch 12 loss = 48.019282549619675\n",
      "Epoch 13 loss = 47.23957505822182\n",
      "Epoch 14 loss = 46.418880090117455\n",
      "Epoch 15 loss = 45.549498587846756\n",
      "Epoch 16 loss = 44.65137520432472\n",
      "Epoch 17 loss = 43.72418200969696\n",
      "Epoch 18 loss = 42.76705865561962\n",
      "Epoch 19 loss = 41.78091523051262\n",
      "Epoch 20 loss = 40.76593078672886\n",
      "Epoch 21 loss = 39.72083881497383\n",
      "Epoch 22 loss = 38.644645400345325\n",
      "Epoch 23 loss = 37.53784937411547\n",
      "Epoch 24 loss = 36.39436551183462\n",
      "Epoch 25 loss = 35.19167825952172\n",
      "Epoch 26 loss = 33.98767836764455\n",
      "Epoch 27 loss = 32.73168759420514\n",
      "Epoch 28 loss = 31.445499220862985\n",
      "Epoch 29 loss = 30.16045959852636\n",
      "Epoch 30 loss = 28.89358583278954\n",
      "Epoch 31 loss = 27.635679287835956\n",
      "Epoch 32 loss = 26.451356172561646\n",
      "Epoch 33 loss = 24.993193933740258\n",
      "Epoch 34 loss = 22.329513754695654\n",
      "Epoch 35 loss = 21.300788438413292\n",
      "Epoch 36 loss = 21.69328035786748\n",
      "Epoch 37 loss = 19.40192669071257\n",
      "Epoch 38 loss = 21.74633002281189\n",
      "Epoch 39 loss = 22.328185795107856\n",
      "Epoch 40 loss = 19.27624101890251\n",
      "Epoch 41 loss = 21.494215379934758\n",
      "Epoch 42 loss = 18.539624228607863\n",
      "Epoch 43 loss = 14.740391871193424\n",
      "Epoch 44 loss = 12.809250887483358\n",
      "Epoch 45 loss = 23.509267386049032\n",
      "Epoch 46 loss = 21.448826832696795\n",
      "Epoch 47 loss = 13.998251458164304\n",
      "Epoch 48 loss = 12.360645934939384\n",
      "Epoch 49 loss = 19.829397476278245\n",
      "Epoch 50 loss = 12.925528926309198\n",
      "Epoch 51 loss = 9.004871571436524\n",
      "Epoch 52 loss = 12.712735519977286\n",
      "Epoch 53 loss = 15.814191723242402\n",
      "Epoch 54 loss = 8.620287388330325\n",
      "Epoch 55 loss = 7.815711344475858\n",
      "Epoch 56 loss = 19.2838870389387\n",
      "Epoch 57 loss = 7.382501360960305\n",
      "Epoch 58 loss = 10.198833256727085\n",
      "Epoch 59 loss = 13.357606679695891\n",
      "Epoch 60 loss = 6.52746866340749\n",
      "Epoch 61 loss = 12.122200927638914\n",
      "Epoch 62 loss = 9.533251075306907\n",
      "Epoch 63 loss = 13.620927825162653\n",
      "Epoch 64 loss = 19.769558089785278\n",
      "Epoch 65 loss = 8.031806251266971\n",
      "Epoch 66 loss = 6.8842518598539755\n",
      "Epoch 67 loss = 6.117898587137461\n",
      "Epoch 68 loss = 5.8591331308707595\n",
      "Epoch 69 loss = 3.542390557762701\n",
      "Epoch 70 loss = 14.470347541675437\n",
      "Epoch 71 loss = 8.51938004483236\n",
      "Epoch 72 loss = 4.182998319272883\n",
      "Epoch 73 loss = 2.651830873102881\n",
      "Epoch 74 loss = 1.7161602322594263\n",
      "Epoch 75 loss = 16.01654776475334\n",
      "Epoch 76 loss = 5.953169251966756\n",
      "Epoch 77 loss = 2.997427246242296\n",
      "Epoch 78 loss = 9.255936907837167\n",
      "Epoch 79 loss = 14.021566861658357\n",
      "Epoch 80 loss = 7.792811411316507\n",
      "Epoch 81 loss = 2.29853199608624\n",
      "Epoch 82 loss = 3.015809479460586\n",
      "Epoch 83 loss = 1.5350954779423773\n",
      "Epoch 84 loss = 0.8139297832094599\n",
      "Epoch 85 loss = 7.46518522471888\n",
      "Epoch 86 loss = 18.022797207639087\n",
      "Epoch 87 loss = 16.096968480589567\n",
      "Epoch 88 loss = 4.842176103615202\n",
      "Epoch 89 loss = 11.231128733430523\n",
      "Epoch 90 loss = 6.885814970242791\n",
      "Epoch 91 loss = 3.763526126218494\n",
      "Epoch 92 loss = 4.0822326296474785\n",
      "Epoch 93 loss = 8.046630994882435\n",
      "Epoch 94 loss = 3.4680433393805288\n",
      "Epoch 95 loss = 1.4004087385255843\n",
      "Epoch 96 loss = 1.8394939813879319\n",
      "Epoch 97 loss = 0.8679366677533835\n",
      "Epoch 98 loss = 0.4531381952401716\n",
      "Epoch 99 loss = 0.32211697948514484\n"
     ]
    }
   ],
   "source": [
    "train_model(MLP, Variable(train_input.view(train_input.size(0),-1)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.455696202531644 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_mlp = compute_nb_errors(MLP, Variable(test_input.view(test_input.size(0),-1)), Variable(train_target), 4)\n",
    "print(100*(train_error_mlp/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.999999999999996 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_mlp = compute_nb_errors(MLP, Variable(test_input.view(test_input.size(0),-1)), Variable(test_target), 4)\n",
    "print(100*(test_error_mlp/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Basic_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Basic_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(640, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.fc1(x.view(-1, 640)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = Basic_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 54.498005986213684\n",
      "Epoch 1 loss = 53.519576370716095\n",
      "Epoch 2 loss = 52.730925261974335\n",
      "Epoch 3 loss = 52.011470079422\n",
      "Epoch 4 loss = 51.306885719299316\n",
      "Epoch 5 loss = 50.57355135679245\n",
      "Epoch 6 loss = 49.77597540616989\n",
      "Epoch 7 loss = 48.90287631750107\n",
      "Epoch 8 loss = 47.96535176038742\n",
      "Epoch 9 loss = 46.977213621139526\n",
      "Epoch 10 loss = 45.9560324549675\n",
      "Epoch 11 loss = 44.91541266441345\n",
      "Epoch 12 loss = 43.863889276981354\n",
      "Epoch 13 loss = 42.80109569430351\n",
      "Epoch 14 loss = 41.72726050019264\n",
      "Epoch 15 loss = 40.64559432864189\n",
      "Epoch 16 loss = 39.55902707576752\n",
      "Epoch 17 loss = 38.47445222735405\n",
      "Epoch 18 loss = 37.40180164575577\n",
      "Epoch 19 loss = 36.34577456116676\n",
      "Epoch 20 loss = 35.31309676170349\n",
      "Epoch 21 loss = 34.30517861247063\n",
      "Epoch 22 loss = 33.31542217731476\n",
      "Epoch 23 loss = 32.36064922809601\n",
      "Epoch 24 loss = 31.455952614545822\n",
      "Epoch 25 loss = 30.621112316846848\n",
      "Epoch 26 loss = 29.83599954843521\n",
      "Epoch 27 loss = 29.091609865427017\n",
      "Epoch 28 loss = 28.385593682527542\n",
      "Epoch 29 loss = 27.71784856915474\n",
      "Epoch 30 loss = 27.08013343811035\n",
      "Epoch 31 loss = 26.471470445394516\n",
      "Epoch 32 loss = 25.892269998788834\n",
      "Epoch 33 loss = 25.33688971400261\n",
      "Epoch 34 loss = 24.80695317685604\n",
      "Epoch 35 loss = 24.297536432743073\n",
      "Epoch 36 loss = 23.8085807710886\n",
      "Epoch 37 loss = 23.3407344520092\n",
      "Epoch 38 loss = 22.890871182084084\n",
      "Epoch 39 loss = 22.462048381567\n",
      "Epoch 40 loss = 22.04953905940056\n",
      "Epoch 41 loss = 21.65613017976284\n",
      "Epoch 42 loss = 21.275663554668427\n",
      "Epoch 43 loss = 20.906475737690926\n",
      "Epoch 44 loss = 20.54767796397209\n",
      "Epoch 45 loss = 20.202588871121407\n",
      "Epoch 46 loss = 19.865014672279358\n",
      "Epoch 47 loss = 19.53286224603653\n",
      "Epoch 48 loss = 19.20836329460144\n",
      "Epoch 49 loss = 18.891260266304016\n",
      "Epoch 50 loss = 18.583666563034058\n",
      "Epoch 51 loss = 18.287342324852943\n",
      "Epoch 52 loss = 18.00163134932518\n",
      "Epoch 53 loss = 17.72761470079422\n",
      "Epoch 54 loss = 17.46428184211254\n",
      "Epoch 55 loss = 17.210159450769424\n",
      "Epoch 56 loss = 16.9658125936985\n",
      "Epoch 57 loss = 16.72996048629284\n",
      "Epoch 58 loss = 16.501679465174675\n",
      "Epoch 59 loss = 16.28134447336197\n",
      "Epoch 60 loss = 16.06744134426117\n",
      "Epoch 61 loss = 15.863640859723091\n",
      "Epoch 62 loss = 15.66768342256546\n",
      "Epoch 63 loss = 15.479718893766403\n",
      "Epoch 64 loss = 15.301828876137733\n",
      "Epoch 65 loss = 15.132765635848045\n",
      "Epoch 66 loss = 14.971958667039871\n",
      "Epoch 67 loss = 14.817156955599785\n",
      "Epoch 68 loss = 14.671055093407631\n",
      "Epoch 69 loss = 14.533242762088776\n",
      "Epoch 70 loss = 14.402028396725655\n",
      "Epoch 71 loss = 14.277263030409813\n",
      "Epoch 72 loss = 14.158192783594131\n",
      "Epoch 73 loss = 14.04104134440422\n",
      "Epoch 74 loss = 13.922865718603134\n",
      "Epoch 75 loss = 13.79942874610424\n",
      "Epoch 76 loss = 13.673272088170052\n",
      "Epoch 77 loss = 13.547573789954185\n",
      "Epoch 78 loss = 13.423323675990105\n",
      "Epoch 79 loss = 13.303480252623558\n",
      "Epoch 80 loss = 13.190399900078773\n",
      "Epoch 81 loss = 13.084183022379875\n",
      "Epoch 82 loss = 12.985377386212349\n",
      "Epoch 83 loss = 12.893768697977066\n",
      "Epoch 84 loss = 12.808810248970985\n",
      "Epoch 85 loss = 12.729156851768494\n",
      "Epoch 86 loss = 12.655004635453224\n",
      "Epoch 87 loss = 12.585136085748672\n",
      "Epoch 88 loss = 12.519690230488777\n",
      "Epoch 89 loss = 12.458024248480797\n",
      "Epoch 90 loss = 12.399600893259048\n",
      "Epoch 91 loss = 12.344199568033218\n",
      "Epoch 92 loss = 12.291459187865257\n",
      "Epoch 93 loss = 12.241384372115135\n",
      "Epoch 94 loss = 12.193539321422577\n",
      "Epoch 95 loss = 12.148138523101807\n",
      "Epoch 96 loss = 12.104768931865692\n",
      "Epoch 97 loss = 12.063279837369919\n",
      "Epoch 98 loss = 12.023726433515549\n",
      "Epoch 99 loss = 11.985645055770874\n"
     ]
    }
   ],
   "source": [
    "train_model(cnn, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31645569620253167 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn = compute_nb_errors(cnn, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "print(100*(train_error_cnn/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.7 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn = compute_nb_errors(cnn, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)\n",
    "print(100*(test_error_cnn/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN w 1D filters\n",
    "\n",
    "v1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_1D_conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_1D_conv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1,5))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1,3))\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(1,5))\n",
    "        self.fc1 = nn.Linear(896*2, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=(1,2), stride=(1,2)))\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=(1,3), stride=(1,3)))\n",
    "        x = F.tanh(F.max_pool2d(self.conv3(x), kernel_size=(1,3), stride=(1,3)))\n",
    "        x = F.tanh(self.fc1(x.view(-1, 896*2)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 55.29474902153015\n",
      "Epoch 1 loss = 55.05597680807114\n",
      "Epoch 2 loss = 54.855808675289154\n",
      "Epoch 3 loss = 54.682802975177765\n",
      "Epoch 4 loss = 54.524170219898224\n",
      "Epoch 5 loss = 54.372238516807556\n",
      "Epoch 6 loss = 54.2225626707077\n",
      "Epoch 7 loss = 54.07223069667816\n",
      "Epoch 8 loss = 53.92045372724533\n",
      "Epoch 9 loss = 53.7664595246315\n",
      "Epoch 10 loss = 53.6088582277298\n",
      "Epoch 11 loss = 53.44771748781204\n",
      "Epoch 12 loss = 53.28130882978439\n",
      "Epoch 13 loss = 53.10787773132324\n",
      "Epoch 14 loss = 52.924775540828705\n",
      "Epoch 15 loss = 52.73032104969025\n",
      "Epoch 16 loss = 52.52050358057022\n",
      "Epoch 17 loss = 52.2923142015934\n",
      "Epoch 18 loss = 52.04249811172485\n",
      "Epoch 19 loss = 51.76665839552879\n",
      "Epoch 20 loss = 51.46092280745506\n",
      "Epoch 21 loss = 51.12171161174774\n",
      "Epoch 22 loss = 50.74609687924385\n",
      "Epoch 23 loss = 50.327125549316406\n",
      "Epoch 24 loss = 49.860064417123795\n",
      "Epoch 25 loss = 49.33736914396286\n",
      "Epoch 26 loss = 48.7524134516716\n",
      "Epoch 27 loss = 48.09661191701889\n",
      "Epoch 28 loss = 47.363799035549164\n",
      "Epoch 29 loss = 46.548118352890015\n",
      "Epoch 30 loss = 45.64110191166401\n",
      "Epoch 31 loss = 44.62833517789841\n",
      "Epoch 32 loss = 43.5095302015543\n",
      "Epoch 33 loss = 42.288945361971855\n",
      "Epoch 34 loss = 40.98556010425091\n",
      "Epoch 35 loss = 39.624012991786\n",
      "Epoch 36 loss = 38.224144004285336\n",
      "Epoch 37 loss = 36.83618910610676\n",
      "Epoch 38 loss = 35.50140971690416\n",
      "Epoch 39 loss = 34.220379054546356\n",
      "Epoch 40 loss = 32.987858686596155\n",
      "Epoch 41 loss = 31.78924137726426\n",
      "Epoch 42 loss = 30.60187002643943\n",
      "Epoch 43 loss = 29.414099875837564\n",
      "Epoch 44 loss = 28.20890563353896\n",
      "Epoch 45 loss = 26.9710533618927\n",
      "Epoch 46 loss = 25.724096946418285\n",
      "Epoch 47 loss = 24.45225503668189\n",
      "Epoch 48 loss = 23.141171157360077\n",
      "Epoch 49 loss = 21.80590709671378\n",
      "Epoch 50 loss = 20.482658119872212\n",
      "Epoch 51 loss = 19.184796921908855\n",
      "Epoch 52 loss = 17.91424261406064\n",
      "Epoch 53 loss = 16.68403071537614\n",
      "Epoch 54 loss = 15.473992392420769\n",
      "Epoch 55 loss = 14.307200957089663\n",
      "Epoch 56 loss = 13.158826867118478\n",
      "Epoch 57 loss = 12.065468588843942\n",
      "Epoch 58 loss = 11.004322789609432\n",
      "Epoch 59 loss = 10.01800771523267\n",
      "Epoch 60 loss = 9.092921589501202\n",
      "Epoch 61 loss = 8.246936591342092\n",
      "Epoch 62 loss = 7.473065987229347\n",
      "Epoch 63 loss = 6.765459856018424\n",
      "Epoch 64 loss = 6.132833236362785\n",
      "Epoch 65 loss = 5.56078403769061\n",
      "Epoch 66 loss = 5.051177802029997\n",
      "Epoch 67 loss = 4.5967433960177\n",
      "Epoch 68 loss = 4.186151412315667\n",
      "Epoch 69 loss = 3.8177803866565228\n",
      "Epoch 70 loss = 3.485889774048701\n",
      "Epoch 71 loss = 3.189264864195138\n",
      "Epoch 72 loss = 2.920124291209504\n",
      "Epoch 73 loss = 2.6755923714954406\n",
      "Epoch 74 loss = 2.4577202387154102\n",
      "Epoch 75 loss = 2.259887665626593\n",
      "Epoch 76 loss = 2.082024419447407\n",
      "Epoch 77 loss = 1.9214594420045614\n",
      "Epoch 78 loss = 1.7766964577604085\n",
      "Epoch 79 loss = 1.6465725870220922\n",
      "Epoch 80 loss = 1.5289256813121028\n",
      "Epoch 81 loss = 1.422422197414562\n",
      "Epoch 82 loss = 1.3253437340026721\n",
      "Epoch 83 loss = 1.238339542527683\n",
      "Epoch 84 loss = 1.1580220417818055\n",
      "Epoch 85 loss = 1.085460348345805\n",
      "Epoch 86 loss = 1.019526947668055\n",
      "Epoch 87 loss = 0.9589260969660245\n",
      "Epoch 88 loss = 0.9038629011774901\n",
      "Epoch 89 loss = 0.8529104291810654\n",
      "Epoch 90 loss = 0.8064452380058356\n",
      "Epoch 91 loss = 0.7638936454895884\n",
      "Epoch 92 loss = 0.7245852873311378\n",
      "Epoch 93 loss = 0.6883455333882011\n",
      "Epoch 94 loss = 0.6548019158944953\n",
      "Epoch 95 loss = 0.6237556178239174\n",
      "Epoch 96 loss = 0.5950728290190455\n",
      "Epoch 97 loss = 0.5683711224410217\n",
      "Epoch 98 loss = 0.5436698693956714\n",
      "Epoch 99 loss = 0.5205931734817568\n"
     ]
    }
   ],
   "source": [
    "train_model(cnn2, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn2 = compute_nb_errors(cnn2, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "print(100*(train_error_cnn2/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn2 = compute_nb_errors(cnn2, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)\n",
    "print(100*(test_error_cnn2/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 56, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(56, 112, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(112, 112, kernel_size=5)\n",
    "        #self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(112, 56)\n",
    "        self.fc2 = nn.Linear(56, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.conv2(x), kernel_size=3, stride=3))\n",
    "        x = F.tanh(F.max_pool1d(self.conv3(x), kernel_size=3, stride=3))\n",
    "        #x = self.dropout(x)\n",
    "        x = F.tanh(self.fc1(x.view(-1, 112)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 54.80717808008194\n",
      "Epoch 1 loss = 54.70321995019913\n",
      "Epoch 2 loss = 54.62144333124161\n",
      "Epoch 3 loss = 54.54609006643295\n",
      "Epoch 4 loss = 54.47215002775192\n",
      "Epoch 5 loss = 54.397279500961304\n",
      "Epoch 6 loss = 54.31979596614838\n",
      "Epoch 7 loss = 54.23857867717743\n",
      "Epoch 8 loss = 54.152719497680664\n",
      "Epoch 9 loss = 54.06127589941025\n",
      "Epoch 10 loss = 53.963149666786194\n",
      "Epoch 11 loss = 53.85764080286026\n",
      "Epoch 12 loss = 53.74411725997925\n",
      "Epoch 13 loss = 53.621960520744324\n",
      "Epoch 14 loss = 53.490806221961975\n",
      "Epoch 15 loss = 53.3504199385643\n",
      "Epoch 16 loss = 53.20103591680527\n",
      "Epoch 17 loss = 53.042898178100586\n",
      "Epoch 18 loss = 52.87592488527298\n",
      "Epoch 19 loss = 52.69968330860138\n",
      "Epoch 20 loss = 52.512857258319855\n",
      "Epoch 21 loss = 52.313249707221985\n",
      "Epoch 22 loss = 52.09775102138519\n",
      "Epoch 23 loss = 51.862221360206604\n",
      "Epoch 24 loss = 51.600734174251556\n",
      "Epoch 25 loss = 51.30662363767624\n",
      "Epoch 26 loss = 50.97217923402786\n",
      "Epoch 27 loss = 50.58827191591263\n",
      "Epoch 28 loss = 50.145546674728394\n",
      "Epoch 29 loss = 49.635470151901245\n",
      "Epoch 30 loss = 49.052971839904785\n",
      "Epoch 31 loss = 48.39720916748047\n",
      "Epoch 32 loss = 47.67743617296219\n",
      "Epoch 33 loss = 46.91091322898865\n",
      "Epoch 34 loss = 46.12178260087967\n",
      "Epoch 35 loss = 45.33747237920761\n",
      "Epoch 36 loss = 44.585057497024536\n",
      "Epoch 37 loss = 43.88276046514511\n",
      "Epoch 38 loss = 43.23992794752121\n",
      "Epoch 39 loss = 42.65469256043434\n",
      "Epoch 40 loss = 42.11176699399948\n",
      "Epoch 41 loss = 41.59381613135338\n",
      "Epoch 42 loss = 41.08444428443909\n",
      "Epoch 43 loss = 40.574191838502884\n",
      "Epoch 44 loss = 40.05659916996956\n",
      "Epoch 45 loss = 39.525228440761566\n",
      "Epoch 46 loss = 38.97643005847931\n",
      "Epoch 47 loss = 38.40751928091049\n",
      "Epoch 48 loss = 37.81886240839958\n",
      "Epoch 49 loss = 37.20732679963112\n",
      "Epoch 50 loss = 36.56616833806038\n",
      "Epoch 51 loss = 35.89628407359123\n",
      "Epoch 52 loss = 35.19750505685806\n",
      "Epoch 53 loss = 34.467127710580826\n",
      "Epoch 54 loss = 33.70280182361603\n",
      "Epoch 55 loss = 32.90799552202225\n",
      "Epoch 56 loss = 32.079257011413574\n",
      "Epoch 57 loss = 31.2220561504364\n",
      "Epoch 58 loss = 30.336877077817917\n",
      "Epoch 59 loss = 29.43371231853962\n",
      "Epoch 60 loss = 28.521252885460854\n",
      "Epoch 61 loss = 27.609381422400475\n",
      "Epoch 62 loss = 26.705064982175827\n",
      "Epoch 63 loss = 25.81715965270996\n",
      "Epoch 64 loss = 24.946798160672188\n",
      "Epoch 65 loss = 24.0969417989254\n",
      "Epoch 66 loss = 23.268715247511864\n",
      "Epoch 67 loss = 22.465265810489655\n",
      "Epoch 68 loss = 21.68551456928253\n",
      "Epoch 69 loss = 20.929760172963142\n",
      "Epoch 70 loss = 20.197027780115604\n",
      "Epoch 71 loss = 19.48685085773468\n",
      "Epoch 72 loss = 18.79366946965456\n",
      "Epoch 73 loss = 18.11371350288391\n",
      "Epoch 74 loss = 17.448977544903755\n",
      "Epoch 75 loss = 16.79688136279583\n",
      "Epoch 76 loss = 16.153860576450825\n",
      "Epoch 77 loss = 15.521179631352425\n",
      "Epoch 78 loss = 14.890969168394804\n",
      "Epoch 79 loss = 14.270723462104797\n",
      "Epoch 80 loss = 13.655808195471764\n",
      "Epoch 81 loss = 13.047254204750061\n",
      "Epoch 82 loss = 12.443563140928745\n",
      "Epoch 83 loss = 11.85163190215826\n",
      "Epoch 84 loss = 11.260812729597092\n",
      "Epoch 85 loss = 10.682115264236927\n",
      "Epoch 86 loss = 10.107771836221218\n",
      "Epoch 87 loss = 9.542313646525145\n",
      "Epoch 88 loss = 8.990537386387587\n",
      "Epoch 89 loss = 8.456313788890839\n",
      "Epoch 90 loss = 7.941506223753095\n",
      "Epoch 91 loss = 7.44299029186368\n",
      "Epoch 92 loss = 6.962817462161183\n",
      "Epoch 93 loss = 6.505012100562453\n",
      "Epoch 94 loss = 6.072084203362465\n",
      "Epoch 95 loss = 5.656321510672569\n",
      "Epoch 96 loss = 5.265139950439334\n",
      "Epoch 97 loss = 4.898181421682239\n",
      "Epoch 98 loss = 4.551967371255159\n",
      "Epoch 99 loss = 4.229025145992637\n",
      "Epoch 100 loss = 3.929819437675178\n",
      "Epoch 101 loss = 3.6509207440540195\n",
      "Epoch 102 loss = 3.3982019387185574\n",
      "Epoch 103 loss = 3.1606581825762987\n",
      "Epoch 104 loss = 2.943011302500963\n",
      "Epoch 105 loss = 2.744522529654205\n",
      "Epoch 106 loss = 2.5625202348455787\n",
      "Epoch 107 loss = 2.3965079355984926\n",
      "Epoch 108 loss = 2.244069124571979\n",
      "Epoch 109 loss = 2.1043527480214834\n",
      "Epoch 110 loss = 1.9743241369724274\n",
      "Epoch 111 loss = 1.8554803794249892\n",
      "Epoch 112 loss = 1.7458152016624808\n",
      "Epoch 113 loss = 1.645016006194055\n",
      "Epoch 114 loss = 1.5512454747222364\n",
      "Epoch 115 loss = 1.465625619981438\n",
      "Epoch 116 loss = 1.3858725586906075\n",
      "Epoch 117 loss = 1.3124052952043712\n",
      "Epoch 118 loss = 1.2445298633538187\n",
      "Epoch 119 loss = 1.1818531164899468\n"
     ]
    }
   ],
   "source": [
    "cnn4 = CNN_1D()\n",
    "train_model(cnn4, Variable(train_input), Variable(train_target), 40, nb_epochs=120, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09493670886075949 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn4 = compute_nb_errors(cnn4, Variable(train_input), Variable(train_target), 40)\n",
    "print(100*(train_error_cnn4/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.5 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn4 = compute_nb_errors(cnn4, Variable(test_input), Variable(test_target), 40)\n",
    "print(100*(1-(test_error_cnn4/test_input.size(0))),'% test accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### previous + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_dropout, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 56, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(56, 112, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(112, 112, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(112, 56)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(56, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.conv2(x), kernel_size=3, stride=3))\n",
    "        x = F.tanh(F.max_pool1d(self.conv3(x), kernel_size=3, stride=3))\n",
    "        x = self.fc1(x.view(-1, 112))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(F.tanh(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 54.81373769044876\n",
      "Epoch 1 loss = 54.668377578258514\n",
      "Epoch 2 loss = 54.62901657819748\n",
      "Epoch 3 loss = 54.515659153461456\n",
      "Epoch 4 loss = 54.52439606189728\n",
      "Epoch 5 loss = 54.237469136714935\n",
      "Epoch 6 loss = 54.175403356552124\n",
      "Epoch 7 loss = 53.991609275341034\n",
      "Epoch 8 loss = 54.01105797290802\n",
      "Epoch 9 loss = 53.85242795944214\n",
      "Epoch 10 loss = 53.63609594106674\n",
      "Epoch 11 loss = 53.5738622546196\n",
      "Epoch 12 loss = 53.413128554821014\n",
      "Epoch 13 loss = 53.16618990898132\n",
      "Epoch 14 loss = 53.06097322702408\n",
      "Epoch 15 loss = 52.77126497030258\n",
      "Epoch 16 loss = 52.46380579471588\n",
      "Epoch 17 loss = 52.251897275447845\n",
      "Epoch 18 loss = 52.10526090860367\n",
      "Epoch 19 loss = 51.59744983911514\n",
      "Epoch 20 loss = 51.36152005195618\n",
      "Epoch 21 loss = 51.03365182876587\n",
      "Epoch 22 loss = 50.29474097490311\n",
      "Epoch 23 loss = 49.955027401447296\n",
      "Epoch 24 loss = 49.26357388496399\n",
      "Epoch 25 loss = 48.516477048397064\n",
      "Epoch 26 loss = 47.920060098171234\n",
      "Epoch 27 loss = 47.34015101194382\n",
      "Epoch 28 loss = 46.479187071323395\n",
      "Epoch 29 loss = 45.700314462184906\n",
      "Epoch 30 loss = 44.72383964061737\n",
      "Epoch 31 loss = 44.17614609003067\n",
      "Epoch 32 loss = 43.38053569197655\n",
      "Epoch 33 loss = 42.77121365070343\n",
      "Epoch 34 loss = 42.21555379033089\n",
      "Epoch 35 loss = 41.67405363917351\n",
      "Epoch 36 loss = 41.0138064622879\n",
      "Epoch 37 loss = 40.74573215842247\n",
      "Epoch 38 loss = 39.836212903261185\n",
      "Epoch 39 loss = 39.55124592781067\n",
      "Epoch 40 loss = 38.96957564353943\n",
      "Epoch 41 loss = 38.38907963037491\n",
      "Epoch 42 loss = 37.75908660888672\n",
      "Epoch 43 loss = 37.14352938532829\n",
      "Epoch 44 loss = 36.45495381951332\n",
      "Epoch 45 loss = 35.88319593667984\n",
      "Epoch 46 loss = 34.8772489130497\n",
      "Epoch 47 loss = 34.17909014225006\n",
      "Epoch 48 loss = 33.52619290351868\n",
      "Epoch 49 loss = 32.5400710105896\n",
      "Epoch 50 loss = 31.758291870355606\n",
      "Epoch 51 loss = 31.05525651574135\n",
      "Epoch 52 loss = 30.072126001119614\n",
      "Epoch 53 loss = 29.31058633327484\n",
      "Epoch 54 loss = 28.287872210144997\n",
      "Epoch 55 loss = 27.471184015274048\n",
      "Epoch 56 loss = 26.46474976837635\n",
      "Epoch 57 loss = 25.680248767137527\n",
      "Epoch 58 loss = 24.95379027724266\n",
      "Epoch 59 loss = 24.195579141378403\n",
      "Epoch 60 loss = 23.45066736638546\n",
      "Epoch 61 loss = 22.522661462426186\n",
      "Epoch 62 loss = 21.853763937950134\n",
      "Epoch 63 loss = 20.866757094860077\n",
      "Epoch 64 loss = 20.337877228856087\n",
      "Epoch 65 loss = 19.709520764648914\n",
      "Epoch 66 loss = 18.942652329802513\n",
      "Epoch 67 loss = 18.20479527115822\n",
      "Epoch 68 loss = 17.33526001125574\n",
      "Epoch 69 loss = 16.81096263974905\n",
      "Epoch 70 loss = 16.007733650505543\n",
      "Epoch 71 loss = 15.481175780296326\n",
      "Epoch 72 loss = 15.118314653635025\n",
      "Epoch 73 loss = 14.511324524879456\n",
      "Epoch 74 loss = 13.760365635156631\n",
      "Epoch 75 loss = 13.099373191595078\n",
      "Epoch 76 loss = 12.453268885612488\n",
      "Epoch 77 loss = 11.958859086036682\n",
      "Epoch 78 loss = 11.293652899563313\n",
      "Epoch 79 loss = 10.891858391463757\n",
      "Epoch 80 loss = 10.670656442642212\n",
      "Epoch 81 loss = 10.054422814399004\n",
      "Epoch 82 loss = 9.269512955099344\n",
      "Epoch 83 loss = 8.906087685376406\n",
      "Epoch 84 loss = 8.168687107041478\n",
      "Epoch 85 loss = 7.794401986524463\n",
      "Epoch 86 loss = 7.4571825712919235\n",
      "Epoch 87 loss = 7.203381430357695\n",
      "Epoch 88 loss = 6.675417762249708\n",
      "Epoch 89 loss = 6.440882029011846\n",
      "Epoch 90 loss = 5.986491912975907\n",
      "Epoch 91 loss = 5.671619711443782\n",
      "Epoch 92 loss = 5.2231133952736855\n",
      "Epoch 93 loss = 4.983916163444519\n",
      "Epoch 94 loss = 4.549118289723992\n",
      "Epoch 95 loss = 4.518581433221698\n",
      "Epoch 96 loss = 4.184387966059148\n",
      "Epoch 97 loss = 3.8590672304853797\n",
      "Epoch 98 loss = 3.671477422118187\n",
      "Epoch 99 loss = 3.4763677520677447\n",
      "Epoch 100 loss = 3.1308714458718896\n",
      "Epoch 101 loss = 2.99355923011899\n",
      "Epoch 102 loss = 2.7842510994523764\n",
      "Epoch 103 loss = 5.583516075275838\n",
      "Epoch 104 loss = 2.4561389591544867\n",
      "Epoch 105 loss = 2.333383840508759\n",
      "Epoch 106 loss = 2.210252503864467\n",
      "Epoch 107 loss = 2.144850231707096\n",
      "Epoch 108 loss = 1.9758856864646077\n",
      "Epoch 109 loss = 1.8891642959788442\n",
      "Epoch 110 loss = 1.7669282350689173\n",
      "Epoch 111 loss = 1.61881736619398\n",
      "Epoch 112 loss = 1.5223373123444617\n",
      "Epoch 113 loss = 1.5418616137467325\n",
      "Epoch 114 loss = 1.4277844629250467\n",
      "Epoch 115 loss = 1.3539427537471056\n",
      "Epoch 116 loss = 1.3008825723081827\n",
      "Epoch 117 loss = 1.2581399232149124\n",
      "Epoch 118 loss = 1.2164256237447262\n",
      "Epoch 119 loss = 1.072476220317185\n",
      "Epoch 120 loss = 1.071033216547221\n",
      "Epoch 121 loss = 1.0725259415339679\n",
      "Epoch 122 loss = 0.9851158270612359\n",
      "Epoch 123 loss = 0.9462492645252496\n",
      "Epoch 124 loss = 0.9438748294487596\n",
      "Epoch 125 loss = 0.9186782687902451\n",
      "Epoch 126 loss = 0.8748916639015079\n",
      "Epoch 127 loss = 0.8157111203763634\n",
      "Epoch 128 loss = 0.8245638785883784\n",
      "Epoch 129 loss = 0.7964270401280373\n",
      "Epoch 130 loss = 0.7138981120660901\n",
      "Epoch 131 loss = 0.6987031803000718\n",
      "Epoch 132 loss = 0.7169414346572012\n",
      "Epoch 133 loss = 0.6530320809688419\n",
      "Epoch 134 loss = 0.6218856945633888\n",
      "Epoch 135 loss = 0.6409058675635606\n",
      "Epoch 136 loss = 0.59526975476183\n",
      "Epoch 137 loss = 0.6066255230689421\n",
      "Epoch 138 loss = 0.5718848146498203\n",
      "Epoch 139 loss = 0.5297152248676866\n"
     ]
    }
   ],
   "source": [
    "cnn3 = CNN_dropout()\n",
    "train_model(cnn3, Variable(train_input), Variable(train_target), 20, nb_epochs=140, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn3 = compute_nb_errors(cnn3, Variable(train_input), Variable(train_target), 40)\n",
    "print(100*(train_error_cnn3/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.1 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn3 = compute_nb_errors(cnn3, Variable(test_input), Variable(test_target))\n",
    "print(100*(1-(test_error_cnn3/test_input.size(0))),'% test accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_sizes = [20, 40]\n",
    "learning_rates = [0.01, 0.0075, 0.005, 0.0025, 0.001]\n",
    "nb_epochs = [100, 110, 120, 130, 140, 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT DROPOUT\n",
      "Batch size 20\n",
      "==============\n",
      "     100 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.26239770650864\n",
      "            Epoch 10 loss = 87.35559633374214\n",
      "            Epoch 20 loss = 29.273580979555845\n",
      "            Epoch 30 loss = 3.4526535307522863\n",
      "            Epoch 40 loss = 0.8238349893363193\n",
      "            Epoch 50 loss = 0.36731731047620997\n",
      "            Epoch 60 loss = 0.22038837501895614\n",
      "            Epoch 70 loss = 0.1540524371812353\n",
      "            Epoch 80 loss = 0.11730458836245816\n",
      "            Epoch 90 loss = 0.09412753221113235\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.53126090765\n",
      "            Epoch 10 loss = 98.59997701644897\n",
      "            Epoch 20 loss = 60.63624921441078\n",
      "            Epoch 30 loss = 18.417776821181178\n",
      "            Epoch 40 loss = 3.300629359902814\n",
      "            Epoch 50 loss = 1.032872662995942\n",
      "            Epoch 60 loss = 0.49333338884753175\n",
      "            Epoch 70 loss = 0.29543064860627055\n",
      "            Epoch 80 loss = 0.20496040883881506\n",
      "            Epoch 90 loss = 0.15500706841703504\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.59150433540344\n",
      "            Epoch 10 loss = 103.38775956630707\n",
      "            Epoch 20 loss = 81.07911455631256\n",
      "            Epoch 30 loss = 46.00190983712673\n",
      "            Epoch 40 loss = 18.180700791999698\n",
      "            Epoch 50 loss = 5.071393890772015\n",
      "            Epoch 60 loss = 1.9003021612297744\n",
      "            Epoch 70 loss = 0.8992983910720795\n",
      "            Epoch 80 loss = 0.5457645792630501\n",
      "            Epoch 90 loss = 0.37807772649102844\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.49709230661392\n",
      "            Epoch 10 loss = 107.37256002426147\n",
      "            Epoch 20 loss = 102.59197014570236\n",
      "            Epoch 30 loss = 90.07705864310265\n",
      "            Epoch 40 loss = 78.27435711026192\n",
      "            Epoch 50 loss = 64.80661743879318\n",
      "            Epoch 60 loss = 47.093069434165955\n",
      "            Epoch 70 loss = 30.86269821226597\n",
      "            Epoch 80 loss = 17.84489591792226\n",
      "            Epoch 90 loss = 8.965906047262251\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.52510005235672\n",
      "            Epoch 10 loss = 108.94973629713058\n",
      "            Epoch 20 loss = 108.36450803279877\n",
      "            Epoch 30 loss = 107.59590619802475\n",
      "            Epoch 40 loss = 106.54223400354385\n",
      "            Epoch 50 loss = 105.13099944591522\n",
      "            Epoch 60 loss = 103.19310754537582\n",
      "            Epoch 70 loss = 100.09640657901764\n",
      "            Epoch 80 loss = 94.72407898306847\n",
      "            Epoch 90 loss = 87.48452967405319\n",
      "      For 100 epochs : [76.8, 73.8, 74.7, 73.4, 69.8]\n",
      "     110 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.51135629415512\n",
      "            Epoch 10 loss = 85.11633035540581\n",
      "            Epoch 20 loss = 27.73670467734337\n",
      "            Epoch 30 loss = 3.029438914731145\n",
      "            Epoch 40 loss = 0.7301638577482663\n",
      "            Epoch 50 loss = 0.3472374341217801\n",
      "            Epoch 60 loss = 0.2161354735144414\n",
      "            Epoch 70 loss = 0.1536087165150093\n",
      "            Epoch 80 loss = 0.11785333220905159\n",
      "            Epoch 90 loss = 0.09494049737986643\n",
      "            Epoch 100 loss = 0.07910489041387336\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.21122533082962\n",
      "            Epoch 10 loss = 90.68756893277168\n",
      "            Epoch 20 loss = 46.324748173356056\n",
      "            Epoch 30 loss = 10.60089641995728\n",
      "            Epoch 40 loss = 2.287317111855373\n",
      "            Epoch 50 loss = 0.7780623835278675\n",
      "            Epoch 60 loss = 0.39880555181298405\n",
      "            Epoch 70 loss = 0.2536020201223437\n",
      "            Epoch 80 loss = 0.18235580129839946\n",
      "            Epoch 90 loss = 0.14098363986704499\n",
      "            Epoch 100 loss = 0.11420035832270514\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.59313678741455\n",
      "            Epoch 10 loss = 103.1334639787674\n",
      "            Epoch 20 loss = 84.66455629467964\n",
      "            Epoch 30 loss = 55.275839656591415\n",
      "            Epoch 40 loss = 24.453213658183813\n",
      "            Epoch 50 loss = 7.159588501788676\n",
      "            Epoch 60 loss = 2.462039712932892\n",
      "            Epoch 70 loss = 1.2023000821936876\n",
      "            Epoch 80 loss = 0.684080587758217\n",
      "            Epoch 90 loss = 0.4453802878560964\n",
      "            Epoch 100 loss = 0.3213124443136621\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 110.26359337568283\n",
      "            Epoch 10 loss = 107.27280557155609\n",
      "            Epoch 20 loss = 103.78146809339523\n",
      "            Epoch 30 loss = 94.11041688919067\n",
      "            Epoch 40 loss = 80.10972613096237\n",
      "            Epoch 50 loss = 66.06647503376007\n",
      "            Epoch 60 loss = 48.04657384753227\n",
      "            Epoch 70 loss = 31.18941330909729\n",
      "            Epoch 80 loss = 17.922683896496892\n",
      "            Epoch 90 loss = 9.307389665395021\n",
      "            Epoch 100 loss = 4.871215416584164\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.42439287900925\n",
      "            Epoch 10 loss = 108.33290356397629\n",
      "            Epoch 20 loss = 107.26255488395691\n",
      "            Epoch 30 loss = 105.97636622190475\n",
      "            Epoch 40 loss = 104.40896564722061\n",
      "            Epoch 50 loss = 102.13122421503067\n",
      "            Epoch 60 loss = 98.1102322936058\n",
      "            Epoch 70 loss = 91.41327068209648\n",
      "            Epoch 80 loss = 84.05899116396904\n",
      "            Epoch 90 loss = 78.43948689103127\n",
      "            Epoch 100 loss = 73.28005756437778\n",
      "      For 110 epochs : [73.8, 72.2, 71.6, 71.7, 71.50000000000001]\n",
      "     120 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.33954054117203\n",
      "            Epoch 10 loss = 88.95104831457138\n",
      "            Epoch 20 loss = 29.583589512854815\n",
      "            Epoch 30 loss = 3.576523730996996\n",
      "            Epoch 40 loss = 0.8272107474622317\n",
      "            Epoch 50 loss = 0.34217565951985307\n",
      "            Epoch 60 loss = 0.20117059792391956\n",
      "            Epoch 70 loss = 0.13983733508212026\n",
      "            Epoch 80 loss = 0.10613988096883986\n",
      "            Epoch 90 loss = 0.08495210919500096\n",
      "            Epoch 100 loss = 0.07050064733630279\n",
      "            Epoch 110 loss = 0.06004626919457223\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.3211213350296\n",
      "            Epoch 10 loss = 95.12313133478165\n",
      "            Epoch 20 loss = 51.758888438344\n",
      "            Epoch 30 loss = 12.688811164349318\n",
      "            Epoch 40 loss = 2.4692856492474675\n",
      "            Epoch 50 loss = 0.8286505435826257\n",
      "            Epoch 60 loss = 0.41267726686783135\n",
      "            Epoch 70 loss = 0.25525985716376454\n",
      "            Epoch 80 loss = 0.18013736401917413\n",
      "            Epoch 90 loss = 0.13755597069393843\n",
      "            Epoch 100 loss = 0.11049488466233015\n",
      "            Epoch 110 loss = 0.09190730296541005\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.59779751300812\n",
      "            Epoch 10 loss = 103.71494746208191\n",
      "            Epoch 20 loss = 83.46086114645004\n",
      "            Epoch 30 loss = 52.33801659941673\n",
      "            Epoch 40 loss = 22.896995320916176\n",
      "            Epoch 50 loss = 6.606267931871116\n",
      "            Epoch 60 loss = 2.0875947633758187\n",
      "            Epoch 70 loss = 0.9650038053514436\n",
      "            Epoch 80 loss = 0.5670202603796497\n",
      "            Epoch 90 loss = 0.38511064572958276\n",
      "            Epoch 100 loss = 0.28549248341005296\n",
      "            Epoch 110 loss = 0.2240935679874383\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.52622425556183\n",
      "            Epoch 10 loss = 107.47157788276672\n",
      "            Epoch 20 loss = 103.63430446386337\n",
      "            Epoch 30 loss = 93.16664415597916\n",
      "            Epoch 40 loss = 79.94392976164818\n",
      "            Epoch 50 loss = 65.77411982417107\n",
      "            Epoch 60 loss = 47.02631939202547\n",
      "            Epoch 70 loss = 29.89090909808874\n",
      "            Epoch 80 loss = 16.461278945207596\n",
      "            Epoch 90 loss = 8.051386162638664\n",
      "            Epoch 100 loss = 4.1116436258889735\n",
      "            Epoch 110 loss = 2.369678360177204\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.73817306756973\n",
      "            Epoch 10 loss = 108.92421078681946\n",
      "            Epoch 20 loss = 108.15279448032379\n",
      "            Epoch 30 loss = 107.17358458042145\n",
      "            Epoch 40 loss = 105.87871193885803\n",
      "            Epoch 50 loss = 104.10844349861145\n",
      "            Epoch 60 loss = 101.31773644685745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 70 loss = 96.2366686463356\n",
      "            Epoch 80 loss = 88.43559655547142\n",
      "            Epoch 90 loss = 81.34048029780388\n",
      "            Epoch 100 loss = 75.77934151887894\n",
      "            Epoch 110 loss = 70.32732009887695\n",
      "      For 120 epochs : [72.7, 73.8, 72.1, 76.3, 70.7]\n",
      "     130 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.41912245750427\n",
      "            Epoch 10 loss = 89.2061228454113\n",
      "            Epoch 20 loss = 29.748647063970566\n",
      "            Epoch 30 loss = 3.4543337021023035\n",
      "            Epoch 40 loss = 0.8478749920614064\n",
      "            Epoch 50 loss = 0.35630048502935097\n",
      "            Epoch 60 loss = 0.20666124668787234\n",
      "            Epoch 70 loss = 0.14177108833973762\n",
      "            Epoch 80 loss = 0.10677547605882864\n",
      "            Epoch 90 loss = 0.08511241612723097\n",
      "            Epoch 100 loss = 0.07045971160550835\n",
      "            Epoch 110 loss = 0.05992539228464011\n",
      "            Epoch 120 loss = 0.05200195697398158\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.15305829048157\n",
      "            Epoch 10 loss = 91.8991006910801\n",
      "            Epoch 20 loss = 47.86110046505928\n",
      "            Epoch 30 loss = 10.336570682935417\n",
      "            Epoch 40 loss = 2.109043473144993\n",
      "            Epoch 50 loss = 0.7552615426830016\n",
      "            Epoch 60 loss = 0.4068705244571902\n",
      "            Epoch 70 loss = 0.2659091118257493\n",
      "            Epoch 80 loss = 0.1936101824248908\n",
      "            Epoch 90 loss = 0.15059762910823338\n",
      "            Epoch 100 loss = 0.12239792483887868\n",
      "            Epoch 110 loss = 0.10262664058973314\n",
      "            Epoch 120 loss = 0.08802390179334907\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.51589125394821\n",
      "            Epoch 10 loss = 103.19141989946365\n",
      "            Epoch 20 loss = 83.09210923314095\n",
      "            Epoch 30 loss = 52.50960059463978\n",
      "            Epoch 40 loss = 22.076795529574156\n",
      "            Epoch 50 loss = 6.162675324827433\n",
      "            Epoch 60 loss = 2.302048502722755\n",
      "            Epoch 70 loss = 1.1022385370451957\n",
      "            Epoch 80 loss = 0.6438154260977171\n",
      "            Epoch 90 loss = 0.429407246701885\n",
      "            Epoch 100 loss = 0.3138679915864486\n",
      "            Epoch 110 loss = 0.24422541467356496\n",
      "            Epoch 120 loss = 0.19840114630642347\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.73865395784378\n",
      "            Epoch 10 loss = 107.27918821573257\n",
      "            Epoch 20 loss = 102.11931222677231\n",
      "            Epoch 30 loss = 88.2502712905407\n",
      "            Epoch 40 loss = 76.38900458812714\n",
      "            Epoch 50 loss = 61.9489713460207\n",
      "            Epoch 60 loss = 43.91553493589163\n",
      "            Epoch 70 loss = 28.6010176949203\n",
      "            Epoch 80 loss = 16.22612714767456\n",
      "            Epoch 90 loss = 8.412237823009491\n",
      "            Epoch 100 loss = 4.4460413423366845\n",
      "            Epoch 110 loss = 2.568923344835639\n",
      "            Epoch 120 loss = 1.6510524593759328\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.72667372226715\n",
      "            Epoch 10 loss = 109.0060288310051\n",
      "            Epoch 20 loss = 108.34453356266022\n",
      "            Epoch 30 loss = 107.51736015081406\n",
      "            Epoch 40 loss = 106.38915151357651\n",
      "            Epoch 50 loss = 104.85490638017654\n",
      "            Epoch 60 loss = 102.63380748033524\n",
      "            Epoch 70 loss = 98.86616849899292\n",
      "            Epoch 80 loss = 92.48658663034439\n",
      "            Epoch 90 loss = 84.99448281526566\n",
      "            Epoch 100 loss = 79.11528906226158\n",
      "            Epoch 110 loss = 73.92963525652885\n",
      "            Epoch 120 loss = 68.48537628352642\n",
      "      For 130 epochs : [73.6, 73.5, 76.7, 74.5, 70.30000000000001]\n",
      "     140 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.54256916046143\n",
      "            Epoch 10 loss = 92.19341304898262\n",
      "            Epoch 20 loss = 37.21160572767258\n",
      "            Epoch 30 loss = 4.623604403808713\n",
      "            Epoch 40 loss = 0.9395581346470863\n",
      "            Epoch 50 loss = 0.3794339745945763\n",
      "            Epoch 60 loss = 0.21969840244855732\n",
      "            Epoch 70 loss = 0.150348510404001\n",
      "            Epoch 80 loss = 0.11300435419252608\n",
      "            Epoch 90 loss = 0.08992603563820012\n",
      "            Epoch 100 loss = 0.07433183456305414\n",
      "            Epoch 110 loss = 0.0631312471232377\n",
      "            Epoch 120 loss = 0.05470631671778392\n",
      "            Epoch 130 loss = 0.04816185816162033\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.80230861902237\n",
      "            Epoch 10 loss = 99.76811307668686\n",
      "            Epoch 20 loss = 66.8306023478508\n",
      "            Epoch 30 loss = 23.846710227429867\n",
      "            Epoch 40 loss = 4.758076078025624\n",
      "            Epoch 50 loss = 1.3080898963380605\n",
      "            Epoch 60 loss = 0.5583072758163325\n",
      "            Epoch 70 loss = 0.32379702359321527\n",
      "            Epoch 80 loss = 0.2210900217469316\n",
      "            Epoch 90 loss = 0.16554462758358568\n",
      "            Epoch 100 loss = 0.13122055954590905\n",
      "            Epoch 110 loss = 0.108059216523543\n",
      "            Epoch 120 loss = 0.09146791165403556\n",
      "            Epoch 130 loss = 0.0790589493335574\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.30878913402557\n",
      "            Epoch 10 loss = 102.08372813463211\n",
      "            Epoch 20 loss = 80.65448519587517\n",
      "            Epoch 30 loss = 48.44690761715174\n",
      "            Epoch 40 loss = 20.499981524422765\n",
      "            Epoch 50 loss = 6.192693277262151\n",
      "            Epoch 60 loss = 2.396822136361152\n",
      "            Epoch 70 loss = 1.1333172344602644\n",
      "            Epoch 80 loss = 0.6427888437174261\n",
      "            Epoch 90 loss = 0.4261807586881332\n",
      "            Epoch 100 loss = 0.3116036104038358\n",
      "            Epoch 110 loss = 0.24239587536430918\n",
      "            Epoch 120 loss = 0.19676272990182042\n",
      "            Epoch 130 loss = 0.16471893312700558\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.99359983205795\n",
      "            Epoch 10 loss = 107.7861071228981\n",
      "            Epoch 20 loss = 104.65491300821304\n",
      "            Epoch 30 loss = 96.15924471616745\n",
      "            Epoch 40 loss = 81.40198436379433\n",
      "            Epoch 50 loss = 68.05208578705788\n",
      "            Epoch 60 loss = 49.8225220143795\n",
      "            Epoch 70 loss = 32.44438426569104\n",
      "            Epoch 80 loss = 19.371986677870154\n",
      "            Epoch 90 loss = 10.22395038139075\n",
      "            Epoch 100 loss = 5.262157975696027\n",
      "            Epoch 110 loss = 2.9042248574551195\n",
      "            Epoch 120 loss = 1.76291148934979\n",
      "            Epoch 130 loss = 1.192128456896171\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.5404104590416\n",
      "            Epoch 10 loss = 108.82304710149765\n",
      "            Epoch 20 loss = 108.22047001123428\n",
      "            Epoch 30 loss = 107.34112256765366\n",
      "            Epoch 40 loss = 105.98184084892273\n",
      "            Epoch 50 loss = 103.97549426555634\n",
      "            Epoch 60 loss = 100.95294708013535\n",
      "            Epoch 70 loss = 95.72450625896454\n",
      "            Epoch 80 loss = 88.79349592328072\n",
      "            Epoch 90 loss = 83.01047411561012\n",
      "            Epoch 100 loss = 77.9922603070736\n",
      "            Epoch 110 loss = 72.6971181333065\n",
      "            Epoch 120 loss = 66.64023360610008\n",
      "            Epoch 130 loss = 59.675648018717766\n",
      "      For 140 epochs : [75.8, 72.1, 69.9, 69.6, 73.3]\n",
      "     150 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.33948373794556\n",
      "            Epoch 10 loss = 89.88141241669655\n",
      "            Epoch 20 loss = 34.782953694462776\n",
      "            Epoch 30 loss = 4.03553219139576\n",
      "            Epoch 40 loss = 0.817672505392693\n",
      "            Epoch 50 loss = 0.36063702509272844\n",
      "            Epoch 60 loss = 0.21731765918957535\n",
      "            Epoch 70 loss = 0.15219226067711134\n",
      "            Epoch 80 loss = 0.11574236159503926\n",
      "            Epoch 90 loss = 0.0927008978324011\n",
      "            Epoch 100 loss = 0.076922493371967\n",
      "            Epoch 110 loss = 0.065478203650855\n",
      "            Epoch 120 loss = 0.0568279410181276\n",
      "            Epoch 130 loss = 0.05008160170473275\n",
      "            Epoch 140 loss = 0.0446804152270488\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.3920766711235\n",
      "            Epoch 10 loss = 97.17600718140602\n",
      "            Epoch 20 loss = 53.97906935214996\n",
      "            Epoch 30 loss = 14.408036476001143\n",
      "            Epoch 40 loss = 2.8318090490065515\n",
      "            Epoch 50 loss = 0.9132919303374365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 60 loss = 0.45351790863787755\n",
      "            Epoch 70 loss = 0.28474878732231446\n",
      "            Epoch 80 loss = 0.2026986279815901\n",
      "            Epoch 90 loss = 0.15548856518580578\n",
      "            Epoch 100 loss = 0.12516894747386687\n",
      "            Epoch 110 loss = 0.10419405820721295\n",
      "            Epoch 120 loss = 0.0889097872786806\n",
      "            Epoch 130 loss = 0.07731400805641897\n",
      "            Epoch 140 loss = 0.06822999032010557\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.05028355121613\n",
      "            Epoch 10 loss = 99.27646064758301\n",
      "            Epoch 20 loss = 74.3832175731659\n",
      "            Epoch 30 loss = 39.40646857023239\n",
      "            Epoch 40 loss = 13.405586401000619\n",
      "            Epoch 50 loss = 3.786835875827819\n",
      "            Epoch 60 loss = 1.5202864434104413\n",
      "            Epoch 70 loss = 0.7924504596740007\n",
      "            Epoch 80 loss = 0.4942314198706299\n",
      "            Epoch 90 loss = 0.34665462566772476\n",
      "            Epoch 100 loss = 0.2624839688069187\n",
      "            Epoch 110 loss = 0.20923993701580912\n",
      "            Epoch 120 loss = 0.17287390466663055\n",
      "            Epoch 130 loss = 0.1466309676907258\n",
      "            Epoch 140 loss = 0.12686985211621504\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.870445728302\n",
      "            Epoch 10 loss = 108.68421161174774\n",
      "            Epoch 20 loss = 107.00756520032883\n",
      "            Epoch 30 loss = 103.70374804735184\n",
      "            Epoch 40 loss = 94.73455563187599\n",
      "            Epoch 50 loss = 81.4477334022522\n",
      "            Epoch 60 loss = 68.14416135847569\n",
      "            Epoch 70 loss = 49.71288953721523\n",
      "            Epoch 80 loss = 31.734343506395817\n",
      "            Epoch 90 loss = 17.634276516735554\n",
      "            Epoch 100 loss = 8.784079026430845\n",
      "            Epoch 110 loss = 4.596821471583098\n",
      "            Epoch 120 loss = 2.6425928063690662\n",
      "            Epoch 130 loss = 1.6660601207986474\n",
      "            Epoch 140 loss = 1.136125419754535\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.6305639743805\n",
      "            Epoch 10 loss = 108.78804099559784\n",
      "            Epoch 20 loss = 107.94434261322021\n",
      "            Epoch 30 loss = 106.82901656627655\n",
      "            Epoch 40 loss = 105.29316902160645\n",
      "            Epoch 50 loss = 103.0724425315857\n",
      "            Epoch 60 loss = 99.35205614566803\n",
      "            Epoch 70 loss = 93.01483535766602\n",
      "            Epoch 80 loss = 85.4832651913166\n",
      "            Epoch 90 loss = 79.40976476669312\n",
      "            Epoch 100 loss = 73.96738460659981\n",
      "            Epoch 110 loss = 68.5034609735012\n",
      "            Epoch 120 loss = 62.68701893091202\n",
      "            Epoch 130 loss = 56.291101060807705\n",
      "            Epoch 140 loss = 49.23604739457369\n",
      "      For 150 epochs : [75.8, 71.89999999999999, 72.1, 73.6, 69.0]\n",
      "  With minibatches of size 20 : [[76.8, 73.8, 74.7, 73.4, 69.8], [73.8, 72.2, 71.6, 71.7, 71.50000000000001], [72.7, 73.8, 72.1, 76.3, 70.7], [73.6, 73.5, 76.7, 74.5, 70.30000000000001], [75.8, 72.1, 69.9, 69.6, 73.3], [75.8, 71.89999999999999, 72.1, 73.6, 69.0]]\n",
      "Batch size 40\n",
      "==============\n",
      "     100 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.66754561662674\n",
      "            Epoch 10 loss = 52.17578774690628\n",
      "            Epoch 20 loss = 42.83812612295151\n",
      "            Epoch 30 loss = 27.779195055365562\n",
      "            Epoch 40 loss = 14.3041006103158\n",
      "            Epoch 50 loss = 4.951415354385972\n",
      "            Epoch 60 loss = 1.4111246773973107\n",
      "            Epoch 70 loss = 0.5795675660483539\n",
      "            Epoch 80 loss = 0.3254280890105292\n",
      "            Epoch 90 loss = 0.21800972754135728\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.735715329647064\n",
      "            Epoch 10 loss = 53.20639383792877\n",
      "            Epoch 20 loss = 48.08678317070007\n",
      "            Epoch 30 loss = 39.754870891571045\n",
      "            Epoch 40 loss = 27.90760374069214\n",
      "            Epoch 50 loss = 15.525358900427818\n",
      "            Epoch 60 loss = 6.288658898323774\n",
      "            Epoch 70 loss = 2.2170936223119497\n",
      "            Epoch 80 loss = 0.9962198711000383\n",
      "            Epoch 90 loss = 0.5573346591554582\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.82963848114014\n",
      "            Epoch 10 loss = 54.0178416967392\n",
      "            Epoch 20 loss = 52.66964244842529\n",
      "            Epoch 30 loss = 49.33854812383652\n",
      "            Epoch 40 loss = 42.12191525101662\n",
      "            Epoch 50 loss = 36.25872269272804\n",
      "            Epoch 60 loss = 27.597199216485023\n",
      "            Epoch 70 loss = 19.213457487523556\n",
      "            Epoch 80 loss = 12.956521198153496\n",
      "            Epoch 90 loss = 7.604127366095781\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 54.64776599407196\n",
      "            Epoch 10 loss = 54.06315577030182\n",
      "            Epoch 20 loss = 53.37432771921158\n",
      "            Epoch 30 loss = 52.284022092819214\n",
      "            Epoch 40 loss = 50.46629011631012\n",
      "            Epoch 50 loss = 46.948627173900604\n",
      "            Epoch 60 loss = 42.345121175050735\n",
      "            Epoch 70 loss = 38.79358607530594\n",
      "            Epoch 80 loss = 35.3970368206501\n",
      "            Epoch 90 loss = 31.46934276819229\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 54.789090394973755\n",
      "            Epoch 10 loss = 54.562498331069946\n",
      "            Epoch 20 loss = 54.38715171813965\n",
      "            Epoch 30 loss = 54.19892704486847\n",
      "            Epoch 40 loss = 53.980597257614136\n",
      "            Epoch 50 loss = 53.718206107616425\n",
      "            Epoch 60 loss = 53.39903849363327\n",
      "            Epoch 70 loss = 53.013598918914795\n",
      "            Epoch 80 loss = 52.554875791072845\n",
      "            Epoch 90 loss = 52.00979828834534\n",
      "      For 100 epochs : [73.8, 74.8, 73.5, 72.1, 48.8]\n",
      "     110 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.99114793539047\n",
      "            Epoch 10 loss = 52.624113619327545\n",
      "            Epoch 20 loss = 44.06384891271591\n",
      "            Epoch 30 loss = 30.340543657541275\n",
      "            Epoch 40 loss = 15.756405740976334\n",
      "            Epoch 50 loss = 5.452044663950801\n",
      "            Epoch 60 loss = 1.5673028687015176\n",
      "            Epoch 70 loss = 0.6613102934788913\n",
      "            Epoch 80 loss = 0.36432035616599023\n",
      "            Epoch 90 loss = 0.23890407790895551\n",
      "            Epoch 100 loss = 0.17374686046969146\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.81189823150635\n",
      "            Epoch 10 loss = 53.188522934913635\n",
      "            Epoch 20 loss = 47.57511180639267\n",
      "            Epoch 30 loss = 38.92222410440445\n",
      "            Epoch 40 loss = 26.223040282726288\n",
      "            Epoch 50 loss = 14.89664851129055\n",
      "            Epoch 60 loss = 6.454238381236792\n",
      "            Epoch 70 loss = 2.413956381380558\n",
      "            Epoch 80 loss = 1.056009391322732\n",
      "            Epoch 90 loss = 0.5833419489208609\n",
      "            Epoch 100 loss = 0.3686164702521637\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.66680073738098\n",
      "            Epoch 10 loss = 53.57331305742264\n",
      "            Epoch 20 loss = 51.44367969036102\n",
      "            Epoch 30 loss = 45.11773216724396\n",
      "            Epoch 40 loss = 38.76580247282982\n",
      "            Epoch 50 loss = 31.162300288677216\n",
      "            Epoch 60 loss = 22.00041927397251\n",
      "            Epoch 70 loss = 15.184137307107449\n",
      "            Epoch 80 loss = 9.153197392821312\n",
      "            Epoch 90 loss = 4.561474008485675\n",
      "            Epoch 100 loss = 2.213690150529146\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 54.82099133729935\n",
      "            Epoch 10 loss = 54.3742059469223\n",
      "            Epoch 20 loss = 53.883790016174316\n",
      "            Epoch 30 loss = 53.16403645277023\n",
      "            Epoch 40 loss = 52.08939790725708\n",
      "            Epoch 50 loss = 50.24830347299576\n",
      "            Epoch 60 loss = 46.603442311286926\n",
      "            Epoch 70 loss = 42.20140612125397\n",
      "            Epoch 80 loss = 38.785076320171356\n",
      "            Epoch 90 loss = 35.40237218141556\n",
      "            Epoch 100 loss = 31.372729003429413\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 55.182909309864044\n",
      "            Epoch 10 loss = 54.52859181165695\n",
      "            Epoch 20 loss = 54.34437143802643\n",
      "            Epoch 30 loss = 54.146613121032715\n",
      "            Epoch 40 loss = 53.920466244220734\n",
      "            Epoch 50 loss = 53.65575164556503\n",
      "            Epoch 60 loss = 53.34422588348389\n",
      "            Epoch 70 loss = 52.979461669921875\n",
      "            Epoch 80 loss = 52.55214697122574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 90 loss = 52.04386931657791\n",
      "            Epoch 100 loss = 51.41359078884125\n",
      "      For 110 epochs : [76.0, 73.3, 76.0, 71.30000000000001, 50.4]\n",
      "     120 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.97470462322235\n",
      "            Epoch 10 loss = 52.39678418636322\n",
      "            Epoch 20 loss = 42.784855753183365\n",
      "            Epoch 30 loss = 28.444043949246407\n",
      "            Epoch 40 loss = 13.125697661191225\n",
      "            Epoch 50 loss = 3.6763510555028915\n",
      "            Epoch 60 loss = 1.1308350977487862\n",
      "            Epoch 70 loss = 0.51169493352063\n",
      "            Epoch 80 loss = 0.30108465324155986\n",
      "            Epoch 90 loss = 0.2061525417957455\n",
      "            Epoch 100 loss = 0.15410713135497645\n",
      "            Epoch 110 loss = 0.121761717076879\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.79721987247467\n",
      "            Epoch 10 loss = 52.45394325256348\n",
      "            Epoch 20 loss = 43.68911558389664\n",
      "            Epoch 30 loss = 33.635463923215866\n",
      "            Epoch 40 loss = 20.267937764525414\n",
      "            Epoch 50 loss = 10.81536839529872\n",
      "            Epoch 60 loss = 4.317954961210489\n",
      "            Epoch 70 loss = 1.729014277458191\n",
      "            Epoch 80 loss = 0.8298705508932471\n",
      "            Epoch 90 loss = 0.4743201616220176\n",
      "            Epoch 100 loss = 0.31465316528920084\n",
      "            Epoch 110 loss = 0.22976338805165142\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.84059250354767\n",
      "            Epoch 10 loss = 53.83105719089508\n",
      "            Epoch 20 loss = 51.76884424686432\n",
      "            Epoch 30 loss = 45.25156119465828\n",
      "            Epoch 40 loss = 39.476033091545105\n",
      "            Epoch 50 loss = 33.045206010341644\n",
      "            Epoch 60 loss = 23.872888788580894\n",
      "            Epoch 70 loss = 15.887348338961601\n",
      "            Epoch 80 loss = 9.421953741461039\n",
      "            Epoch 90 loss = 4.765824591740966\n",
      "            Epoch 100 loss = 2.353211272507906\n",
      "            Epoch 110 loss = 1.2904391041956842\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 54.77375012636185\n",
      "            Epoch 10 loss = 54.39390289783478\n",
      "            Epoch 20 loss = 53.938524305820465\n",
      "            Epoch 30 loss = 53.26983588933945\n",
      "            Epoch 40 loss = 52.26481330394745\n",
      "            Epoch 50 loss = 50.6056302189827\n",
      "            Epoch 60 loss = 47.36139702796936\n",
      "            Epoch 70 loss = 42.87279549241066\n",
      "            Epoch 80 loss = 39.3280993103981\n",
      "            Epoch 90 loss = 36.12770280241966\n",
      "            Epoch 100 loss = 32.64522647857666\n",
      "            Epoch 110 loss = 28.536820888519287\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 55.004267036914825\n",
      "            Epoch 10 loss = 54.69224292039871\n",
      "            Epoch 20 loss = 54.52435439825058\n",
      "            Epoch 30 loss = 54.36496818065643\n",
      "            Epoch 40 loss = 54.194056272506714\n",
      "            Epoch 50 loss = 54.000950396060944\n",
      "            Epoch 60 loss = 53.77789753675461\n",
      "            Epoch 70 loss = 53.51873731613159\n",
      "            Epoch 80 loss = 53.220074355602264\n",
      "            Epoch 90 loss = 52.88147991895676\n",
      "            Epoch 100 loss = 52.500760316848755\n",
      "            Epoch 110 loss = 52.06730276346207\n",
      "      For 120 epochs : [72.6, 74.0, 76.9, 68.6, 47.699999999999996]\n",
      "     130 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 55.006598234176636\n",
      "            Epoch 10 loss = 51.78282827138901\n",
      "            Epoch 20 loss = 42.536252617836\n",
      "            Epoch 30 loss = 27.037459209561348\n",
      "            Epoch 40 loss = 12.979643918573856\n",
      "            Epoch 50 loss = 3.9964003786444664\n",
      "            Epoch 60 loss = 1.0807534735649824\n",
      "            Epoch 70 loss = 0.47833989653736353\n",
      "            Epoch 80 loss = 0.2804392884718254\n",
      "            Epoch 90 loss = 0.19204920146148652\n",
      "            Epoch 100 loss = 0.1436459799297154\n",
      "            Epoch 110 loss = 0.1135464133694768\n",
      "            Epoch 120 loss = 0.09319474038784392\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.777180671691895\n",
      "            Epoch 10 loss = 53.15110844373703\n",
      "            Epoch 20 loss = 47.75750434398651\n",
      "            Epoch 30 loss = 38.94899433851242\n",
      "            Epoch 40 loss = 26.6340029835701\n",
      "            Epoch 50 loss = 15.662989042699337\n",
      "            Epoch 60 loss = 7.397490777075291\n",
      "            Epoch 70 loss = 2.6175115620717406\n",
      "            Epoch 80 loss = 1.1056039473041892\n",
      "            Epoch 90 loss = 0.5979774950537831\n",
      "            Epoch 100 loss = 0.3829023251309991\n",
      "            Epoch 110 loss = 0.27228992455638945\n",
      "            Epoch 120 loss = 0.2073502178536728\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.89448547363281\n",
      "            Epoch 10 loss = 53.80050837993622\n",
      "            Epoch 20 loss = 51.794042229652405\n",
      "            Epoch 30 loss = 46.52611416578293\n",
      "            Epoch 40 loss = 40.35341250896454\n",
      "            Epoch 50 loss = 34.29915902018547\n",
      "            Epoch 60 loss = 25.138756811618805\n",
      "            Epoch 70 loss = 17.002617590129375\n",
      "            Epoch 80 loss = 10.612093292176723\n",
      "            Epoch 90 loss = 5.517472978681326\n",
      "            Epoch 100 loss = 2.6348052201792598\n",
      "            Epoch 110 loss = 1.4383708937093616\n",
      "            Epoch 120 loss = 0.8774522850289941\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 55.00401175022125\n",
      "            Epoch 10 loss = 54.378345370292664\n",
      "            Epoch 20 loss = 54.01057493686676\n",
      "            Epoch 30 loss = 53.45912957191467\n",
      "            Epoch 40 loss = 52.608040392398834\n",
      "            Epoch 50 loss = 51.29152715206146\n",
      "            Epoch 60 loss = 48.813499093055725\n",
      "            Epoch 70 loss = 44.611000418663025\n",
      "            Epoch 80 loss = 40.652277529239655\n",
      "            Epoch 90 loss = 37.49709552526474\n",
      "            Epoch 100 loss = 34.20795840024948\n",
      "            Epoch 110 loss = 30.38519185781479\n",
      "            Epoch 120 loss = 25.826048478484154\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 55.05037760734558\n",
      "            Epoch 10 loss = 54.51166749000549\n",
      "            Epoch 20 loss = 54.32361578941345\n",
      "            Epoch 30 loss = 54.12968283891678\n",
      "            Epoch 40 loss = 53.905211329460144\n",
      "            Epoch 50 loss = 53.63668870925903\n",
      "            Epoch 60 loss = 53.31091898679733\n",
      "            Epoch 70 loss = 52.915276288986206\n",
      "            Epoch 80 loss = 52.437257409095764\n",
      "            Epoch 90 loss = 51.85525196790695\n",
      "            Epoch 100 loss = 51.126911640167236\n",
      "            Epoch 110 loss = 50.17719501256943\n",
      "            Epoch 120 loss = 48.91055208444595\n",
      "      For 130 epochs : [72.7, 74.4, 73.9, 73.2, 57.3]\n",
      "     140 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.769703567028046\n",
      "            Epoch 10 loss = 51.12030255794525\n",
      "            Epoch 20 loss = 39.559786438941956\n",
      "            Epoch 30 loss = 22.70552448928356\n",
      "            Epoch 40 loss = 10.236005309969187\n",
      "            Epoch 50 loss = 2.724784357473254\n",
      "            Epoch 60 loss = 0.8804652122780681\n",
      "            Epoch 70 loss = 0.4263364910148084\n",
      "            Epoch 80 loss = 0.26129920152015984\n",
      "            Epoch 90 loss = 0.18340027355588973\n",
      "            Epoch 100 loss = 0.13913226983277127\n",
      "            Epoch 110 loss = 0.11095665825996548\n",
      "            Epoch 120 loss = 0.09162205207394436\n",
      "            Epoch 130 loss = 0.07760268772835843\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.97760850191116\n",
      "            Epoch 10 loss = 53.44898933172226\n",
      "            Epoch 20 loss = 50.54381674528122\n",
      "            Epoch 30 loss = 42.18211683630943\n",
      "            Epoch 40 loss = 31.539533883333206\n",
      "            Epoch 50 loss = 19.298571303486824\n",
      "            Epoch 60 loss = 10.410389345139265\n",
      "            Epoch 70 loss = 4.081314915791154\n",
      "            Epoch 80 loss = 1.5268641719594598\n",
      "            Epoch 90 loss = 0.7569278310984373\n",
      "            Epoch 100 loss = 0.4557512227911502\n",
      "            Epoch 110 loss = 0.3115565123734996\n",
      "            Epoch 120 loss = 0.2313867323100567\n",
      "            Epoch 130 loss = 0.18166035693138838\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.76911520957947\n",
      "            Epoch 10 loss = 53.82147318124771\n",
      "            Epoch 20 loss = 51.776063203811646\n",
      "            Epoch 30 loss = 45.65087455511093\n",
      "            Epoch 40 loss = 38.72139695286751\n",
      "            Epoch 50 loss = 30.918205499649048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 60 loss = 21.77935630083084\n",
      "            Epoch 70 loss = 15.155114434659481\n",
      "            Epoch 80 loss = 9.387870378792286\n",
      "            Epoch 90 loss = 4.896859247237444\n",
      "            Epoch 100 loss = 2.426760943606496\n",
      "            Epoch 110 loss = 1.3154400973580778\n",
      "            Epoch 120 loss = 0.7958167036995292\n",
      "            Epoch 130 loss = 0.5358434673398733\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 54.71114772558212\n",
      "            Epoch 10 loss = 54.425776302814484\n",
      "            Epoch 20 loss = 54.0970014333725\n",
      "            Epoch 30 loss = 53.601517260074615\n",
      "            Epoch 40 loss = 52.83477580547333\n",
      "            Epoch 50 loss = 51.60841566324234\n",
      "            Epoch 60 loss = 49.16741096973419\n",
      "            Epoch 70 loss = 44.63211044669151\n",
      "            Epoch 80 loss = 40.15624663233757\n",
      "            Epoch 90 loss = 36.55785861611366\n",
      "            Epoch 100 loss = 32.687134593725204\n",
      "            Epoch 110 loss = 28.083193972706795\n",
      "            Epoch 120 loss = 23.119685024023056\n",
      "            Epoch 130 loss = 18.650906533002853\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 54.89372169971466\n",
      "            Epoch 10 loss = 54.58433675765991\n",
      "            Epoch 20 loss = 54.36311739683151\n",
      "            Epoch 30 loss = 54.13249331712723\n",
      "            Epoch 40 loss = 53.87733495235443\n",
      "            Epoch 50 loss = 53.58671796321869\n",
      "            Epoch 60 loss = 53.25131368637085\n",
      "            Epoch 70 loss = 52.865584552288055\n",
      "            Epoch 80 loss = 52.42429792881012\n",
      "            Epoch 90 loss = 51.91383081674576\n",
      "            Epoch 100 loss = 51.30327498912811\n",
      "            Epoch 110 loss = 50.54018872976303\n",
      "            Epoch 120 loss = 49.55833750963211\n",
      "            Epoch 130 loss = 48.295898377895355\n",
      "      For 140 epochs : [73.8, 71.6, 73.9, 73.4, 60.9]\n",
      "     150 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.68795830011368\n",
      "            Epoch 10 loss = 52.194209814071655\n",
      "            Epoch 20 loss = 43.08055913448334\n",
      "            Epoch 30 loss = 27.847311109304428\n",
      "            Epoch 40 loss = 14.302263721823692\n",
      "            Epoch 50 loss = 4.390429727733135\n",
      "            Epoch 60 loss = 1.21917623328045\n",
      "            Epoch 70 loss = 0.5235659331083298\n",
      "            Epoch 80 loss = 0.3034535350743681\n",
      "            Epoch 90 loss = 0.20626391132827848\n",
      "            Epoch 100 loss = 0.15348196076229215\n",
      "            Epoch 110 loss = 0.12088342645438388\n",
      "            Epoch 120 loss = 0.09897654806263745\n",
      "            Epoch 130 loss = 0.0833329466113355\n",
      "            Epoch 140 loss = 0.07168021364486776\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.77105134725571\n",
      "            Epoch 10 loss = 53.53048688173294\n",
      "            Epoch 20 loss = 49.50888645648956\n",
      "            Epoch 30 loss = 40.25131168961525\n",
      "            Epoch 40 loss = 27.598665669560432\n",
      "            Epoch 50 loss = 16.6984051913023\n",
      "            Epoch 60 loss = 8.022907147184014\n",
      "            Epoch 70 loss = 2.990978012792766\n",
      "            Epoch 80 loss = 1.2008131272159517\n",
      "            Epoch 90 loss = 0.6299593772273511\n",
      "            Epoch 100 loss = 0.39841008570510894\n",
      "            Epoch 110 loss = 0.2812212856952101\n",
      "            Epoch 120 loss = 0.21312323363963515\n",
      "            Epoch 130 loss = 0.16950440441723913\n",
      "            Epoch 140 loss = 0.13954783766530454\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.72742861509323\n",
      "            Epoch 10 loss = 53.38663446903229\n",
      "            Epoch 20 loss = 50.53505688905716\n",
      "            Epoch 30 loss = 43.42550548911095\n",
      "            Epoch 40 loss = 37.88986724615097\n",
      "            Epoch 50 loss = 30.195675641298294\n",
      "            Epoch 60 loss = 21.697554230690002\n",
      "            Epoch 70 loss = 14.874081142246723\n",
      "            Epoch 80 loss = 8.842218365520239\n",
      "            Epoch 90 loss = 4.5216663889586926\n",
      "            Epoch 100 loss = 2.323651684448123\n",
      "            Epoch 110 loss = 1.323018483351916\n",
      "            Epoch 120 loss = 0.8325624358840287\n",
      "            Epoch 130 loss = 0.571109163807705\n",
      "            Epoch 140 loss = 0.4201448066160083\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 55.17585498094559\n",
      "            Epoch 10 loss = 54.36979407072067\n",
      "            Epoch 20 loss = 53.9248086810112\n",
      "            Epoch 30 loss = 53.23407542705536\n",
      "            Epoch 40 loss = 52.11975234746933\n",
      "            Epoch 50 loss = 50.2425742149353\n",
      "            Epoch 60 loss = 46.65105164051056\n",
      "            Epoch 70 loss = 42.047994405031204\n",
      "            Epoch 80 loss = 38.28564888238907\n",
      "            Epoch 90 loss = 34.49957099556923\n",
      "            Epoch 100 loss = 30.120719075202942\n",
      "            Epoch 110 loss = 25.09438280761242\n",
      "            Epoch 120 loss = 20.165710777044296\n",
      "            Epoch 130 loss = 15.974449090659618\n",
      "            Epoch 140 loss = 12.46282709017396\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 54.926306784152985\n",
      "            Epoch 10 loss = 54.59055608510971\n",
      "            Epoch 20 loss = 54.41056847572327\n",
      "            Epoch 30 loss = 54.22239601612091\n",
      "            Epoch 40 loss = 54.009112894535065\n",
      "            Epoch 50 loss = 53.759326100349426\n",
      "            Epoch 60 loss = 53.46080952882767\n",
      "            Epoch 70 loss = 53.101615607738495\n",
      "            Epoch 80 loss = 52.66638767719269\n",
      "            Epoch 90 loss = 52.13139981031418\n",
      "            Epoch 100 loss = 51.449551701545715\n",
      "            Epoch 110 loss = 50.54446029663086\n",
      "            Epoch 120 loss = 49.31231904029846\n",
      "            Epoch 130 loss = 47.666195333004\n",
      "            Epoch 140 loss = 45.64242082834244\n",
      "      For 150 epochs : [74.4, 74.9, 73.9, 74.1, 62.4]\n",
      "  With minibatches of size 40 : [[73.8, 74.8, 73.5, 72.1, 48.8], [76.0, 73.3, 76.0, 71.30000000000001, 50.4], [72.6, 74.0, 76.9, 68.6, 47.699999999999996], [72.7, 74.4, 73.9, 73.2, 57.3], [73.8, 71.6, 73.9, 73.4, 60.9], [74.4, 74.9, 73.9, 74.1, 62.4]]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"WITHOUT DROPOUT\")\n",
    "print(\"\")\n",
    "no_dropout = []\n",
    "for b_size in batch_sizes:\n",
    "    print(\"Batch size\", b_size)\n",
    "    print(\"=\"*14)\n",
    "    epoch_acc = []\n",
    "    for e in nb_epochs: \n",
    "        print(\"    \", e,\"epochs\")\n",
    "        print(\"  \", \"-\"*15)\n",
    "        lr_acc =[]\n",
    "        for lr in learning_rates:\n",
    "            print(\"        Learning rate\", lr, \":\")\n",
    "            model = CNN_1D()\n",
    "            train_model(model, Variable(train_input), Variable(train_target), b_size, nb_epochs=e, learning_rate=lr)\n",
    "            test_error = compute_nb_errors(model, Variable(test_input), Variable(test_target))\n",
    "            lr_acc.append(100*(1-(test_error/test_input.size(0))))\n",
    "        epoch_acc.append(lr_acc)\n",
    "        print(\"      For\", e, \"epochs :\", lr_acc)\n",
    "    print(\"  With minibatches of size\", b_size, \":\", epoch_acc)\n",
    "    no_dropout.append(epoch_acc)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_results = {\"batch_sizes\":batch_sizes, \"learning_rates\":learning_rates, \"nb_epochs\":nb_epochs, \"results\":no_dropout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(normal_results,open('results_no_dropout.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_sizes': [20, 40],\n",
       " 'learning_rates': [0.01, 0.0075, 0.005, 0.0025, 0.001],\n",
       " 'nb_epochs': [100, 110, 120, 130, 140, 150],\n",
       " 'results': [[[76.8, 73.8, 74.7, 73.4, 69.8],\n",
       "   [73.8, 72.2, 71.6, 71.7, 71.50000000000001],\n",
       "   [72.7, 73.8, 72.1, 76.3, 70.7],\n",
       "   [73.6, 73.5, 76.7, 74.5, 70.30000000000001],\n",
       "   [75.8, 72.1, 69.9, 69.6, 73.3],\n",
       "   [75.8, 71.89999999999999, 72.1, 73.6, 69.0]],\n",
       "  [[73.8, 74.8, 73.5, 72.1, 48.8],\n",
       "   [76.0, 73.3, 76.0, 71.30000000000001, 50.4],\n",
       "   [72.6, 74.0, 76.9, 68.6, 47.699999999999996],\n",
       "   [72.7, 74.4, 73.9, 73.2, 57.3],\n",
       "   [73.8, 71.6, 73.9, 73.4, 60.9],\n",
       "   [74.4, 74.9, 73.9, 74.1, 62.4]]]}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(open('results_no_dropout.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH DROPOUT\n",
      "\n",
      "Batch size 20\n",
      "==============\n",
      "    Learning rate 0.01 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.56861990690231\n",
      "            Epoch 10 loss = 97.68512445688248\n",
      "            Epoch 20 loss = 44.219020545482635\n",
      "            Epoch 30 loss = 5.120638834312558\n",
      "            Epoch 40 loss = 1.2338459701277316\n",
      "            Epoch 50 loss = 0.5521879402222112\n",
      "            Epoch 60 loss = 0.34920062322635204\n",
      "            Epoch 70 loss = 0.2755592145549599\n",
      "            Epoch 80 loss = 0.19016350763558876\n",
      "            Epoch 90 loss = 0.15994437631161418\n",
      "            Epoch 100 loss = 0.12974347618728643\n",
      "            Epoch 1 loss = 0.13672240971209249\n",
      "            Epoch 10 loss = 0.12727110175183043\n",
      "            Epoch 1 loss = 0.10125427930688602\n",
      "            Epoch 10 loss = 0.0968204613745911\n",
      "            Epoch 1 loss = 0.10534073887538398\n",
      "            Epoch 10 loss = 0.08921499579082592\n",
      "            Epoch 1 loss = 0.0914028889455949\n",
      "            Epoch 10 loss = 0.07541410003614146\n",
      "            Epoch 1 loss = 0.07375158538343385\n",
      "            Epoch 10 loss = 0.08126419101972715\n",
      "      For a learning rate of 0.01 : [73.0, 73.1, 73.5, 72.3, 72.39999999999999, 73.0]\n",
      "    Learning rate 0.0075 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.62843090295792\n",
      "            Epoch 10 loss = 98.21608650684357\n",
      "            Epoch 20 loss = 63.43432921171188\n",
      "            Epoch 30 loss = 22.24169982969761\n",
      "            Epoch 40 loss = 4.341281499247998\n",
      "            Epoch 50 loss = 1.3381346956593916\n",
      "            Epoch 60 loss = 0.6578830982325599\n",
      "            Epoch 70 loss = 0.4369636698102113\n",
      "            Epoch 80 loss = 0.3122253053879831\n",
      "            Epoch 90 loss = 0.23963762546190992\n",
      "            Epoch 100 loss = 0.20411846942442935\n",
      "            Epoch 1 loss = 0.19837641302729025\n",
      "            Epoch 10 loss = 0.1728045970085077\n",
      "            Epoch 1 loss = 0.15700763886707136\n",
      "            Epoch 10 loss = 0.1435071559244534\n",
      "            Epoch 1 loss = 0.1388901480968343\n",
      "            Epoch 10 loss = 0.12629083030697075\n",
      "            Epoch 1 loss = 0.11654621600609971\n",
      "            Epoch 10 loss = 0.09773171833512606\n",
      "            Epoch 1 loss = 0.11018532399612013\n",
      "            Epoch 10 loss = 0.1102513519799686\n",
      "      For a learning rate of 0.0075 : [75.2, 75.1, 75.4, 75.4, 75.6, 75.3]\n",
      "    Learning rate 0.005 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.57152128219604\n",
      "            Epoch 10 loss = 104.8253173828125\n",
      "            Epoch 20 loss = 86.22930234670639\n",
      "            Epoch 30 loss = 56.883259519934654\n",
      "            Epoch 40 loss = 26.098600275814533\n",
      "            Epoch 50 loss = 8.068019286729395\n",
      "            Epoch 60 loss = 2.9940146636217833\n",
      "            Epoch 70 loss = 1.4079134740168229\n",
      "            Epoch 80 loss = 0.8640981721691787\n",
      "            Epoch 90 loss = 0.6214228210155852\n",
      "            Epoch 100 loss = 0.47480609541526064\n",
      "            Epoch 1 loss = 0.42907167854718864\n",
      "            Epoch 10 loss = 0.3684646670008078\n",
      "            Epoch 1 loss = 0.35554498550482094\n"
     ]
    }
   ],
   "source": [
    "print(\"WITH DROPOUT\")\n",
    "print(\"\")\n",
    "with_dropout = []\n",
    "for b_size in batch_sizes:\n",
    "    print(\"Batch size\", b_size)\n",
    "    print(\"=\"*14)\n",
    "    lr_acc = []\n",
    "    for lr in learning_rates: \n",
    "        print(\"    Learning rate\", lr, \":\")\n",
    "        print(\"  \", \"-\"*22)\n",
    "        epoch_acc = []\n",
    "        model = CNN_dropout()\n",
    "        train_model(model, Variable(train_input), Variable(train_target), b_size, learning_rate=lr)\n",
    "        test_error = compute_nb_errors(model, Variable(test_input), Variable(test_target))\n",
    "        epoch_acc.append(100*(1-(test_error/test_input.size(0))))\n",
    "        for i in range(5):\n",
    "            train_model(model, Variable(train_input), Variable(train_target), b_size, nb_epochs=10, learning_rate=lr)\n",
    "            test_error = compute_nb_errors(model, Variable(test_input), Variable(test_target))\n",
    "            epoch_acc.append(100*(1-(test_error/test_input.size(0))))\n",
    "        lr_acc.append(epoch_acc)\n",
    "        print(\"      For a learning rate of\", lr, \":\", epoch_acc)\n",
    "    print(\"  With minibatches of size\", b_size, \":\", lr_acc)\n",
    "    with_dropout.append(lr_acc)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_results = {\"batch_sizes\":batch_sizes, \"learning_rates\":learning_rates, \"nb_epochs\":nb_epochs, \"results\":with_dropout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(dropout_results,open('results_with_dropout.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.load(open('results_with_dropout.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAk0AAAI3CAYAAACGQ2//AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10d045080>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "plt.subplot(221)\n",
    "plt.imshow(normal_results['results'][0], cmap='Reds')\n",
    "plt.xlabel('Learning rate')\n",
    "#plt.xticks(normal_results['learning_rates'])\n",
    "plt.colorbar()\n",
    "plt.subplot(222)\n",
    "plt.imshow(normal_results['results'][1], cmap='Reds')\n",
    "plt.colorbar()\n",
    "plt.subplot(223)\n",
    "plt.imshow(dropout_results['results'][0], cmap='Reds')\n",
    "plt.xlabel('Learning rate')\n",
    "#plt.xticks(normal_results['learning_rates'])\n",
    "plt.colorbar()\n",
    "plt.subplot(224)\n",
    "plt.imshow(dropout_results['results'][1], cmap='Reds')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADxCAYAAAD1LG0eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10db038d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "im = axes[0][0].imshow(normal_results['results'][0], vmin=50, vmax=80)\n",
    "im = axes[0][1].imshow(normal_results['results'][1], vmin=50, vmax=80)\n",
    "im = axes[1][0].imshow(dropout_results['results'][0], vmin=50, vmax=80)\n",
    "im = axes[1][1].imshow(dropout_results['results'][1], vmin=50, vmax=80)\n",
    "\n",
    "fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 30)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(dropout_results['results']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
