{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f847ca4ed30>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn import linear_model\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n",
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "import dlc_bci as bci\n",
    "train_input , train_target = bci.load(root = './data_bci')\n",
    "print(str(type(train_input)), train_input.size()) \n",
    "print(str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(str(type(test_input)), test_input.size()) \n",
    "print(str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# normalize mean to 0\n",
    "train_input.sub_(train_input.mean())\n",
    "test_input.sub_(test_input.mean())\n",
    "\n",
    "# normalize variance to 1\n",
    "train_input.div_(train_input.std())\n",
    "test_input.div_(test_input.std())\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = linear_model.LinearRegression()\n",
    "\n",
    "X_train = train_input.view(train_input.size(0), -1)\n",
    "X_test = test_input.view(test_input.size(0), -1)\n",
    "\n",
    "# cross_val_predict returns an array of the same size as `y` where each entry\n",
    "# is a prediction obtained by cross validation:\n",
    "lr.fit(X=X_train, y=train_target)\n",
    "preds = lr.predict(X_test)\n",
    "\n",
    "preds = torch.FloatTensor(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = discrete_predictions(preds)\n",
    "accuracy = compute_accuracy(test_target, preds)\n",
    "print('Accuracy ' + str(accuracy*100) + '%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "train_input , train_target = bci.load(root = './data_bci')\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input2 = train_input[0:300]\n",
    "train_target2 = train_target[0:300]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net(nn.Module):\n",
    "    def __init__(self, nb_hidden=64):\n",
    "        super(Net, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(640, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"======\")\n",
    "        print(x.size())\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        print(x.size())\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        print(x.size())\n",
    "        x = F.tanh(self.fc1(x.view(-1, 640)))\n",
    "        print(x.size())\n",
    "        #print(\"======\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "my_cool_net = Net()\n",
    "train_model(my_cool_net, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4, nb_epochs=50, learning_rate=1e-2, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute error rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_errors = compute_nb_errors(my_cool_net, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_errors/test_input.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_errors = compute_nb_errors(my_cool_net, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "train_errors / train_input.size(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grid search on model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "step_sizes = [0.01, 0.005, 0.0025, 0.001, 0.0005]\n",
    "nb_iters_ = [25, 50, 100, 250, 500]\n",
    "\n",
    "test_errors = []\n",
    "train_errors = []\n",
    "\n",
    "for lr in step_sizes:\n",
    "    test_errs = []\n",
    "    train_errs = []\n",
    "    for nb_iters in nb_iters_:\n",
    "        # reset and train the network\n",
    "        my_net = Net()\n",
    "        train_model(my_net, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), mini_batch_size=4, nb_epochs=nb_iters, learning_rate=lr)\n",
    "        \n",
    "        # compute the number of errors\n",
    "        n_train_errors = compute_nb_errors(my_net, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "        n_test_errors = compute_nb_errors(my_net, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)\n",
    "        \n",
    "        train_error = n_train_errors / train_input.size(0)\n",
    "        test_error = n_test_errors / test_input.size(0)\n",
    "        \n",
    "        test_errs.append(test_error)\n",
    "        train_errs.append(train_error)\n",
    "        \n",
    "        print('step size: ' + str(lr) + ' nb epochs: ' + str(nb_iters) + ' train error: ' + str(train_error) + ' test error: ' + str(test_error))\n",
    "    test_errors.append(test_errs)\n",
    "    train_errors.append(train_errs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Net2(nn.Module):\n",
    "    def __init__(self, nb_hidden=64):\n",
    "        super(Net2, self).__init__()\n",
    "        self.inp = nn.Linear(50, 32)\n",
    "        self.lstm = nn.LSTM(32, 32, 2, dropout=0.05)\n",
    "        #self.lstm2 = nn.LSTM(32, 32, 2, dropout=0.2)\n",
    "        self.out = nn.Linear(28*32, 2)\n",
    "\n",
    "    def forward(self, x, hc=None):\n",
    "        #print(\"======\")\n",
    "        #print(x.size())\n",
    "        #x = F.tanh(F.max_pool2d(self.lstm2(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.inp(x))\n",
    "        #print(x.size())\n",
    "        x, hidden = self.lstm(x.squeeze(1), hc)\n",
    "        x = F.tanh(x)\n",
    "        #print(x.size())\n",
    "        #x, hidden = self.lstm2(x, hc)\n",
    "        #print(x.size())\n",
    "        x = F.tanh(self.out(x.view(-1, 28*32)))\n",
    "        #print(x.size())\n",
    "        #print(\"======\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 55.73849070072174\n",
      "1 55.38568425178528\n",
      "2 55.21029359102249\n",
      "3 55.10095900297165\n",
      "4 54.998302578926086\n",
      "5 54.86130082607269\n",
      "6 54.700638473033905\n",
      "7 54.42222648859024\n",
      "8 54.12572079896927\n",
      "9 53.78873461484909\n",
      "10 53.358709782361984\n",
      "11 52.860850274562836\n",
      "12 52.26632231473923\n",
      "13 51.40733340382576\n",
      "14 50.350856959819794\n",
      "15 49.017218232154846\n",
      "16 47.77862358093262\n",
      "17 46.44073760509491\n",
      "18 45.236290737986565\n",
      "19 44.11910480260849\n",
      "20 43.10380092263222\n",
      "21 41.45946538448334\n",
      "22 40.12917344272137\n",
      "23 38.74941794574261\n",
      "24 38.039336293935776\n",
      "25 37.47816778719425\n",
      "26 36.333162158727646\n",
      "27 35.75167179107666\n",
      "28 34.646584168076515\n",
      "29 33.99380961060524\n",
      "30 32.875477120280266\n",
      "31 32.5782028734684\n",
      "32 31.80992814898491\n",
      "33 31.27399778366089\n",
      "34 29.31551556289196\n",
      "35 29.736986055970192\n",
      "36 28.919235825538635\n",
      "37 28.256519332528114\n",
      "38 28.86168311536312\n",
      "39 26.187440767884254\n",
      "40 26.5390802025795\n",
      "41 24.347222849726677\n",
      "42 24.032073840498924\n",
      "43 23.85285922884941\n",
      "44 22.041225388646126\n",
      "45 23.528141885995865\n",
      "46 22.044113844633102\n",
      "47 20.883514970541\n",
      "48 21.381750389933586\n",
      "49 19.351817965507507\n",
      "50 19.060399115085602\n",
      "51 17.80833277106285\n",
      "52 19.22486835718155\n",
      "53 18.76729106903076\n",
      "54 17.17082269489765\n",
      "55 17.23658362030983\n",
      "56 17.128219708800316\n",
      "57 16.356691405177116\n",
      "58 15.400875121355057\n",
      "59 15.91217328608036\n",
      "60 14.729904815554619\n",
      "61 17.10370272397995\n",
      "62 18.90830971300602\n",
      "63 15.27075146138668\n",
      "64 14.304899975657463\n",
      "65 15.419042810797691\n",
      "66 14.035026848316193\n",
      "67 15.018988475203514\n",
      "68 14.914601519703865\n",
      "69 13.373949646949768\n",
      "70 13.413112938404083\n",
      "71 13.321911424398422\n",
      "72 13.455599620938301\n",
      "73 13.088376507163048\n",
      "74 13.035824447870255\n",
      "75 13.576140448451042\n",
      "76 13.790584236383438\n",
      "77 13.071847572922707\n",
      "78 14.492312118411064\n",
      "79 13.21926173567772\n",
      "80 13.086053654551506\n",
      "81 13.226331666111946\n",
      "82 12.96977673470974\n",
      "83 12.828159347176552\n",
      "84 12.863415390253067\n",
      "85 12.878726318478584\n",
      "86 12.85698901116848\n",
      "87 12.836821541190147\n",
      "88 12.779027804732323\n",
      "89 12.798610791563988\n",
      "90 12.75390800833702\n",
      "91 12.986572340130806\n",
      "92 12.734722450375557\n",
      "93 12.75072056055069\n",
      "94 12.706512227654457\n",
      "95 12.78019692003727\n",
      "96 12.695581272244453\n",
      "97 12.743234783411026\n",
      "98 12.737225815653801\n",
      "99 12.709226697683334\n"
     ]
    }
   ],
   "source": [
    "model = Net2()\n",
    "nb_epochs=100\n",
    "mini_batch_size=4\n",
    "learning_rate=0.1\n",
    "verbose=True\n",
    "train_in = Variable(train_input.view(-1, 1, 28, 50))\n",
    "train_targ = Variable(train_target)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr = learning_rate)\n",
    "for e in range(0, nb_epochs):\n",
    "    sum_loss = 0\n",
    "    for b in range(0, train_in.size(0), mini_batch_size):\n",
    "        output = model(train_in.narrow(0, b, mini_batch_size))\n",
    "        #print(train_targ.narrow(0, b, mini_batch_size).size())\n",
    "        #print(output.size())\n",
    "        loss = criterion(output, train_targ.narrow(0, b, mini_batch_size))\n",
    "        sum_loss = sum_loss + loss.data[0]\n",
    "        model.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    if verbose:\n",
    "        print(e, sum_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.69"
      ]
     },
     "execution_count": 205,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nb_errors = compute_nb_errors(model, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)\n",
    "1-nb_errors/test_input.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9841772151898734"
      ]
     },
     "execution_count": 206,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_errors = compute_nb_errors(model, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "1 - train_errors / train_input.size(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import math, random\n",
    "\n",
    "# Generating a noisy multi-sin wave \n",
    "\n",
    "def sine_2(X, signal_freq=60.):\n",
    "    return (np.sin(2 * np.pi * (X) / signal_freq) + np.sin(4 * np.pi * (X) / signal_freq)) / 2.0\n",
    "\n",
    "def noisy(Y, noise_range=(-0.05, 0.05)):\n",
    "    noise = np.random.uniform(noise_range[0], noise_range[1], size=Y.shape)\n",
    "    return Y + noise\n",
    "\n",
    "def sample(sample_size):\n",
    "    random_offset = random.randint(0, sample_size)\n",
    "    X = np.arange(sample_size)\n",
    "    Y = noisy(sine_2(X + random_offset))\n",
    "    print(Y.shape)\n",
    "    print('==========')\n",
    "    return Y\n",
    "\n",
    "class SimpleRNN(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(SimpleRNN, self).__init__()\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.inp = nn.Linear(1, hidden_size)\n",
    "        self.rnn = nn.LSTM(hidden_size, hidden_size, 2, dropout=0.05)\n",
    "        self.out = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def step(self, input, hidden=None):\n",
    "        input = self.inp(input.view(1, -1)).unsqueeze(1)\n",
    "        output, hidden = self.rnn(input, hidden)\n",
    "        output = self.out(output.squeeze(1))\n",
    "        return output, hidden\n",
    "\n",
    "    def forward(self, inputs, hidden=None, force=True, steps=0):\n",
    "        if force or steps == 0: steps = len(inputs)\n",
    "        outputs = Variable(torch.zeros(steps, 1, 1))\n",
    "        for i in range(steps):\n",
    "            if force or i == 0:\n",
    "                input = inputs[i]\n",
    "            else:\n",
    "                input = output\n",
    "            output, hidden = self.step(input, hidden)\n",
    "            outputs[i] = output\n",
    "        return outputs, hidden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#lstm_net = Net2()\n",
    "#train_model(lstm_net, Variable(train_input), Variable(train_target), 4, nb_epochs=50, learning_rate=1e-2, verbose=True)\n",
    "\n",
    "\n",
    "n_epochs = 100\n",
    "n_iters = 50\n",
    "hidden_size = 10\n",
    "\n",
    "model = SimpleRNN(hidden_size)\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "losses = np.zeros(n_epochs) # For plotting\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "\n",
    "    for iter in range(n_iters):\n",
    "        _inputs = sample(50)\n",
    "        inputs = Variable(torch.from_numpy(_inputs[:-1]).float())\n",
    "        targets = Variable(torch.from_numpy(_inputs[1:]).float())\n",
    "\n",
    "        # Use teacher forcing 50% of the time\n",
    "        force = random.random() < 0.5\n",
    "        outputs, hidden = model(inputs, None)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        losses[epoch] += loss.data[0]\n",
    "\n",
    "    if epoch > 0:\n",
    "        print(epoch, loss.data[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "time_steps = 10\n",
    "batch_size = 3\n",
    "in_size = 5\n",
    "classes_no = 7\n",
    "\n",
    "model = nn.LSTM(in_size, classes_no, 2)\n",
    "input_seq = Variable(torch.randn(time_steps, batch_size, in_size))\n",
    "output_seq, _ = model(input_seq)\n",
    "last_output = output_seq[-1]\n",
    "\n",
    "loss = nn.CrossEntropyLoss()\n",
    "target = Variable(torch.LongTensor(batch_size).random_(0, classes_no-1))\n",
    "err = loss(last_output, target)\n",
    "err.backward()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
