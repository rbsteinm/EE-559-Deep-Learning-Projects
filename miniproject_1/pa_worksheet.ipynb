{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pickle \n",
    "import json\n",
    "\n",
    "import dlc_bci as bci\n",
    "from helpers import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (10,10)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.set_num_threads(4)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Subset of data sampled at 100Hz\n",
    "train_input , train_target = bci.load(root = './data_bci')\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Full data sampled at 1Khz\n",
    "train_input , train_target = bci.load(root = './data_bci', one_khz = True)\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False, one_khz = True)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "Normalizing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_mean = train_input.mean(2).mean(0).unsqueeze(1).expand(-1,train_input.size(2))\n",
    "train_std = train_input.std(2).std(0).unsqueeze(1).expand(-1,train_input.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_mean = test_input.mean(2).mean(0).unsqueeze(1).expand(-1,test_input.size(2))\n",
    "test_std = test_input.std(2).std(0).unsqueeze(1).expand(-1,test_input.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# normalize mean to 0\n",
    "train_input.sub_(train_mean)\n",
    "test_input.sub_(test_mean)\n",
    "\n",
    "# normalize variance to 1\n",
    "train_input.div_(train_input.std())\n",
    "test_input.div_(test_input.std())\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window slicing for data augmentation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq = [ train_input[:,:,i::10] for i in range(10) ]\n",
    "train_input = torch.cat(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_target = torch.cat([train_target]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq = [ test_input[:,:,i::10] for i in range(10) ]\n",
    "test_input = torch.cat(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_target = torch.cat([test_target]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting training and validation sets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(train_input.size(0))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_input = train_input.clone()\n",
    "train_input = full_input[indices[:split].tolist()]\n",
    "validation_input = full_input[indices[split:].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_target = train_target.clone()\n",
    "train_target = full_target[indices[:split].tolist()]\n",
    "validation_target = full_target[indices[split:].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_pred_train = discrete_predictions(torch.FloatTensor(linear_reg.predict(train_input.view(train_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*((linear_pred_train - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_pred_test = discrete_predictions(torch.FloatTensor(linear_reg.predict(test_input.view(test_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*((linear_pred_test - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_reg = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_pred_train = discrete_predictions(torch.FloatTensor(ridge_reg.predict(train_input.view(train_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*((ridge_pred_train - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_pred_test = discrete_predictions(torch.FloatTensor(ridge_reg.predict(test_input.view(test_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*((ridge_pred_test - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_pred_train = torch.LongTensor(logistic_reg.predict(train_input.view(train_input.size(0),-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*((logistic_pred_train - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_pred_test = logistic_reg.predict(test_input.view(test_input.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*((logistic_pred_test - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP = nn.Sequential(\n",
    "        nn.Linear(1400, 140),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(140, 28),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(28, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(MLP, Variable(full_input.view(full_input.size(0),-1)), Variable(full_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_mlp = compute_nb_errors(MLP, Variable(full_input.view(full_input.size(0),-1)), Variable(full_target))\n",
    "print(100*(train_error_mlp/full_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_mlp = compute_nb_errors(MLP, Variable(test_input.view(test_input.size(0),-1)), Variable(test_target))\n",
    "print(100*(test_error_mlp/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Basic_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Basic_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(128, 28)\n",
    "        self.fc2 = nn.Linear(28, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=(3,2), stride=(3,2)))\n",
    "        x = F.tanh(self.conv3(x))\n",
    "        x = F.tanh(self.fc1(x.view(x.size(0),-1)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = Basic_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(cnn, Variable(full_input.unsqueeze(1)), Variable(full_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_cnn = compute_nb_errors(cnn, Variable(full_input.unsqueeze(1)), Variable(full_target))\n",
    "print(100*(train_error_cnn/full_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_cnn = compute_nb_errors(cnn, Variable(test_input.unsqueeze(1)), Variable(test_target))\n",
    "print(100*(test_error_cnn/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN w 1D filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 28, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(28, 14, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(14, 1, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(9, 36)\n",
    "        self.fc2 = nn.Linear(36, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.conv3(x))\n",
    "        x = self.fc1(x.squeeze(1))\n",
    "        x = self.fc2(F.tanh(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_1D = CNN_1D()\n",
    "train_model(cnn_1D, Variable(full_input), Variable(full_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_cnn_1D = compute_nb_errors(cnn_1D, Variable(full_input), Variable(full_target))\n",
    "print(100*(train_error_cnn_1D/full_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_cnn_1D = compute_nb_errors(cnn_1D, Variable(test_input), Variable(test_target))\n",
    "print(100*(test_error_cnn_1D/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### previous + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_dropout(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(CNN_dropout, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 28, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(28, 14, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(14, 1, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(9, 30)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(30, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.conv3(x))\n",
    "        x = self.fc1(x.squeeze(1))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(F.tanh(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn_drop = CNN_dropout()\n",
    "train_model(cnn_drop, Variable(full_input), Variable(full_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_cnn_drop = compute_nb_errors(cnn_drop, Variable(full_input), Variable(full_target))\n",
    "print(100*(train_error_cnn_drop/full_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_cnn_drop = compute_nb_errors(cnn_drop, Variable(test_input), Variable(test_target))\n",
    "print(100*(test_error_cnn_drop/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_batchnorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_batchnorm, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 28, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(28)\n",
    "        self.conv2 = nn.Conv1d(28, 14, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(14)\n",
    "        self.conv3 = nn.Conv1d(14, 1, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(1)\n",
    "        self.fc1 = nn.Linear(9, 30)\n",
    "        self.bn4 = nn.BatchNorm1d(30)\n",
    "        self.fc2 = nn.Linear(30, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.bn1(self.conv1(x)), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.bn2(self.conv2(x)), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.bn3(self.conv3(x)))\n",
    "        x = self.fc1(x.squeeze(1))\n",
    "        x = self.fc2(F.tanh(self.bn4(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_norm = CNN_batchnorm()\n",
    "train_model(cnn_norm, Variable(full_input), Variable(full_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_cnn_norm = compute_nb_errors(cnn_norm, Variable(full_input), Variable(full_target))\n",
    "print(100*(train_error_cnn_norm/full_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_cnn_norm = compute_nb_errors(cnn_norm, Variable(test_input), Variable(test_target))\n",
    "print(100*(test_error_cnn_norm/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both dropout and batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_both(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(CNN_both, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 28, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(28)\n",
    "        self.conv2 = nn.Conv1d(28, 14, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(14)\n",
    "        self.conv3 = nn.Conv1d(14, 1, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(1)\n",
    "        self.fc1 = nn.Linear(9, 30)\n",
    "        self.bn4 = nn.BatchNorm1d(30)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(30, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.bn1(self.conv1(x)), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.bn2(self.conv2(x)), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.bn3(self.conv3(x)))\n",
    "        x = self.fc1(x.squeeze(1))\n",
    "        x = self.dropout(self.bn4(x))\n",
    "        x = self.fc2(F.tanh(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_both = CNN_both()\n",
    "train_model(cnn_both, Variable(full_input), Variable(full_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_cnn_both = compute_nb_errors(cnn_both, Variable(full_input), Variable(full_target))\n",
    "print(100*(train_error_cnn_both/full_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_cnn_both = compute_nb_errors(cnn_both, Variable(test_input), Variable(test_target))\n",
    "print(100*(test_error_cnn_both/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_best_params(network, train_input, train_target, val_input, val_target, learning_rates, nb_epochs_total, epoch_step, mini_batch_size):\n",
    "    res = []\n",
    "    best_error = val_input.size(0)+1\n",
    "    #print(\"MLP\")\n",
    "    print(\"MODEL\", network().__class__.__name__)\n",
    "    print(\"=\"*18)\n",
    "    for lr in learning_rates: \n",
    "        print(\"  Learning rate\", lr, \":\")\n",
    "        print(\"-\"*23)\n",
    "        epoch_acc = []\n",
    "        torch.manual_seed(1)\n",
    "        model = network()\n",
    "        #model = nn.Sequential(\n",
    "        #    nn.Linear(1400, 140),\n",
    "        #    nn.Tanh(),\n",
    "        #    nn.Linear(140, 28),\n",
    "        #    nn.Tanh(),\n",
    "        #    nn.Linear(28, 2)\n",
    "        #)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        for e in range(nb_epochs_total):\n",
    "            sum_loss = 0\n",
    "            model.train()\n",
    "            for b in range(0, train_input.size(0), mini_batch_size):\n",
    "                output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "                loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "                sum_loss = sum_loss + loss.data[0]\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            if (e+1)%epoch_step == 0 or e==0:\n",
    "                validation_error = compute_nb_errors(model, val_input, val_target)\n",
    "                if (e+1)%(10*epoch_step) == 0 or e==0:\n",
    "                    print(\"    Epoch {:>4} : loss = {:1.8f} | validation error = {:>4}\".format(e+1, sum_loss, validation_error))\n",
    "                epoch_acc.append(100*(validation_error/val_input.size(0)))\n",
    "                if (validation_error < best_error) or ( (validation_error == best_error) and ((e+1) < best_epoch) ):\n",
    "                    best_error = validation_error\n",
    "                    best_epoch = e+1\n",
    "                    best_lr = lr\n",
    "                    \n",
    "        res.append(epoch_acc)\n",
    "    print(\"Done.\")\n",
    "    return res, best_epoch, best_lr, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.0025, 0.005, 0.0075, 0.01]\n",
    "nb_epochs_total = 500\n",
    "epoch_step = 5\n",
    "mini_batch_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "==================\n",
      "  Learning rate 0.001 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.58272851 | validation error =  266\n",
      "    Epoch   50 : loss = 0.02747743 | validation error =    1\n",
      "    Epoch  100 : loss = 0.00345947 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00154126 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00087404 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00054547 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00036133 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00025275 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00018162 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00013295 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00009861 | validation error =    0\n",
      "  Learning rate 0.0025 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 7.34417886 | validation error =  258\n",
      "    Epoch   50 : loss = 0.06056490 | validation error =    3\n",
      "    Epoch  100 : loss = 0.00642308 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00158234 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00078511 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00043369 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00028768 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00018278 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00012656 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00009290 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00006950 | validation error =    0\n",
      "  Learning rate 0.005 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 7.91710365 | validation error =  304\n",
      "    Epoch   50 : loss = 1.17758017 | validation error =   31\n",
      "    Epoch  100 : loss = 0.22221207 | validation error =    3\n",
      "    Epoch  150 : loss = 0.29805648 | validation error =   11\n",
      "    Epoch  200 : loss = 2.48091140 | validation error =   84\n",
      "    Epoch  250 : loss = 1.33387130 | validation error =   54\n",
      "    Epoch  300 : loss = 1.58557414 | validation error =   47\n",
      "    Epoch  350 : loss = 1.49120295 | validation error =   49\n",
      "    Epoch  400 : loss = 0.92642700 | validation error =   23\n",
      "    Epoch  450 : loss = 1.97704534 | validation error =   54\n",
      "    Epoch  500 : loss = 1.09550447 | validation error =   54\n",
      "  Learning rate 0.0075 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.51360637 | validation error =  221\n",
      "    Epoch   50 : loss = 1.35570261 | validation error =   44\n",
      "    Epoch  100 : loss = 1.58497602 | validation error =   58\n",
      "    Epoch  150 : loss = 2.34984440 | validation error =   76\n",
      "    Epoch  200 : loss = 2.46555337 | validation error =   49\n",
      "    Epoch  250 : loss = 2.28744882 | validation error =   62\n",
      "    Epoch  300 : loss = 1.25698131 | validation error =   23\n",
      "    Epoch  350 : loss = 2.06657006 | validation error =   73\n",
      "    Epoch  400 : loss = 2.17587188 | validation error =   83\n",
      "    Epoch  450 : loss = 1.91646652 | validation error =   76\n",
      "    Epoch  500 : loss = 1.74978118 | validation error =   80\n",
      "  Learning rate 0.01 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 7.02446103 | validation error =  234\n",
      "    Epoch   50 : loss = 4.05570415 | validation error =  112\n",
      "    Epoch  100 : loss = 2.61769988 | validation error =  103\n",
      "    Epoch  150 : loss = 2.25605474 | validation error =   90\n",
      "    Epoch  200 : loss = 2.71502692 | validation error =   54\n",
      "    Epoch  250 : loss = 2.09560867 | validation error =   55\n",
      "    Epoch  300 : loss = 1.64866436 | validation error =   39\n",
      "    Epoch  350 : loss = 1.88079852 | validation error =   40\n",
      "    Epoch  400 : loss = 1.60038717 | validation error =   66\n",
      "    Epoch  450 : loss = 1.10028414 | validation error =   28\n",
      "    Epoch  500 : loss = 0.87865715 | validation error =   36\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "all_results, best_epoch, best_lr, best_error = find_best_params(MLP, Variable(train_input.view(train_input.size(0),-1)), Variable(train_target), Variable(validation_input.view(validation_input.size(0),-1)), Variable(validation_target), learning_rates, nb_epochs_total, epoch_step, mini_batch_size)\n",
    "json_res = {\"model\":\"MLP\",   \"learning_rates\":learning_rates, \"nb_epochs_total\":nb_epochs_total, \"epoch_step\":epoch_step, \"mini_batch_size\":mini_batch_size, \"best_epoch\":best_epoch, \"best_lr\":best_lr, \"best_error\":best_error, \"results\":all_results}\n",
    "json.dump(json_res,open('results/adam/'+json_res[\"model\"]+'.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic CNN test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.0025, 0.005, 0.0075, 0.01]\n",
    "nb_epochs_total = 100\n",
    "epoch_step = 5\n",
    "mini_batch_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch  100 : loss = 0.00016045 | validation error =    1\n",
      "  Learning rate 0.01 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 7.07645875 | validation error =  298\n",
      "    Epoch   50 : loss = 0.00403607 | validation error =    1\n",
      "    Epoch  100 : loss = 0.00092013 | validation error =    1\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "all_results, best_epoch, best_lr, best_error = find_best_params(Basic_CNN, Variable(train_input.unsqueeze(1)), Variable(train_target), Variable(validation_input.unsqueeze(1)), Variable(validation_target), learning_rates, nb_epochs_total, epoch_step, mini_batch_size)\n",
    "json_res = {\"model\":\"Basic_CNN\",   \"learning_rates\":learning_rates, \"nb_epochs_total\":nb_epochs_total, \"epoch_step\":epoch_step, \"mini_batch_size\":mini_batch_size, \"best_epoch\":best_epoch, \"best_lr\":best_lr, \"best_error\":best_error, \"results\":all_results}\n",
    "json.dump(json_res,open('results/adam/'+json_res[\"model\"]+'.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the 4 other models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.0025, 0.005, 0.0075, 0.01]\n",
    "nb_epochs_total = 500\n",
    "epoch_step = 5\n",
    "mini_batch_size = 250\n",
    "\n",
    "models = [ CNN_1D, CNN_dropout, CNN_batchnorm, CNN_both ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CNN_1D\n",
      "==================\n",
      "  Learning rate 0.001 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.93182629 | validation error =  291\n",
      "    Epoch   50 : loss = 0.33676023 | validation error =   10\n",
      "    Epoch  100 : loss = 0.02029281 | validation error =    1\n",
      "    Epoch  150 : loss = 0.01263946 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00515445 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00284423 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00175310 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00114687 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00077883 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00054319 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00038605 | validation error =    0\n",
      "  Learning rate 0.0025 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.92652816 | validation error =  299\n",
      "    Epoch   50 : loss = 0.00904764 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00185936 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00078888 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00041950 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00025013 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00015969 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00010665 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00007351 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00005181 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00003710 | validation error =    0\n",
      "  Learning rate 0.005 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.91268241 | validation error =  228\n",
      "    Epoch   50 : loss = 0.00233369 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00055874 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00026002 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00014731 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00009194 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00006071 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00004165 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00002932 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00002104 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00001531 | validation error =    0\n",
      "  Learning rate 0.0075 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.90015680 | validation error =  224\n",
      "    Epoch   50 : loss = 0.00167385 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00043983 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00019483 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00010599 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00006424 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00004154 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00002803 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00001948 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00001382 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00000997 | validation error =    0\n",
      "  Learning rate 0.01 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.89811349 | validation error =  213\n",
      "    Epoch   50 : loss = 0.00139998 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00026271 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00013186 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00007902 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00005152 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00003533 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00002497 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00001802 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00001319 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00000975 | validation error =    0\n",
      "Done.\n",
      "\n",
      "MODEL CNN_dropout\n",
      "==================\n",
      "  Learning rate 0.001 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.93076003 | validation error =  309\n",
      "    Epoch   50 : loss = 0.38784362 | validation error =    9\n",
      "    Epoch  100 : loss = 0.09268174 | validation error =    2\n",
      "    Epoch  150 : loss = 0.00915618 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00526616 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00351215 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00224624 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00158159 | validation error =    1\n",
      "    Epoch  400 : loss = 0.00117378 | validation error =    1\n",
      "    Epoch  450 : loss = 0.00087133 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00056146 | validation error =    1\n",
      "  Learning rate 0.0025 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.92633766 | validation error =  308\n",
      "    Epoch   50 : loss = 0.02554662 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00576568 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00241916 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00152782 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00139886 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00074520 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00058801 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00026141 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00031133 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00015521 | validation error =    0\n",
      "  Learning rate 0.005 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.93111020 | validation error =  314\n",
      "    Epoch   50 : loss = 0.00359675 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00120497 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00063539 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00050893 | validation error =    0\n",
      "    Epoch  250 : loss = 0.17216128 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00383337 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00205845 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00112330 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00078863 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00047109 | validation error =    0\n",
      "  Learning rate 0.0075 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.94530612 | validation error =  304\n",
      "    Epoch   50 : loss = 0.00389780 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00094637 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00047790 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00038153 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00058831 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00017735 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00008290 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00005665 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00007873 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00003445 | validation error =    0\n",
      "  Learning rate 0.01 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.95453483 | validation error =  313\n",
      "    Epoch   50 : loss = 0.00222562 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00067874 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00033565 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00022737 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00060602 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00017172 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00007929 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00009133 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00003883 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00002959 | validation error =    0\n",
      "Done.\n",
      "\n",
      "MODEL CNN_batchnorm\n",
      "==================\n",
      "  Learning rate 0.001 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.69720048 | validation error =  320\n",
      "    Epoch   50 : loss = 0.03089847 | validation error =    2\n",
      "    Epoch  100 : loss = 0.00665729 | validation error =    1\n",
      "    Epoch  150 : loss = 0.00284194 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00151966 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00091161 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00058550 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00039324 | validation error =    1\n",
      "    Epoch  400 : loss = 0.00027236 | validation error =    1\n",
      "    Epoch  450 : loss = 0.00019288 | validation error =    1\n",
      "    Epoch  500 : loss = 0.00013886 | validation error =    1\n",
      "  Learning rate 0.0025 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.68886632 | validation error =  299\n",
      "    Epoch   50 : loss = 0.00483852 | validation error =    1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch  100 : loss = 0.00123714 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00054394 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00029474 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00017874 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00011590 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00007855 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00005485 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00003914 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00002838 | validation error =    0\n",
      "  Learning rate 0.005 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.67551619 | validation error =  247\n",
      "    Epoch   50 : loss = 0.00217162 | validation error =    1\n",
      "    Epoch  100 : loss = 0.00057067 | validation error =    1\n",
      "    Epoch  150 : loss = 0.00025403 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00013840 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00008406 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00005445 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00003679 | validation error =    1\n",
      "    Epoch  400 : loss = 0.00002564 | validation error =    1\n",
      "    Epoch  450 : loss = 0.00001827 | validation error =    1\n",
      "    Epoch  500 : loss = 0.00001321 | validation error =    1\n",
      "  Learning rate 0.0075 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.72024930 | validation error =  248\n",
      "    Epoch   50 : loss = 0.00201235 | validation error =    2\n",
      "    Epoch  100 : loss = 0.00047428 | validation error =    2\n",
      "    Epoch  150 : loss = 0.00020959 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00011455 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00006986 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00004547 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00003087 | validation error =    1\n",
      "    Epoch  400 : loss = 0.00002158 | validation error =    1\n",
      "    Epoch  450 : loss = 0.00001539 | validation error =    1\n",
      "    Epoch  500 : loss = 0.00001113 | validation error =    1\n",
      "  Learning rate 0.01 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.75308830 | validation error =  256\n",
      "    Epoch   50 : loss = 0.00230215 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00048784 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00021310 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00011552 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00006981 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00004518 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00003058 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00002133 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00001521 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00001101 | validation error =    0\n",
      "Done.\n",
      "\n",
      "MODEL CNN_both\n",
      "==================\n",
      "  Learning rate 0.001 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.81872702 | validation error =  320\n",
      "    Epoch   50 : loss = 0.27624991 | validation error =   56\n",
      "    Epoch  100 : loss = 0.01950400 | validation error =    3\n",
      "    Epoch  150 : loss = 0.00842463 | validation error =    4\n",
      "    Epoch  200 : loss = 0.00540241 | validation error =    3\n",
      "    Epoch  250 : loss = 0.00308240 | validation error =    3\n",
      "    Epoch  300 : loss = 0.00221305 | validation error =    3\n",
      "    Epoch  350 : loss = 0.00170226 | validation error =    3\n",
      "    Epoch  400 : loss = 0.00103406 | validation error =    3\n",
      "    Epoch  450 : loss = 0.00090529 | validation error =    3\n",
      "    Epoch  500 : loss = 0.00080296 | validation error =    3\n",
      "  Learning rate 0.0025 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.81706023 | validation error =  308\n",
      "    Epoch   50 : loss = 0.01650432 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00507541 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00243535 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00254844 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00082509 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00059321 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00068289 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00027478 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00029348 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00030552 | validation error =    0\n",
      "  Learning rate 0.005 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.86475396 | validation error =  272\n",
      "    Epoch   50 : loss = 0.00792036 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00221528 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00114931 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00278448 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00040161 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00028508 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00031895 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00012401 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00012056 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00019986 | validation error =    0\n",
      "  Learning rate 0.0075 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.91153252 | validation error =  282\n",
      "    Epoch   50 : loss = 0.16602148 | validation error =    6\n",
      "    Epoch  100 : loss = 0.00127320 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00065544 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00296914 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00027875 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00023925 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00022204 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00006504 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00007114 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00014392 | validation error =    0\n",
      "  Learning rate 0.01 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.88934731 | validation error =  247\n",
      "    Epoch   50 : loss = 0.03345517 | validation error =    4\n",
      "    Epoch  100 : loss = 0.00113633 | validation error =    1\n",
      "    Epoch  150 : loss = 0.00059468 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00280179 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00021011 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00017098 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00016705 | validation error =    1\n",
      "    Epoch  400 : loss = 0.00005291 | validation error =    1\n",
      "    Epoch  450 : loss = 0.00005732 | validation error =    1\n",
      "    Epoch  500 : loss = 0.00014149 | validation error =    1\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    all_results, best_epoch, best_lr, best_error = find_best_params(m, Variable(train_input), Variable(train_target), Variable(validation_input), Variable(validation_target), learning_rates, nb_epochs_total, epoch_step, mini_batch_size)\n",
    "    print(\"\")\n",
    "    json_res = {\"model\":m().__class__.__name__,   \"learning_rates\":learning_rates, \"nb_epochs_total\":nb_epochs_total, \"epoch_step\":epoch_step, \"mini_batch_size\":mini_batch_size, \"best_epoch\":best_epoch, \"best_lr\":best_lr, \"best_error\":best_error, \"results\":all_results}\n",
    "    json.dump(json_res,open('results/adam/'+json_res[\"model\"]+'.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "For 30 epochs, with a learning rate of 0.001\n",
      "31.2\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(1400, 140),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(140, 28),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(28, 2)\n",
    "        )\n",
    "json_res = json.load(open('results/adam/MLP.json','r'))\n",
    "print(\"MLP\")\n",
    "print(\"For\", json_res['best_epoch'], \"epochs, with a learning rate of\", json_res['best_lr'])\n",
    "train_model(model, Variable(train_input.view(train_input.size(0),-1)), Variable(train_target), json_res['mini_batch_size'], nb_epochs=json_res['best_epoch'], learning_rate=json_res['best_lr'], verbose=False)\n",
    "test_error = compute_nb_errors(model, Variable(test_input.view(test_input.size(0),-1)), Variable(test_target))\n",
    "print(100*(test_error/test_input.size(0)))\n",
    "json_res[\"test_error\"] = 100*(test_error/test_input.size(0))\n",
    "json.dump(json_res, open('results/adam/MLP.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic CNN\n",
      "For 25 epochs, with a learning rate of 0.005\n",
      "37.1\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model = Basic_CNN()\n",
    "json_res = json.load(open('results/adam/Basic_CNN.json','r'))\n",
    "print(\"Basic CNN\")\n",
    "print(\"For\", json_res['best_epoch'], \"epochs, with a learning rate of\", json_res['best_lr'])\n",
    "train_model(model, Variable(train_input.unsqueeze(1)), Variable(train_target), json_res['mini_batch_size'], nb_epochs=json_res['best_epoch'], learning_rate=json_res['best_lr'], verbose=False)\n",
    "test_error = compute_nb_errors(model, Variable(test_input.unsqueeze(1)), Variable(test_target))\n",
    "print(100*(test_error/test_input.size(0)))\n",
    "json_res[\"test_error\"] = 100*(test_error/test_input.size(0))\n",
    "json.dump(json_res, open('results/adam/Basic_CNN.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_1D\n",
      "For 20 epochs, with a learning rate of 0.005\n",
      "25.4\n",
      "==========\n",
      "CNN_dropout\n",
      "For 20 epochs, with a learning rate of 0.01\n",
      "25.900000000000002\n",
      "==========\n",
      "CNN_batchnorm\n",
      "For 30 epochs, with a learning rate of 0.005\n",
      "25.2\n",
      "==========\n",
      "CNN_both\n",
      "For 30 epochs, with a learning rate of 0.0025\n",
      "27.800000000000004\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "error_rates = []\n",
    "for m in models:\n",
    "    torch.manual_seed(1)\n",
    "    model = m()\n",
    "    json_res = json.load(open('results/adam/'+model.__class__.__name__+'.json','r'))\n",
    "    print(json_res[\"model\"])\n",
    "    print(\"For\", json_res['best_epoch'], \"epochs, with a learning rate of\", json_res['best_lr'])\n",
    "    train_model(model, Variable(train_input), Variable(train_target), json_res['mini_batch_size'], nb_epochs=json_res['best_epoch'], learning_rate=json_res['best_lr'], verbose=False)\n",
    "    test_error = compute_nb_errors(model, Variable(test_input), Variable(test_target))\n",
    "    print(100*(test_error/test_input.size(0)))\n",
    "    json_res[\"test_error\"] = 100*(test_error/test_input.size(0))\n",
    "    json.dump(json_res, open('results/adam/'+model.__class__.__name__+'.json','w'))\n",
    "    error_rates.append(100*(test_error/test_input.size(0)))\n",
    "    print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "colormap = 'Blues'\n",
    "normal_xticks = normal_results['nb_epochs']\n",
    "normal_yticks = normal_results['learning_rates']\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(np.array(normal_results['results'][0]).T, cmap=colormap)\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(normal_xticks)), normal_xticks)\n",
    "plt.yticks(range(len(normal_yticks)), normal_yticks)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.title('Without dropout layer and batch size 20')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(np.array(normal_results['results'][1]).T, cmap=colormap)\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(normal_xticks)), normal_xticks)\n",
    "plt.yticks(range(len(normal_yticks)), normal_yticks)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.title('Without dropout layer and batch size 40')\n",
    "\n",
    "dropout_xticks = dropout_results['nb_epochs']\n",
    "dropout_yticks = dropout_results['learning_rates']\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(dropout_results['results'][0], cmap=colormap)\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(dropout_xticks)), dropout_xticks)\n",
    "plt.yticks(range(len(dropout_yticks)), dropout_yticks)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.title('With dropout layer and batch size 20')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(dropout_results['results'][1], cmap=colormap)\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(dropout_xticks)), dropout_xticks)\n",
    "plt.yticks(range(len(dropout_yticks)), dropout_yticks)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.title('With dropout layer and batch size 40')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results_1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
