{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "from helpers import *\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n",
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "import dlc_bci as bci\n",
    "train_input , train_target = bci.load(root = './data_bci')\n",
    "print(str(type(train_input)), train_input.size()) \n",
    "print(str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(str(type(test_input)), test_input.size()) \n",
    "print(str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://documents.epfl.ch/users/f/fl/fleuret/www/data/bci/sp1s_aa_train_1000Hz.txt\n",
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 500])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n",
      "Downloading https://documents.epfl.ch/users/f/fl/fleuret/www/data/bci/sp1s_aa_test_1000Hz.txt\n",
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 500])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "import dlc_bci as bci\n",
    "train_input , train_target = bci.load(root = './data_bci', one_khz = True)\n",
    "print(str(type(train_input)), train_input.size()) \n",
    "print(str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False, one_khz = True)\n",
    "print(str(type(test_input)), test_input.size()) \n",
    "print(str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  41.8000   44.8000   47.1000  ...    69.8000   72.6000   76.1000\n",
       " -10.3000   -5.9000   -3.3000  ...    12.6000   24.0000   26.5000\n",
       "  38.1000   25.2000   46.0000  ...    45.1000   74.1000   64.8000\n",
       "             ...                â‹±                ...             \n",
       "   7.9000   11.2000   14.3000  ...    32.7000   43.4000   45.5000\n",
       "  19.2000   33.6000   33.8000  ...    46.7000   53.7000   43.4000\n",
       "  -0.4000   12.7000   12.0000  ...    30.7000   40.6000   33.1000\n",
       "[torch.FloatTensor of size 28x50]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.LongTensor of size 316]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pred = logistic_reg.predict(test_input.view(test_input.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "74.0"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "100*(1-(torch.LongTensor(pred) - test_target).abs().sum()/test_input.size(0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks\n",
    "\n",
    "#### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# normalize mean to 0\n",
    "train_input.sub_(train_input.mean())\n",
    "test_input.sub_(test_input.mean())\n",
    "\n",
    "# normalize variance to 1\n",
    "train_input.div_(train_input.std())\n",
    "test_input.div_(test_input.std())\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert targets to one hot labels\n",
    "train_target = convert_to_one_hot_labels(train_input, train_target)\n",
    "test_target = convert_to_one_hot_labels(test_input, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Basic_CNN(nn.Module):\n",
    "    def __init__(self, nb_hidden=64):\n",
    "        super(Basic_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=(1,5))\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=(1,5))\n",
    "        #self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(640, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"======\")\n",
    "        #print(x.size())\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        #print(x.size())\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        #print(x.size())\n",
    "        #x = self.dropout(x)\n",
    "        x = F.tanh(self.fc1(x.view(-1, 640)))\n",
    "        #print(x.size())\n",
    "        x = self.fc2(x)\n",
    "        #print(\"======\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "class experimental_CNN(nn.Module):\n",
    "    def __init__(self, nb_hidden=64):\n",
    "        super(experimental_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1,5))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1,3))\n",
    "        self.conv3 = nn.Conv2d(32, 32, kernel_size=(1,5))\n",
    "        #self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(896, nb_hidden)\n",
    "        self.fc2 = nn.Linear(nb_hidden, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(\"======\")\n",
    "        #print(x.size())\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=(1,2), stride=(1,2)))\n",
    "        #print(x.size())\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=(1,3), stride=(1,3)))\n",
    "        #print(x.size())\n",
    "        x = F.tanh(F.max_pool2d(self.conv3(x), kernel_size=(1,3), stride=(1,3)))\n",
    "        #x = self.dropout(x)\n",
    "        x = F.tanh(self.fc1(x.view(-1, 896)))\n",
    "        #print(x.size())\n",
    "        x = self.fc2(x)\n",
    "        #print(\"======\")\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "#cnn = Basic_CNN()\n",
    "cnn = experimental_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 55.095009446144104\n",
      "1 54.99918049573898\n",
      "2 54.92152577638626\n",
      "3 54.84894508123398\n",
      "4 54.780639827251434\n",
      "5 54.71426981687546\n",
      "6 54.647876620292664\n",
      "7 54.57976907491684\n",
      "8 54.50793778896332\n",
      "9 54.430730640888214\n",
      "10 54.34650015830994\n",
      "11 54.253846645355225\n",
      "12 54.15158933401108\n",
      "13 54.038151264190674\n",
      "14 53.91265952587128\n",
      "15 53.774734139442444\n",
      "16 53.62448137998581\n",
      "17 53.46393597126007\n",
      "18 53.29462254047394\n",
      "19 53.118749141693115\n",
      "20 52.93729496002197\n",
      "21 52.7488893866539\n",
      "22 52.551105320453644\n",
      "23 52.34062194824219\n",
      "24 52.11320811510086\n",
      "25 51.86375144124031\n",
      "26 51.58703017234802\n",
      "27 51.27950391173363\n",
      "28 50.93730083107948\n",
      "29 50.555081397295\n",
      "30 50.122004598379135\n",
      "31 49.63593265414238\n",
      "32 49.08892187476158\n",
      "33 48.47401976585388\n",
      "34 47.784880608320236\n",
      "35 47.01476362347603\n",
      "36 46.1605309844017\n",
      "37 45.216025829315186\n",
      "38 44.177474707365036\n",
      "39 43.04993197321892\n",
      "40 41.85715167224407\n",
      "41 40.62057623267174\n",
      "42 39.359276711940765\n",
      "43 38.090604677796364\n",
      "44 36.839844703674316\n",
      "45 35.61009614914656\n",
      "46 34.402923196554184\n",
      "47 33.20454475283623\n",
      "48 32.0074196010828\n",
      "49 30.79458413645625\n",
      "50 29.568408265709877\n",
      "51 28.321750942617655\n",
      "52 27.058682586997747\n",
      "53 25.76095699891448\n",
      "54 24.447042524814606\n",
      "55 23.098567551001906\n",
      "56 21.7328064981848\n",
      "57 20.35446566529572\n",
      "58 18.97455314360559\n",
      "59 17.63265263196081\n",
      "60 16.30439924262464\n",
      "61 15.007767985574901\n",
      "62 13.765139293856919\n",
      "63 12.557813172228634\n",
      "64 11.384983465541154\n",
      "65 10.255154702346772\n",
      "66 9.196573263034225\n",
      "67 8.215247260406613\n",
      "68 7.3224310092628\n",
      "69 6.517704235622659\n",
      "70 5.809853431535885\n",
      "71 5.180612564552575\n",
      "72 4.637276410590857\n",
      "73 4.161453976063058\n",
      "74 3.741453304537572\n",
      "75 3.3688698075711727\n",
      "76 3.0415177773102187\n",
      "77 2.752821707923431\n",
      "78 2.4987903970177285\n",
      "79 2.2742102858028375\n",
      "80 2.0747069633507635\n",
      "81 1.901008048618678\n",
      "82 1.7454082937329076\n",
      "83 1.608146947575733\n",
      "84 1.4862088221125305\n",
      "85 1.3773084461863618\n",
      "86 1.2799277716549113\n",
      "87 1.1926923883584095\n",
      "88 1.1140590213180985\n",
      "89 1.043263736370136\n",
      "90 0.9794635658763582\n",
      "91 0.9212824954156531\n",
      "92 0.8688790336746024\n",
      "93 0.8207620340108406\n",
      "94 0.7768346683733398\n",
      "95 0.7367475096398266\n",
      "96 0.6999045428747195\n",
      "97 0.6659605057502631\n",
      "98 0.6347340621578041\n",
      "99 0.6058592666158802\n"
     ]
    }
   ],
   "source": [
    "train_model(cnn, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4, learning_rate=0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0 % training accuracy\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn = compute_nb_errors(cnn, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "print(100*(1-(train_error_cnn/train_input.size(0))),'% training accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78.0 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn = compute_nb_errors(cnn, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)\n",
    "print(100*(1-(test_error_cnn/test_input.size(0))),'% test accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.LongTensor of size 100]"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output = cnn(Variable(test_input.view(-1, 1, 28, 50)))\n",
    "_, predicted_classes = torch.max(output.data, 1)\n",
    "predicted_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Variable containing:\n",
       "-0.7196  0.6930\n",
       " 0.6668 -0.6224\n",
       " 1.0499 -1.0930\n",
       " 0.4554 -0.4490\n",
       "-1.0458  1.0564\n",
       " 0.4398 -0.4385\n",
       " 0.2946 -0.2884\n",
       " 0.3804 -0.3914\n",
       " 0.6147 -0.6129\n",
       " 0.6399 -0.6136\n",
       " 0.0827 -0.0386\n",
       " 0.2206 -0.2163\n",
       "-0.2460  0.1662\n",
       "-0.8378  0.8327\n",
       "-0.6485  0.6702\n",
       " 0.2339 -0.2571\n",
       " 0.5485 -0.5420\n",
       "-0.3975  0.4288\n",
       "-0.6710  0.6800\n",
       " 1.3327 -1.3745\n",
       "-0.3952  0.3567\n",
       "-0.3144  0.3394\n",
       "-0.1057  0.0654\n",
       "-0.0207 -0.0128\n",
       " 0.1883 -0.2306\n",
       "-0.1562  0.1888\n",
       "-0.6425  0.6204\n",
       "-1.6709  1.6749\n",
       " 0.8789 -0.8450\n",
       " 0.1683 -0.1773\n",
       " 0.2310 -0.2369\n",
       " 1.4392 -1.4383\n",
       " 0.4689 -0.4099\n",
       "-1.2051  1.2111\n",
       " 0.1250 -0.1663\n",
       " 0.4937 -0.5129\n",
       "-0.7417  0.7450\n",
       "-0.3211  0.3298\n",
       "-0.2980  0.3249\n",
       "-0.3036  0.3132\n",
       "-0.3558  0.3548\n",
       "-0.9419  0.9164\n",
       " 1.0998 -1.0973\n",
       " 0.3867 -0.4003\n",
       " 0.2432 -0.2202\n",
       "-0.7651  0.7783\n",
       "-0.4237  0.4156\n",
       " 1.3667 -1.3871\n",
       "-0.2305  0.2244\n",
       "-0.4608  0.4342\n",
       "-1.9480  1.9512\n",
       "-0.7766  0.7487\n",
       " 0.1940 -0.1450\n",
       " 0.8935 -0.8784\n",
       " 1.0315 -1.0242\n",
       "-0.5146  0.4940\n",
       "-0.9630  0.9805\n",
       " 0.4162 -0.4086\n",
       " 0.8827 -0.8671\n",
       "-0.6418  0.6260\n",
       "-1.2385  1.2261\n",
       " 0.4285 -0.3528\n",
       "-0.4349  0.4168\n",
       "-0.0309  0.0652\n",
       " 0.6501 -0.6505\n",
       " 0.4378 -0.4821\n",
       " 0.1230 -0.1400\n",
       "-0.4222  0.3857\n",
       "-0.2670  0.2836\n",
       "-0.6728  0.6535\n",
       " 0.1378 -0.1419\n",
       "-1.8664  1.8355\n",
       "-0.7966  0.7579\n",
       "-0.1854  0.1356\n",
       " 0.8511 -0.8585\n",
       " 1.6094 -1.6122\n",
       "-0.3706  0.3715\n",
       " 0.5587 -0.6042\n",
       " 0.0715 -0.0980\n",
       "-0.4000  0.3689\n",
       " 0.5547 -0.5287\n",
       "-1.3629  1.4081\n",
       "-0.4460  0.4424\n",
       " 0.0035 -0.0067\n",
       "-0.1018  0.0954\n",
       "-0.6298  0.6225\n",
       " 0.1405 -0.1408\n",
       "-0.2443  0.2659\n",
       "-1.7519  1.7711\n",
       " 0.3237 -0.3375\n",
       " 0.5721 -0.5618\n",
       " 1.4896 -1.4873\n",
       "-0.8555  0.8547\n",
       " 0.2650 -0.2744\n",
       "-0.0446 -0.0073\n",
       "-0.8330  0.8186\n",
       " 0.4571 -0.4901\n",
       " 0.1428 -0.1261\n",
       " 0.0811 -0.0686\n",
       " 0.1522 -0.1805\n",
       "[torch.FloatTensor of size 100x2]"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = nn.Sequential(\n",
    "        nn.Linear(1400, 2000),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(2000, 1000),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(1000, 500),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(500, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 84.22748374938965\n",
      "1 78.6625856757164\n",
      "2 76.88258475065231\n",
      "3 75.6174096763134\n",
      "4 74.4524522125721\n",
      "5 73.27225422859192\n",
      "6 72.0347093641758\n",
      "7 70.72124433517456\n",
      "8 69.32641085982323\n",
      "9 67.85856041312218\n",
      "10 66.33831188082695\n",
      "11 64.79076832532883\n",
      "12 63.238619327545166\n",
      "13 61.699680134654045\n",
      "14 60.18659369647503\n",
      "15 58.70646731555462\n",
      "16 57.262306079268456\n",
      "17 55.854502737522125\n",
      "18 54.481747180223465\n",
      "19 53.14174821227789\n",
      "20 51.83167317509651\n",
      "21 50.54841583222151\n",
      "22 49.28881733492017\n",
      "23 48.049859169870615\n",
      "24 46.82878374494612\n",
      "25 45.62308695912361\n",
      "26 44.43051684927195\n",
      "27 43.24908941425383\n",
      "28 42.07707139104605\n",
      "29 40.9129038406536\n",
      "30 39.75506218150258\n",
      "31 38.60188074968755\n",
      "32 37.451371217146516\n",
      "33 36.3011418543756\n",
      "34 35.14850836805999\n",
      "35 33.991073086857796\n",
      "36 32.828062711283565\n",
      "37 31.661880346015096\n",
      "38 30.496470361948013\n",
      "39 29.326055778190494\n",
      "40 28.11888513341546\n",
      "41 26.809642999898642\n",
      "42 25.283866177604068\n",
      "43 27.8936515878886\n",
      "44 25.90353285893798\n",
      "45 24.137515405891463\n",
      "46 26.614787702681497\n",
      "47 19.512454811949283\n",
      "48 26.959604728152044\n",
      "49 24.559788373066112\n",
      "50 24.126482935855165\n",
      "51 21.90891512366943\n",
      "52 24.20096513815224\n",
      "53 25.136377278948203\n",
      "54 26.431601520976983\n",
      "55 16.806533028837293\n",
      "56 19.92869182676077\n",
      "57 21.897819264733698\n",
      "58 20.25950303359423\n",
      "59 21.468874339479953\n",
      "60 18.377803095267154\n",
      "61 17.8231231670361\n",
      "62 20.58625188935548\n",
      "63 17.456613284070045\n",
      "64 14.718780209077522\n",
      "65 19.848996008804534\n",
      "66 13.379573480982799\n",
      "67 19.621529886266217\n",
      "68 12.345481812488288\n",
      "69 15.073628757847473\n",
      "70 11.950407367752632\n",
      "71 8.015390528133139\n",
      "72 26.007308118161745\n",
      "73 7.844267739303177\n",
      "74 14.27870593679836\n",
      "75 8.429812430636957\n",
      "76 6.906730450107716\n",
      "77 11.537795767973876\n",
      "78 5.753530302667059\n",
      "79 9.922485827177297\n",
      "80 10.002816835185513\n",
      "81 7.095969573478214\n",
      "82 6.733146003040019\n",
      "83 12.017459306414821\n",
      "84 8.250159446615726\n",
      "85 7.122882539319107\n",
      "86 5.216738820599858\n",
      "87 9.510830021463335\n",
      "88 8.277917151834117\n",
      "89 9.824000173131935\n",
      "90 13.838865953606728\n",
      "91 8.408570075385796\n",
      "92 3.361519630125258\n",
      "93 2.889499995362712\n",
      "94 1.5916201253130566\n",
      "95 1.4425257325638086\n",
      "96 2.0942940415116027\n",
      "97 6.262975302190171\n",
      "98 19.138477490574587\n",
      "99 1.2087780717411079\n"
     ]
    }
   ],
   "source": [
    "train_model(MLP, Variable(train_input.view(train_input.size(0),-1)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85.44303797468355 % training accuracy\n"
     ]
    }
   ],
   "source": [
    "train_error_mlp = compute_nb_errors(MLP, Variable(train_input.view(train_input.size(0),-1)), Variable(train_target), 4)\n",
    "print(100*(1-(train_error_mlp/train_input.size(0))),'% training accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70.0 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "test_error_mlp = compute_nb_errors(MLP, Variable(test_input.view(test_input.size(0),-1)), Variable(test_target), 4)\n",
    "print(100*(1-(test_error_mlp/test_input.size(0))),'% test accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_parameters(model, step_sizes, nb_epochs):\n",
    "    for eta in step_sizes:\n",
    "        for epoch in nb_epochs:\n",
    "            model = Basic_CNN()\n",
    "            train_model(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
