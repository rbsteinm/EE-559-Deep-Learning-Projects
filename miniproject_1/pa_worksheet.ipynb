{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pickle \n",
    "import json\n",
    "\n",
    "import dlc_bci as bci\n",
    "from helpers import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (10,10)\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input : <class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "Train target : <class 'torch.LongTensor'> torch.Size([316])\n",
      "Test input : <class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "Test target : <class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Subset of data sampled at 100Hz\n",
    "train_input , train_target = bci.load(root = './data_bci')\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input : <class 'torch.FloatTensor'> torch.Size([316, 28, 500])\n",
      "Train target : <class 'torch.LongTensor'> torch.Size([316])\n",
      "Test input : <class 'torch.FloatTensor'> torch.Size([100, 28, 500])\n",
      "Test target : <class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Full data sampled at 1Khz\n",
    "train_input , train_target = bci.load(root = './data_bci', one_khz = True)\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False, one_khz = True)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  41.8000   44.8000   47.1000  ...    69.8000   72.6000   76.1000\n",
       " -10.3000   -5.9000   -3.3000  ...    12.6000   24.0000   26.5000\n",
       "  38.1000   25.2000   46.0000  ...    45.1000   74.1000   64.8000\n",
       "             ...                â‹±                ...             \n",
       "   7.9000   11.2000   14.3000  ...    32.7000   43.4000   45.5000\n",
       "  19.2000   33.6000   33.8000  ...    46.7000   53.7000   43.4000\n",
       "  -0.4000   12.7000   12.0000  ...    30.7000   40.6000   33.1000\n",
       "[torch.FloatTensor of size 28x50]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.LongTensor of size 316]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# normalize mean to 0\n",
    "train_input.sub_(train_input.mean())\n",
    "test_input.sub_(test_input.mean())\n",
    "\n",
    "# normalize variance to 1\n",
    "train_input.div_(train_input.std())\n",
    "test_input.div_(test_input.std())\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq = []\n",
    "for i in range(10):\n",
    "    seq.append(train_input[:,:,i::10])\n",
    "train_input = torch.cat(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = torch.cat([train_target]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq = []\n",
    "for i in range(10):\n",
    "    seq.append(test_input[:,:,i::10])\n",
    "test_input = torch.cat(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_target = torch.cat([test_target]*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_pred_train = discrete_predictions(torch.FloatTensor(linear_reg.predict(train_input.view(train_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % train error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(linear_pred_train) - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_pred_test = discrete_predictions(torch.FloatTensor(linear_reg.predict(test_input.view(test_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0 % test error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(linear_pred_test) - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_pred_train = logistic_reg.predict(train_input.view(train_input.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % train error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(logistic_pred_train) - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_pred_test = logistic_reg.predict(test_input.view(test_input.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.000000000000004 % test error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(logistic_pred_test) - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert targets to one hot labels\n",
    "#train_target = convert_to_one_hot_labels(train_input, train_target)\n",
    "#test_target = convert_to_one_hot_labels(test_input, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP_big = nn.Sequential(\n",
    "        nn.Linear(14000, 1400),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(1400, 280),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(280, 28),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(28, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP = nn.Sequential(\n",
    "        nn.Linear(50, 280),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(280, 140),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(140, 28),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(28, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1 at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/THNN/generic/SpatialClassNLLCriterion.c:60",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-967537cad33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Cours/Deep Learning/DL_projects/miniproject_1/helpers.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_input, train_target, mini_batch_size, nb_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                         \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[0;32m--> 679\u001b[0;31m                                self.ignore_index, self.reduce)\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \"\"\"\n\u001b[0;32m-> 1161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1 at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/THNN/generic/SpatialClassNLLCriterion.c:60"
     ]
    }
   ],
   "source": [
    "train_model(MLP, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 55.861808478832245\n",
      "Epoch 1 loss = 54.59076404571533\n",
      "Epoch 2 loss = 53.797380566596985\n",
      "Epoch 3 loss = 53.191094636917114\n",
      "Epoch 4 loss = 52.66807159781456\n",
      "Epoch 5 loss = 52.18098449707031\n",
      "Epoch 6 loss = 51.69751679897308\n",
      "Epoch 7 loss = 51.194580405950546\n",
      "Epoch 8 loss = 50.65688365697861\n",
      "Epoch 9 loss = 50.074628949165344\n",
      "Epoch 10 loss = 49.44200122356415\n",
      "Epoch 11 loss = 48.756370306015015\n",
      "Epoch 12 loss = 48.019282549619675\n",
      "Epoch 13 loss = 47.23957505822182\n",
      "Epoch 14 loss = 46.418880090117455\n",
      "Epoch 15 loss = 45.549498587846756\n",
      "Epoch 16 loss = 44.65137520432472\n",
      "Epoch 17 loss = 43.72418200969696\n",
      "Epoch 18 loss = 42.76705865561962\n",
      "Epoch 19 loss = 41.78091523051262\n",
      "Epoch 20 loss = 40.76593078672886\n",
      "Epoch 21 loss = 39.72083881497383\n",
      "Epoch 22 loss = 38.644645400345325\n",
      "Epoch 23 loss = 37.53784937411547\n",
      "Epoch 24 loss = 36.39436551183462\n",
      "Epoch 25 loss = 35.19167825952172\n",
      "Epoch 26 loss = 33.98767836764455\n",
      "Epoch 27 loss = 32.73168759420514\n",
      "Epoch 28 loss = 31.445499220862985\n",
      "Epoch 29 loss = 30.16045959852636\n",
      "Epoch 30 loss = 28.89358583278954\n",
      "Epoch 31 loss = 27.635679287835956\n",
      "Epoch 32 loss = 26.451356172561646\n",
      "Epoch 33 loss = 24.993193933740258\n",
      "Epoch 34 loss = 22.329513754695654\n",
      "Epoch 35 loss = 21.300788438413292\n",
      "Epoch 36 loss = 21.69328035786748\n",
      "Epoch 37 loss = 19.40192669071257\n",
      "Epoch 38 loss = 21.74633002281189\n",
      "Epoch 39 loss = 22.328185795107856\n",
      "Epoch 40 loss = 19.27624101890251\n",
      "Epoch 41 loss = 21.494215379934758\n",
      "Epoch 42 loss = 18.539624228607863\n",
      "Epoch 43 loss = 14.740391871193424\n",
      "Epoch 44 loss = 12.809250887483358\n",
      "Epoch 45 loss = 23.509267386049032\n",
      "Epoch 46 loss = 21.448826832696795\n",
      "Epoch 47 loss = 13.998251458164304\n",
      "Epoch 48 loss = 12.360645934939384\n",
      "Epoch 49 loss = 19.829397476278245\n",
      "Epoch 50 loss = 12.925528926309198\n",
      "Epoch 51 loss = 9.004871571436524\n",
      "Epoch 52 loss = 12.712735519977286\n",
      "Epoch 53 loss = 15.814191723242402\n",
      "Epoch 54 loss = 8.620287388330325\n",
      "Epoch 55 loss = 7.815711344475858\n",
      "Epoch 56 loss = 19.2838870389387\n",
      "Epoch 57 loss = 7.382501360960305\n",
      "Epoch 58 loss = 10.198833256727085\n",
      "Epoch 59 loss = 13.357606679695891\n",
      "Epoch 60 loss = 6.52746866340749\n",
      "Epoch 61 loss = 12.122200927638914\n",
      "Epoch 62 loss = 9.533251075306907\n",
      "Epoch 63 loss = 13.620927825162653\n",
      "Epoch 64 loss = 19.769558089785278\n",
      "Epoch 65 loss = 8.031806251266971\n",
      "Epoch 66 loss = 6.8842518598539755\n",
      "Epoch 67 loss = 6.117898587137461\n",
      "Epoch 68 loss = 5.8591331308707595\n",
      "Epoch 69 loss = 3.542390557762701\n",
      "Epoch 70 loss = 14.470347541675437\n",
      "Epoch 71 loss = 8.51938004483236\n",
      "Epoch 72 loss = 4.182998319272883\n",
      "Epoch 73 loss = 2.651830873102881\n",
      "Epoch 74 loss = 1.7161602322594263\n",
      "Epoch 75 loss = 16.01654776475334\n",
      "Epoch 76 loss = 5.953169251966756\n",
      "Epoch 77 loss = 2.997427246242296\n",
      "Epoch 78 loss = 9.255936907837167\n",
      "Epoch 79 loss = 14.021566861658357\n",
      "Epoch 80 loss = 7.792811411316507\n",
      "Epoch 81 loss = 2.29853199608624\n",
      "Epoch 82 loss = 3.015809479460586\n",
      "Epoch 83 loss = 1.5350954779423773\n",
      "Epoch 84 loss = 0.8139297832094599\n",
      "Epoch 85 loss = 7.46518522471888\n",
      "Epoch 86 loss = 18.022797207639087\n",
      "Epoch 87 loss = 16.096968480589567\n",
      "Epoch 88 loss = 4.842176103615202\n",
      "Epoch 89 loss = 11.231128733430523\n",
      "Epoch 90 loss = 6.885814970242791\n",
      "Epoch 91 loss = 3.763526126218494\n",
      "Epoch 92 loss = 4.0822326296474785\n",
      "Epoch 93 loss = 8.046630994882435\n",
      "Epoch 94 loss = 3.4680433393805288\n",
      "Epoch 95 loss = 1.4004087385255843\n",
      "Epoch 96 loss = 1.8394939813879319\n",
      "Epoch 97 loss = 0.8679366677533835\n",
      "Epoch 98 loss = 0.4531381952401716\n",
      "Epoch 99 loss = 0.32211697948514484\n"
     ]
    }
   ],
   "source": [
    "train_model(MLP, Variable(train_input.view(train_input.size(0),-1)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.455696202531644 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_mlp = compute_nb_errors(MLP, Variable(test_input.view(test_input.size(0),-1)), Variable(train_target), 4)\n",
    "print(100*(train_error_mlp/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.999999999999996 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_mlp = compute_nb_errors(MLP, Variable(test_input.view(test_input.size(0),-1)), Variable(test_target), 4)\n",
    "print(100*(test_error_mlp/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Basic_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Basic_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(640, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.fc1(x.view(-1, 640)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = Basic_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 54.498005986213684\n",
      "Epoch 1 loss = 53.519576370716095\n",
      "Epoch 2 loss = 52.730925261974335\n",
      "Epoch 3 loss = 52.011470079422\n",
      "Epoch 4 loss = 51.306885719299316\n",
      "Epoch 5 loss = 50.57355135679245\n",
      "Epoch 6 loss = 49.77597540616989\n",
      "Epoch 7 loss = 48.90287631750107\n",
      "Epoch 8 loss = 47.96535176038742\n",
      "Epoch 9 loss = 46.977213621139526\n",
      "Epoch 10 loss = 45.9560324549675\n",
      "Epoch 11 loss = 44.91541266441345\n",
      "Epoch 12 loss = 43.863889276981354\n",
      "Epoch 13 loss = 42.80109569430351\n",
      "Epoch 14 loss = 41.72726050019264\n",
      "Epoch 15 loss = 40.64559432864189\n",
      "Epoch 16 loss = 39.55902707576752\n",
      "Epoch 17 loss = 38.47445222735405\n",
      "Epoch 18 loss = 37.40180164575577\n",
      "Epoch 19 loss = 36.34577456116676\n",
      "Epoch 20 loss = 35.31309676170349\n",
      "Epoch 21 loss = 34.30517861247063\n",
      "Epoch 22 loss = 33.31542217731476\n",
      "Epoch 23 loss = 32.36064922809601\n",
      "Epoch 24 loss = 31.455952614545822\n",
      "Epoch 25 loss = 30.621112316846848\n",
      "Epoch 26 loss = 29.83599954843521\n",
      "Epoch 27 loss = 29.091609865427017\n",
      "Epoch 28 loss = 28.385593682527542\n",
      "Epoch 29 loss = 27.71784856915474\n",
      "Epoch 30 loss = 27.08013343811035\n",
      "Epoch 31 loss = 26.471470445394516\n",
      "Epoch 32 loss = 25.892269998788834\n",
      "Epoch 33 loss = 25.33688971400261\n",
      "Epoch 34 loss = 24.80695317685604\n",
      "Epoch 35 loss = 24.297536432743073\n",
      "Epoch 36 loss = 23.8085807710886\n",
      "Epoch 37 loss = 23.3407344520092\n",
      "Epoch 38 loss = 22.890871182084084\n",
      "Epoch 39 loss = 22.462048381567\n",
      "Epoch 40 loss = 22.04953905940056\n",
      "Epoch 41 loss = 21.65613017976284\n",
      "Epoch 42 loss = 21.275663554668427\n",
      "Epoch 43 loss = 20.906475737690926\n",
      "Epoch 44 loss = 20.54767796397209\n",
      "Epoch 45 loss = 20.202588871121407\n",
      "Epoch 46 loss = 19.865014672279358\n",
      "Epoch 47 loss = 19.53286224603653\n",
      "Epoch 48 loss = 19.20836329460144\n",
      "Epoch 49 loss = 18.891260266304016\n",
      "Epoch 50 loss = 18.583666563034058\n",
      "Epoch 51 loss = 18.287342324852943\n",
      "Epoch 52 loss = 18.00163134932518\n",
      "Epoch 53 loss = 17.72761470079422\n",
      "Epoch 54 loss = 17.46428184211254\n",
      "Epoch 55 loss = 17.210159450769424\n",
      "Epoch 56 loss = 16.9658125936985\n",
      "Epoch 57 loss = 16.72996048629284\n",
      "Epoch 58 loss = 16.501679465174675\n",
      "Epoch 59 loss = 16.28134447336197\n",
      "Epoch 60 loss = 16.06744134426117\n",
      "Epoch 61 loss = 15.863640859723091\n",
      "Epoch 62 loss = 15.66768342256546\n",
      "Epoch 63 loss = 15.479718893766403\n",
      "Epoch 64 loss = 15.301828876137733\n",
      "Epoch 65 loss = 15.132765635848045\n",
      "Epoch 66 loss = 14.971958667039871\n",
      "Epoch 67 loss = 14.817156955599785\n",
      "Epoch 68 loss = 14.671055093407631\n",
      "Epoch 69 loss = 14.533242762088776\n",
      "Epoch 70 loss = 14.402028396725655\n",
      "Epoch 71 loss = 14.277263030409813\n",
      "Epoch 72 loss = 14.158192783594131\n",
      "Epoch 73 loss = 14.04104134440422\n",
      "Epoch 74 loss = 13.922865718603134\n",
      "Epoch 75 loss = 13.79942874610424\n",
      "Epoch 76 loss = 13.673272088170052\n",
      "Epoch 77 loss = 13.547573789954185\n",
      "Epoch 78 loss = 13.423323675990105\n",
      "Epoch 79 loss = 13.303480252623558\n",
      "Epoch 80 loss = 13.190399900078773\n",
      "Epoch 81 loss = 13.084183022379875\n",
      "Epoch 82 loss = 12.985377386212349\n",
      "Epoch 83 loss = 12.893768697977066\n",
      "Epoch 84 loss = 12.808810248970985\n",
      "Epoch 85 loss = 12.729156851768494\n",
      "Epoch 86 loss = 12.655004635453224\n",
      "Epoch 87 loss = 12.585136085748672\n",
      "Epoch 88 loss = 12.519690230488777\n",
      "Epoch 89 loss = 12.458024248480797\n",
      "Epoch 90 loss = 12.399600893259048\n",
      "Epoch 91 loss = 12.344199568033218\n",
      "Epoch 92 loss = 12.291459187865257\n",
      "Epoch 93 loss = 12.241384372115135\n",
      "Epoch 94 loss = 12.193539321422577\n",
      "Epoch 95 loss = 12.148138523101807\n",
      "Epoch 96 loss = 12.104768931865692\n",
      "Epoch 97 loss = 12.063279837369919\n",
      "Epoch 98 loss = 12.023726433515549\n",
      "Epoch 99 loss = 11.985645055770874\n"
     ]
    }
   ],
   "source": [
    "train_model(cnn, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31645569620253167 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn = compute_nb_errors(cnn, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "print(100*(train_error_cnn/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.7 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn = compute_nb_errors(cnn, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)\n",
    "print(100*(test_error_cnn/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN w 1D filters\n",
    "\n",
    "v1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_1D_conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_1D_conv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1,5))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1,3))\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(1,5))\n",
    "        self.fc1 = nn.Linear(896*2, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=(1,2), stride=(1,2)))\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=(1,3), stride=(1,3)))\n",
    "        x = F.tanh(F.max_pool2d(self.conv3(x), kernel_size=(1,3), stride=(1,3)))\n",
    "        x = F.tanh(self.fc1(x.view(-1, 896*2)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 55.29474902153015\n",
      "Epoch 1 loss = 55.05597680807114\n",
      "Epoch 2 loss = 54.855808675289154\n",
      "Epoch 3 loss = 54.682802975177765\n",
      "Epoch 4 loss = 54.524170219898224\n",
      "Epoch 5 loss = 54.372238516807556\n",
      "Epoch 6 loss = 54.2225626707077\n",
      "Epoch 7 loss = 54.07223069667816\n",
      "Epoch 8 loss = 53.92045372724533\n",
      "Epoch 9 loss = 53.7664595246315\n",
      "Epoch 10 loss = 53.6088582277298\n",
      "Epoch 11 loss = 53.44771748781204\n",
      "Epoch 12 loss = 53.28130882978439\n",
      "Epoch 13 loss = 53.10787773132324\n",
      "Epoch 14 loss = 52.924775540828705\n",
      "Epoch 15 loss = 52.73032104969025\n",
      "Epoch 16 loss = 52.52050358057022\n",
      "Epoch 17 loss = 52.2923142015934\n",
      "Epoch 18 loss = 52.04249811172485\n",
      "Epoch 19 loss = 51.76665839552879\n",
      "Epoch 20 loss = 51.46092280745506\n",
      "Epoch 21 loss = 51.12171161174774\n",
      "Epoch 22 loss = 50.74609687924385\n",
      "Epoch 23 loss = 50.327125549316406\n",
      "Epoch 24 loss = 49.860064417123795\n",
      "Epoch 25 loss = 49.33736914396286\n",
      "Epoch 26 loss = 48.7524134516716\n",
      "Epoch 27 loss = 48.09661191701889\n",
      "Epoch 28 loss = 47.363799035549164\n",
      "Epoch 29 loss = 46.548118352890015\n",
      "Epoch 30 loss = 45.64110191166401\n",
      "Epoch 31 loss = 44.62833517789841\n",
      "Epoch 32 loss = 43.5095302015543\n",
      "Epoch 33 loss = 42.288945361971855\n",
      "Epoch 34 loss = 40.98556010425091\n",
      "Epoch 35 loss = 39.624012991786\n",
      "Epoch 36 loss = 38.224144004285336\n",
      "Epoch 37 loss = 36.83618910610676\n",
      "Epoch 38 loss = 35.50140971690416\n",
      "Epoch 39 loss = 34.220379054546356\n",
      "Epoch 40 loss = 32.987858686596155\n",
      "Epoch 41 loss = 31.78924137726426\n",
      "Epoch 42 loss = 30.60187002643943\n",
      "Epoch 43 loss = 29.414099875837564\n",
      "Epoch 44 loss = 28.20890563353896\n",
      "Epoch 45 loss = 26.9710533618927\n",
      "Epoch 46 loss = 25.724096946418285\n",
      "Epoch 47 loss = 24.45225503668189\n",
      "Epoch 48 loss = 23.141171157360077\n",
      "Epoch 49 loss = 21.80590709671378\n",
      "Epoch 50 loss = 20.482658119872212\n",
      "Epoch 51 loss = 19.184796921908855\n",
      "Epoch 52 loss = 17.91424261406064\n",
      "Epoch 53 loss = 16.68403071537614\n",
      "Epoch 54 loss = 15.473992392420769\n",
      "Epoch 55 loss = 14.307200957089663\n",
      "Epoch 56 loss = 13.158826867118478\n",
      "Epoch 57 loss = 12.065468588843942\n",
      "Epoch 58 loss = 11.004322789609432\n",
      "Epoch 59 loss = 10.01800771523267\n",
      "Epoch 60 loss = 9.092921589501202\n",
      "Epoch 61 loss = 8.246936591342092\n",
      "Epoch 62 loss = 7.473065987229347\n",
      "Epoch 63 loss = 6.765459856018424\n",
      "Epoch 64 loss = 6.132833236362785\n",
      "Epoch 65 loss = 5.56078403769061\n",
      "Epoch 66 loss = 5.051177802029997\n",
      "Epoch 67 loss = 4.5967433960177\n",
      "Epoch 68 loss = 4.186151412315667\n",
      "Epoch 69 loss = 3.8177803866565228\n",
      "Epoch 70 loss = 3.485889774048701\n",
      "Epoch 71 loss = 3.189264864195138\n",
      "Epoch 72 loss = 2.920124291209504\n",
      "Epoch 73 loss = 2.6755923714954406\n",
      "Epoch 74 loss = 2.4577202387154102\n",
      "Epoch 75 loss = 2.259887665626593\n",
      "Epoch 76 loss = 2.082024419447407\n",
      "Epoch 77 loss = 1.9214594420045614\n",
      "Epoch 78 loss = 1.7766964577604085\n",
      "Epoch 79 loss = 1.6465725870220922\n",
      "Epoch 80 loss = 1.5289256813121028\n",
      "Epoch 81 loss = 1.422422197414562\n",
      "Epoch 82 loss = 1.3253437340026721\n",
      "Epoch 83 loss = 1.238339542527683\n",
      "Epoch 84 loss = 1.1580220417818055\n",
      "Epoch 85 loss = 1.085460348345805\n",
      "Epoch 86 loss = 1.019526947668055\n",
      "Epoch 87 loss = 0.9589260969660245\n",
      "Epoch 88 loss = 0.9038629011774901\n",
      "Epoch 89 loss = 0.8529104291810654\n",
      "Epoch 90 loss = 0.8064452380058356\n",
      "Epoch 91 loss = 0.7638936454895884\n",
      "Epoch 92 loss = 0.7245852873311378\n",
      "Epoch 93 loss = 0.6883455333882011\n",
      "Epoch 94 loss = 0.6548019158944953\n",
      "Epoch 95 loss = 0.6237556178239174\n",
      "Epoch 96 loss = 0.5950728290190455\n",
      "Epoch 97 loss = 0.5683711224410217\n",
      "Epoch 98 loss = 0.5436698693956714\n",
      "Epoch 99 loss = 0.5205931734817568\n"
     ]
    }
   ],
   "source": [
    "train_model(cnn2, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn2 = compute_nb_errors(cnn2, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "print(100*(train_error_cnn2/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn2 = compute_nb_errors(cnn2, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)\n",
    "print(100*(test_error_cnn2/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 56, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(56, 112, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(112, 112, kernel_size=5)\n",
    "        #self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(112, 56)\n",
    "        self.fc2 = nn.Linear(56, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.conv2(x), kernel_size=3, stride=3))\n",
    "        x = F.tanh(F.max_pool1d(self.conv3(x), kernel_size=3, stride=3))\n",
    "        #x = self.dropout(x)\n",
    "        x = F.tanh(self.fc1(x.view(-1, 112)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 54.80717808008194\n",
      "Epoch 1 loss = 54.70321995019913\n",
      "Epoch 2 loss = 54.62144333124161\n",
      "Epoch 3 loss = 54.54609006643295\n",
      "Epoch 4 loss = 54.47215002775192\n",
      "Epoch 5 loss = 54.397279500961304\n",
      "Epoch 6 loss = 54.31979596614838\n",
      "Epoch 7 loss = 54.23857867717743\n",
      "Epoch 8 loss = 54.152719497680664\n",
      "Epoch 9 loss = 54.06127589941025\n",
      "Epoch 10 loss = 53.963149666786194\n",
      "Epoch 11 loss = 53.85764080286026\n",
      "Epoch 12 loss = 53.74411725997925\n",
      "Epoch 13 loss = 53.621960520744324\n",
      "Epoch 14 loss = 53.490806221961975\n",
      "Epoch 15 loss = 53.3504199385643\n",
      "Epoch 16 loss = 53.20103591680527\n",
      "Epoch 17 loss = 53.042898178100586\n",
      "Epoch 18 loss = 52.87592488527298\n",
      "Epoch 19 loss = 52.69968330860138\n",
      "Epoch 20 loss = 52.512857258319855\n",
      "Epoch 21 loss = 52.313249707221985\n",
      "Epoch 22 loss = 52.09775102138519\n",
      "Epoch 23 loss = 51.862221360206604\n",
      "Epoch 24 loss = 51.600734174251556\n",
      "Epoch 25 loss = 51.30662363767624\n",
      "Epoch 26 loss = 50.97217923402786\n",
      "Epoch 27 loss = 50.58827191591263\n",
      "Epoch 28 loss = 50.145546674728394\n",
      "Epoch 29 loss = 49.635470151901245\n",
      "Epoch 30 loss = 49.052971839904785\n",
      "Epoch 31 loss = 48.39720916748047\n",
      "Epoch 32 loss = 47.67743617296219\n",
      "Epoch 33 loss = 46.91091322898865\n",
      "Epoch 34 loss = 46.12178260087967\n",
      "Epoch 35 loss = 45.33747237920761\n",
      "Epoch 36 loss = 44.585057497024536\n",
      "Epoch 37 loss = 43.88276046514511\n",
      "Epoch 38 loss = 43.23992794752121\n",
      "Epoch 39 loss = 42.65469256043434\n",
      "Epoch 40 loss = 42.11176699399948\n",
      "Epoch 41 loss = 41.59381613135338\n",
      "Epoch 42 loss = 41.08444428443909\n",
      "Epoch 43 loss = 40.574191838502884\n",
      "Epoch 44 loss = 40.05659916996956\n",
      "Epoch 45 loss = 39.525228440761566\n",
      "Epoch 46 loss = 38.97643005847931\n",
      "Epoch 47 loss = 38.40751928091049\n",
      "Epoch 48 loss = 37.81886240839958\n",
      "Epoch 49 loss = 37.20732679963112\n",
      "Epoch 50 loss = 36.56616833806038\n",
      "Epoch 51 loss = 35.89628407359123\n",
      "Epoch 52 loss = 35.19750505685806\n",
      "Epoch 53 loss = 34.467127710580826\n",
      "Epoch 54 loss = 33.70280182361603\n",
      "Epoch 55 loss = 32.90799552202225\n",
      "Epoch 56 loss = 32.079257011413574\n",
      "Epoch 57 loss = 31.2220561504364\n",
      "Epoch 58 loss = 30.336877077817917\n",
      "Epoch 59 loss = 29.43371231853962\n",
      "Epoch 60 loss = 28.521252885460854\n",
      "Epoch 61 loss = 27.609381422400475\n",
      "Epoch 62 loss = 26.705064982175827\n",
      "Epoch 63 loss = 25.81715965270996\n",
      "Epoch 64 loss = 24.946798160672188\n",
      "Epoch 65 loss = 24.0969417989254\n",
      "Epoch 66 loss = 23.268715247511864\n",
      "Epoch 67 loss = 22.465265810489655\n",
      "Epoch 68 loss = 21.68551456928253\n",
      "Epoch 69 loss = 20.929760172963142\n",
      "Epoch 70 loss = 20.197027780115604\n",
      "Epoch 71 loss = 19.48685085773468\n",
      "Epoch 72 loss = 18.79366946965456\n",
      "Epoch 73 loss = 18.11371350288391\n",
      "Epoch 74 loss = 17.448977544903755\n",
      "Epoch 75 loss = 16.79688136279583\n",
      "Epoch 76 loss = 16.153860576450825\n",
      "Epoch 77 loss = 15.521179631352425\n",
      "Epoch 78 loss = 14.890969168394804\n",
      "Epoch 79 loss = 14.270723462104797\n",
      "Epoch 80 loss = 13.655808195471764\n",
      "Epoch 81 loss = 13.047254204750061\n",
      "Epoch 82 loss = 12.443563140928745\n",
      "Epoch 83 loss = 11.85163190215826\n",
      "Epoch 84 loss = 11.260812729597092\n",
      "Epoch 85 loss = 10.682115264236927\n",
      "Epoch 86 loss = 10.107771836221218\n",
      "Epoch 87 loss = 9.542313646525145\n",
      "Epoch 88 loss = 8.990537386387587\n",
      "Epoch 89 loss = 8.456313788890839\n",
      "Epoch 90 loss = 7.941506223753095\n",
      "Epoch 91 loss = 7.44299029186368\n",
      "Epoch 92 loss = 6.962817462161183\n",
      "Epoch 93 loss = 6.505012100562453\n",
      "Epoch 94 loss = 6.072084203362465\n",
      "Epoch 95 loss = 5.656321510672569\n",
      "Epoch 96 loss = 5.265139950439334\n",
      "Epoch 97 loss = 4.898181421682239\n",
      "Epoch 98 loss = 4.551967371255159\n",
      "Epoch 99 loss = 4.229025145992637\n",
      "Epoch 100 loss = 3.929819437675178\n",
      "Epoch 101 loss = 3.6509207440540195\n",
      "Epoch 102 loss = 3.3982019387185574\n",
      "Epoch 103 loss = 3.1606581825762987\n",
      "Epoch 104 loss = 2.943011302500963\n",
      "Epoch 105 loss = 2.744522529654205\n",
      "Epoch 106 loss = 2.5625202348455787\n",
      "Epoch 107 loss = 2.3965079355984926\n",
      "Epoch 108 loss = 2.244069124571979\n",
      "Epoch 109 loss = 2.1043527480214834\n",
      "Epoch 110 loss = 1.9743241369724274\n",
      "Epoch 111 loss = 1.8554803794249892\n",
      "Epoch 112 loss = 1.7458152016624808\n",
      "Epoch 113 loss = 1.645016006194055\n",
      "Epoch 114 loss = 1.5512454747222364\n",
      "Epoch 115 loss = 1.465625619981438\n",
      "Epoch 116 loss = 1.3858725586906075\n",
      "Epoch 117 loss = 1.3124052952043712\n",
      "Epoch 118 loss = 1.2445298633538187\n",
      "Epoch 119 loss = 1.1818531164899468\n"
     ]
    }
   ],
   "source": [
    "cnn4 = CNN_1D()\n",
    "train_model(cnn4, Variable(train_input), Variable(train_target), 40, nb_epochs=120, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09493670886075949 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn4 = compute_nb_errors(cnn4, Variable(train_input), Variable(train_target), 40)\n",
    "print(100*(train_error_cnn4/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.5 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn4 = compute_nb_errors(cnn4, Variable(test_input), Variable(test_target), 40)\n",
    "print(100*(1-(test_error_cnn4/test_input.size(0))),'% test accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### previous + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_dropout, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 56, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(56, 112, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(112, 112, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(112, 56)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(56, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.conv2(x), kernel_size=3, stride=3))\n",
    "        x = F.tanh(F.max_pool1d(self.conv3(x), kernel_size=3, stride=3))\n",
    "        x = self.fc1(x.view(-1, 112))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(F.tanh(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 54.81373769044876\n",
      "Epoch 1 loss = 54.668377578258514\n",
      "Epoch 2 loss = 54.62901657819748\n",
      "Epoch 3 loss = 54.515659153461456\n",
      "Epoch 4 loss = 54.52439606189728\n",
      "Epoch 5 loss = 54.237469136714935\n",
      "Epoch 6 loss = 54.175403356552124\n",
      "Epoch 7 loss = 53.991609275341034\n",
      "Epoch 8 loss = 54.01105797290802\n",
      "Epoch 9 loss = 53.85242795944214\n",
      "Epoch 10 loss = 53.63609594106674\n",
      "Epoch 11 loss = 53.5738622546196\n",
      "Epoch 12 loss = 53.413128554821014\n",
      "Epoch 13 loss = 53.16618990898132\n",
      "Epoch 14 loss = 53.06097322702408\n",
      "Epoch 15 loss = 52.77126497030258\n",
      "Epoch 16 loss = 52.46380579471588\n",
      "Epoch 17 loss = 52.251897275447845\n",
      "Epoch 18 loss = 52.10526090860367\n",
      "Epoch 19 loss = 51.59744983911514\n",
      "Epoch 20 loss = 51.36152005195618\n",
      "Epoch 21 loss = 51.03365182876587\n",
      "Epoch 22 loss = 50.29474097490311\n",
      "Epoch 23 loss = 49.955027401447296\n",
      "Epoch 24 loss = 49.26357388496399\n",
      "Epoch 25 loss = 48.516477048397064\n",
      "Epoch 26 loss = 47.920060098171234\n",
      "Epoch 27 loss = 47.34015101194382\n",
      "Epoch 28 loss = 46.479187071323395\n",
      "Epoch 29 loss = 45.700314462184906\n",
      "Epoch 30 loss = 44.72383964061737\n",
      "Epoch 31 loss = 44.17614609003067\n",
      "Epoch 32 loss = 43.38053569197655\n",
      "Epoch 33 loss = 42.77121365070343\n",
      "Epoch 34 loss = 42.21555379033089\n",
      "Epoch 35 loss = 41.67405363917351\n",
      "Epoch 36 loss = 41.0138064622879\n",
      "Epoch 37 loss = 40.74573215842247\n",
      "Epoch 38 loss = 39.836212903261185\n",
      "Epoch 39 loss = 39.55124592781067\n",
      "Epoch 40 loss = 38.96957564353943\n",
      "Epoch 41 loss = 38.38907963037491\n",
      "Epoch 42 loss = 37.75908660888672\n",
      "Epoch 43 loss = 37.14352938532829\n",
      "Epoch 44 loss = 36.45495381951332\n",
      "Epoch 45 loss = 35.88319593667984\n",
      "Epoch 46 loss = 34.8772489130497\n",
      "Epoch 47 loss = 34.17909014225006\n",
      "Epoch 48 loss = 33.52619290351868\n",
      "Epoch 49 loss = 32.5400710105896\n",
      "Epoch 50 loss = 31.758291870355606\n",
      "Epoch 51 loss = 31.05525651574135\n",
      "Epoch 52 loss = 30.072126001119614\n",
      "Epoch 53 loss = 29.31058633327484\n",
      "Epoch 54 loss = 28.287872210144997\n",
      "Epoch 55 loss = 27.471184015274048\n",
      "Epoch 56 loss = 26.46474976837635\n",
      "Epoch 57 loss = 25.680248767137527\n",
      "Epoch 58 loss = 24.95379027724266\n",
      "Epoch 59 loss = 24.195579141378403\n",
      "Epoch 60 loss = 23.45066736638546\n",
      "Epoch 61 loss = 22.522661462426186\n",
      "Epoch 62 loss = 21.853763937950134\n",
      "Epoch 63 loss = 20.866757094860077\n",
      "Epoch 64 loss = 20.337877228856087\n",
      "Epoch 65 loss = 19.709520764648914\n",
      "Epoch 66 loss = 18.942652329802513\n",
      "Epoch 67 loss = 18.20479527115822\n",
      "Epoch 68 loss = 17.33526001125574\n",
      "Epoch 69 loss = 16.81096263974905\n",
      "Epoch 70 loss = 16.007733650505543\n",
      "Epoch 71 loss = 15.481175780296326\n",
      "Epoch 72 loss = 15.118314653635025\n",
      "Epoch 73 loss = 14.511324524879456\n",
      "Epoch 74 loss = 13.760365635156631\n",
      "Epoch 75 loss = 13.099373191595078\n",
      "Epoch 76 loss = 12.453268885612488\n",
      "Epoch 77 loss = 11.958859086036682\n",
      "Epoch 78 loss = 11.293652899563313\n",
      "Epoch 79 loss = 10.891858391463757\n",
      "Epoch 80 loss = 10.670656442642212\n",
      "Epoch 81 loss = 10.054422814399004\n",
      "Epoch 82 loss = 9.269512955099344\n",
      "Epoch 83 loss = 8.906087685376406\n",
      "Epoch 84 loss = 8.168687107041478\n",
      "Epoch 85 loss = 7.794401986524463\n",
      "Epoch 86 loss = 7.4571825712919235\n",
      "Epoch 87 loss = 7.203381430357695\n",
      "Epoch 88 loss = 6.675417762249708\n",
      "Epoch 89 loss = 6.440882029011846\n",
      "Epoch 90 loss = 5.986491912975907\n",
      "Epoch 91 loss = 5.671619711443782\n",
      "Epoch 92 loss = 5.2231133952736855\n",
      "Epoch 93 loss = 4.983916163444519\n",
      "Epoch 94 loss = 4.549118289723992\n",
      "Epoch 95 loss = 4.518581433221698\n",
      "Epoch 96 loss = 4.184387966059148\n",
      "Epoch 97 loss = 3.8590672304853797\n",
      "Epoch 98 loss = 3.671477422118187\n",
      "Epoch 99 loss = 3.4763677520677447\n",
      "Epoch 100 loss = 3.1308714458718896\n",
      "Epoch 101 loss = 2.99355923011899\n",
      "Epoch 102 loss = 2.7842510994523764\n",
      "Epoch 103 loss = 5.583516075275838\n",
      "Epoch 104 loss = 2.4561389591544867\n",
      "Epoch 105 loss = 2.333383840508759\n",
      "Epoch 106 loss = 2.210252503864467\n",
      "Epoch 107 loss = 2.144850231707096\n",
      "Epoch 108 loss = 1.9758856864646077\n",
      "Epoch 109 loss = 1.8891642959788442\n",
      "Epoch 110 loss = 1.7669282350689173\n",
      "Epoch 111 loss = 1.61881736619398\n",
      "Epoch 112 loss = 1.5223373123444617\n",
      "Epoch 113 loss = 1.5418616137467325\n",
      "Epoch 114 loss = 1.4277844629250467\n",
      "Epoch 115 loss = 1.3539427537471056\n",
      "Epoch 116 loss = 1.3008825723081827\n",
      "Epoch 117 loss = 1.2581399232149124\n",
      "Epoch 118 loss = 1.2164256237447262\n",
      "Epoch 119 loss = 1.072476220317185\n",
      "Epoch 120 loss = 1.071033216547221\n",
      "Epoch 121 loss = 1.0725259415339679\n",
      "Epoch 122 loss = 0.9851158270612359\n",
      "Epoch 123 loss = 0.9462492645252496\n",
      "Epoch 124 loss = 0.9438748294487596\n",
      "Epoch 125 loss = 0.9186782687902451\n",
      "Epoch 126 loss = 0.8748916639015079\n",
      "Epoch 127 loss = 0.8157111203763634\n",
      "Epoch 128 loss = 0.8245638785883784\n",
      "Epoch 129 loss = 0.7964270401280373\n",
      "Epoch 130 loss = 0.7138981120660901\n",
      "Epoch 131 loss = 0.6987031803000718\n",
      "Epoch 132 loss = 0.7169414346572012\n",
      "Epoch 133 loss = 0.6530320809688419\n",
      "Epoch 134 loss = 0.6218856945633888\n",
      "Epoch 135 loss = 0.6409058675635606\n",
      "Epoch 136 loss = 0.59526975476183\n",
      "Epoch 137 loss = 0.6066255230689421\n",
      "Epoch 138 loss = 0.5718848146498203\n",
      "Epoch 139 loss = 0.5297152248676866\n"
     ]
    }
   ],
   "source": [
    "cnn3 = CNN_dropout()\n",
    "train_model(cnn3, Variable(train_input), Variable(train_target), 20, nb_epochs=140, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn3 = compute_nb_errors(cnn3, Variable(train_input), Variable(train_target), 40)\n",
    "print(100*(train_error_cnn3/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.1 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn3 = compute_nb_errors(cnn3, Variable(test_input), Variable(test_target))\n",
    "print(100*(1-(test_error_cnn3/test_input.size(0))),'% test accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_sizes = [20, 40]\n",
    "learning_rates = [0.01, 0.0075, 0.005, 0.0025, 0.001]\n",
    "nb_epochs = [100, 110, 120, 130, 140, 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT DROPOUT\n",
      "Batch size 20\n",
      "==============\n",
      "     100 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.26239770650864\n",
      "            Epoch 10 loss = 87.35559633374214\n",
      "            Epoch 20 loss = 29.273580979555845\n",
      "            Epoch 30 loss = 3.4526535307522863\n",
      "            Epoch 40 loss = 0.8238349893363193\n",
      "            Epoch 50 loss = 0.36731731047620997\n",
      "            Epoch 60 loss = 0.22038837501895614\n",
      "            Epoch 70 loss = 0.1540524371812353\n",
      "            Epoch 80 loss = 0.11730458836245816\n",
      "            Epoch 90 loss = 0.09412753221113235\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.53126090765\n",
      "            Epoch 10 loss = 98.59997701644897\n",
      "            Epoch 20 loss = 60.63624921441078\n",
      "            Epoch 30 loss = 18.417776821181178\n",
      "            Epoch 40 loss = 3.300629359902814\n",
      "            Epoch 50 loss = 1.032872662995942\n",
      "            Epoch 60 loss = 0.49333338884753175\n",
      "            Epoch 70 loss = 0.29543064860627055\n",
      "            Epoch 80 loss = 0.20496040883881506\n",
      "            Epoch 90 loss = 0.15500706841703504\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.59150433540344\n",
      "            Epoch 10 loss = 103.38775956630707\n",
      "            Epoch 20 loss = 81.07911455631256\n",
      "            Epoch 30 loss = 46.00190983712673\n",
      "            Epoch 40 loss = 18.180700791999698\n",
      "            Epoch 50 loss = 5.071393890772015\n",
      "            Epoch 60 loss = 1.9003021612297744\n",
      "            Epoch 70 loss = 0.8992983910720795\n",
      "            Epoch 80 loss = 0.5457645792630501\n",
      "            Epoch 90 loss = 0.37807772649102844\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.49709230661392\n",
      "            Epoch 10 loss = 107.37256002426147\n",
      "            Epoch 20 loss = 102.59197014570236\n",
      "            Epoch 30 loss = 90.07705864310265\n",
      "            Epoch 40 loss = 78.27435711026192\n",
      "            Epoch 50 loss = 64.80661743879318\n",
      "            Epoch 60 loss = 47.093069434165955\n",
      "            Epoch 70 loss = 30.86269821226597\n",
      "            Epoch 80 loss = 17.84489591792226\n",
      "            Epoch 90 loss = 8.965906047262251\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.52510005235672\n",
      "            Epoch 10 loss = 108.94973629713058\n",
      "            Epoch 20 loss = 108.36450803279877\n",
      "            Epoch 30 loss = 107.59590619802475\n",
      "            Epoch 40 loss = 106.54223400354385\n",
      "            Epoch 50 loss = 105.13099944591522\n",
      "            Epoch 60 loss = 103.19310754537582\n",
      "            Epoch 70 loss = 100.09640657901764\n",
      "            Epoch 80 loss = 94.72407898306847\n",
      "            Epoch 90 loss = 87.48452967405319\n",
      "      For 100 epochs : [76.8, 73.8, 74.7, 73.4, 69.8]\n",
      "     110 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.51135629415512\n",
      "            Epoch 10 loss = 85.11633035540581\n",
      "            Epoch 20 loss = 27.73670467734337\n",
      "            Epoch 30 loss = 3.029438914731145\n",
      "            Epoch 40 loss = 0.7301638577482663\n",
      "            Epoch 50 loss = 0.3472374341217801\n",
      "            Epoch 60 loss = 0.2161354735144414\n",
      "            Epoch 70 loss = 0.1536087165150093\n",
      "            Epoch 80 loss = 0.11785333220905159\n",
      "            Epoch 90 loss = 0.09494049737986643\n",
      "            Epoch 100 loss = 0.07910489041387336\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.21122533082962\n",
      "            Epoch 10 loss = 90.68756893277168\n",
      "            Epoch 20 loss = 46.324748173356056\n",
      "            Epoch 30 loss = 10.60089641995728\n",
      "            Epoch 40 loss = 2.287317111855373\n",
      "            Epoch 50 loss = 0.7780623835278675\n",
      "            Epoch 60 loss = 0.39880555181298405\n",
      "            Epoch 70 loss = 0.2536020201223437\n",
      "            Epoch 80 loss = 0.18235580129839946\n",
      "            Epoch 90 loss = 0.14098363986704499\n",
      "            Epoch 100 loss = 0.11420035832270514\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.59313678741455\n",
      "            Epoch 10 loss = 103.1334639787674\n",
      "            Epoch 20 loss = 84.66455629467964\n",
      "            Epoch 30 loss = 55.275839656591415\n",
      "            Epoch 40 loss = 24.453213658183813\n",
      "            Epoch 50 loss = 7.159588501788676\n",
      "            Epoch 60 loss = 2.462039712932892\n",
      "            Epoch 70 loss = 1.2023000821936876\n",
      "            Epoch 80 loss = 0.684080587758217\n",
      "            Epoch 90 loss = 0.4453802878560964\n",
      "            Epoch 100 loss = 0.3213124443136621\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 110.26359337568283\n",
      "            Epoch 10 loss = 107.27280557155609\n",
      "            Epoch 20 loss = 103.78146809339523\n",
      "            Epoch 30 loss = 94.11041688919067\n",
      "            Epoch 40 loss = 80.10972613096237\n",
      "            Epoch 50 loss = 66.06647503376007\n",
      "            Epoch 60 loss = 48.04657384753227\n",
      "            Epoch 70 loss = 31.18941330909729\n",
      "            Epoch 80 loss = 17.922683896496892\n",
      "            Epoch 90 loss = 9.307389665395021\n",
      "            Epoch 100 loss = 4.871215416584164\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.42439287900925\n",
      "            Epoch 10 loss = 108.33290356397629\n",
      "            Epoch 20 loss = 107.26255488395691\n",
      "            Epoch 30 loss = 105.97636622190475\n",
      "            Epoch 40 loss = 104.40896564722061\n",
      "            Epoch 50 loss = 102.13122421503067\n",
      "            Epoch 60 loss = 98.1102322936058\n",
      "            Epoch 70 loss = 91.41327068209648\n",
      "            Epoch 80 loss = 84.05899116396904\n",
      "            Epoch 90 loss = 78.43948689103127\n",
      "            Epoch 100 loss = 73.28005756437778\n",
      "      For 110 epochs : [73.8, 72.2, 71.6, 71.7, 71.50000000000001]\n",
      "     120 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.33954054117203\n",
      "            Epoch 10 loss = 88.95104831457138\n",
      "            Epoch 20 loss = 29.583589512854815\n",
      "            Epoch 30 loss = 3.576523730996996\n",
      "            Epoch 40 loss = 0.8272107474622317\n",
      "            Epoch 50 loss = 0.34217565951985307\n",
      "            Epoch 60 loss = 0.20117059792391956\n",
      "            Epoch 70 loss = 0.13983733508212026\n",
      "            Epoch 80 loss = 0.10613988096883986\n",
      "            Epoch 90 loss = 0.08495210919500096\n",
      "            Epoch 100 loss = 0.07050064733630279\n",
      "            Epoch 110 loss = 0.06004626919457223\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.3211213350296\n",
      "            Epoch 10 loss = 95.12313133478165\n",
      "            Epoch 20 loss = 51.758888438344\n",
      "            Epoch 30 loss = 12.688811164349318\n",
      "            Epoch 40 loss = 2.4692856492474675\n",
      "            Epoch 50 loss = 0.8286505435826257\n",
      "            Epoch 60 loss = 0.41267726686783135\n",
      "            Epoch 70 loss = 0.25525985716376454\n",
      "            Epoch 80 loss = 0.18013736401917413\n",
      "            Epoch 90 loss = 0.13755597069393843\n",
      "            Epoch 100 loss = 0.11049488466233015\n",
      "            Epoch 110 loss = 0.09190730296541005\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.59779751300812\n",
      "            Epoch 10 loss = 103.71494746208191\n",
      "            Epoch 20 loss = 83.46086114645004\n",
      "            Epoch 30 loss = 52.33801659941673\n",
      "            Epoch 40 loss = 22.896995320916176\n",
      "            Epoch 50 loss = 6.606267931871116\n",
      "            Epoch 60 loss = 2.0875947633758187\n",
      "            Epoch 70 loss = 0.9650038053514436\n",
      "            Epoch 80 loss = 0.5670202603796497\n",
      "            Epoch 90 loss = 0.38511064572958276\n",
      "            Epoch 100 loss = 0.28549248341005296\n",
      "            Epoch 110 loss = 0.2240935679874383\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.52622425556183\n",
      "            Epoch 10 loss = 107.47157788276672\n",
      "            Epoch 20 loss = 103.63430446386337\n",
      "            Epoch 30 loss = 93.16664415597916\n",
      "            Epoch 40 loss = 79.94392976164818\n",
      "            Epoch 50 loss = 65.77411982417107\n",
      "            Epoch 60 loss = 47.02631939202547\n",
      "            Epoch 70 loss = 29.89090909808874\n",
      "            Epoch 80 loss = 16.461278945207596\n",
      "            Epoch 90 loss = 8.051386162638664\n",
      "            Epoch 100 loss = 4.1116436258889735\n",
      "            Epoch 110 loss = 2.369678360177204\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.73817306756973\n",
      "            Epoch 10 loss = 108.92421078681946\n",
      "            Epoch 20 loss = 108.15279448032379\n",
      "            Epoch 30 loss = 107.17358458042145\n",
      "            Epoch 40 loss = 105.87871193885803\n",
      "            Epoch 50 loss = 104.10844349861145\n",
      "            Epoch 60 loss = 101.31773644685745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 70 loss = 96.2366686463356\n",
      "            Epoch 80 loss = 88.43559655547142\n",
      "            Epoch 90 loss = 81.34048029780388\n",
      "            Epoch 100 loss = 75.77934151887894\n",
      "            Epoch 110 loss = 70.32732009887695\n",
      "      For 120 epochs : [72.7, 73.8, 72.1, 76.3, 70.7]\n",
      "     130 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.41912245750427\n",
      "            Epoch 10 loss = 89.2061228454113\n",
      "            Epoch 20 loss = 29.748647063970566\n",
      "            Epoch 30 loss = 3.4543337021023035\n",
      "            Epoch 40 loss = 0.8478749920614064\n",
      "            Epoch 50 loss = 0.35630048502935097\n",
      "            Epoch 60 loss = 0.20666124668787234\n",
      "            Epoch 70 loss = 0.14177108833973762\n",
      "            Epoch 80 loss = 0.10677547605882864\n",
      "            Epoch 90 loss = 0.08511241612723097\n",
      "            Epoch 100 loss = 0.07045971160550835\n",
      "            Epoch 110 loss = 0.05992539228464011\n",
      "            Epoch 120 loss = 0.05200195697398158\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.15305829048157\n",
      "            Epoch 10 loss = 91.8991006910801\n",
      "            Epoch 20 loss = 47.86110046505928\n",
      "            Epoch 30 loss = 10.336570682935417\n",
      "            Epoch 40 loss = 2.109043473144993\n",
      "            Epoch 50 loss = 0.7552615426830016\n",
      "            Epoch 60 loss = 0.4068705244571902\n",
      "            Epoch 70 loss = 0.2659091118257493\n",
      "            Epoch 80 loss = 0.1936101824248908\n",
      "            Epoch 90 loss = 0.15059762910823338\n",
      "            Epoch 100 loss = 0.12239792483887868\n",
      "            Epoch 110 loss = 0.10262664058973314\n",
      "            Epoch 120 loss = 0.08802390179334907\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.51589125394821\n",
      "            Epoch 10 loss = 103.19141989946365\n",
      "            Epoch 20 loss = 83.09210923314095\n",
      "            Epoch 30 loss = 52.50960059463978\n",
      "            Epoch 40 loss = 22.076795529574156\n",
      "            Epoch 50 loss = 6.162675324827433\n",
      "            Epoch 60 loss = 2.302048502722755\n",
      "            Epoch 70 loss = 1.1022385370451957\n",
      "            Epoch 80 loss = 0.6438154260977171\n",
      "            Epoch 90 loss = 0.429407246701885\n",
      "            Epoch 100 loss = 0.3138679915864486\n",
      "            Epoch 110 loss = 0.24422541467356496\n",
      "            Epoch 120 loss = 0.19840114630642347\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.73865395784378\n",
      "            Epoch 10 loss = 107.27918821573257\n",
      "            Epoch 20 loss = 102.11931222677231\n",
      "            Epoch 30 loss = 88.2502712905407\n",
      "            Epoch 40 loss = 76.38900458812714\n",
      "            Epoch 50 loss = 61.9489713460207\n",
      "            Epoch 60 loss = 43.91553493589163\n",
      "            Epoch 70 loss = 28.6010176949203\n",
      "            Epoch 80 loss = 16.22612714767456\n",
      "            Epoch 90 loss = 8.412237823009491\n",
      "            Epoch 100 loss = 4.4460413423366845\n",
      "            Epoch 110 loss = 2.568923344835639\n",
      "            Epoch 120 loss = 1.6510524593759328\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.72667372226715\n",
      "            Epoch 10 loss = 109.0060288310051\n",
      "            Epoch 20 loss = 108.34453356266022\n",
      "            Epoch 30 loss = 107.51736015081406\n",
      "            Epoch 40 loss = 106.38915151357651\n",
      "            Epoch 50 loss = 104.85490638017654\n",
      "            Epoch 60 loss = 102.63380748033524\n",
      "            Epoch 70 loss = 98.86616849899292\n",
      "            Epoch 80 loss = 92.48658663034439\n",
      "            Epoch 90 loss = 84.99448281526566\n",
      "            Epoch 100 loss = 79.11528906226158\n",
      "            Epoch 110 loss = 73.92963525652885\n",
      "            Epoch 120 loss = 68.48537628352642\n",
      "      For 130 epochs : [73.6, 73.5, 76.7, 74.5, 70.30000000000001]\n",
      "     140 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.54256916046143\n",
      "            Epoch 10 loss = 92.19341304898262\n",
      "            Epoch 20 loss = 37.21160572767258\n",
      "            Epoch 30 loss = 4.623604403808713\n",
      "            Epoch 40 loss = 0.9395581346470863\n",
      "            Epoch 50 loss = 0.3794339745945763\n",
      "            Epoch 60 loss = 0.21969840244855732\n",
      "            Epoch 70 loss = 0.150348510404001\n",
      "            Epoch 80 loss = 0.11300435419252608\n",
      "            Epoch 90 loss = 0.08992603563820012\n",
      "            Epoch 100 loss = 0.07433183456305414\n",
      "            Epoch 110 loss = 0.0631312471232377\n",
      "            Epoch 120 loss = 0.05470631671778392\n",
      "            Epoch 130 loss = 0.04816185816162033\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.80230861902237\n",
      "            Epoch 10 loss = 99.76811307668686\n",
      "            Epoch 20 loss = 66.8306023478508\n",
      "            Epoch 30 loss = 23.846710227429867\n",
      "            Epoch 40 loss = 4.758076078025624\n",
      "            Epoch 50 loss = 1.3080898963380605\n",
      "            Epoch 60 loss = 0.5583072758163325\n",
      "            Epoch 70 loss = 0.32379702359321527\n",
      "            Epoch 80 loss = 0.2210900217469316\n",
      "            Epoch 90 loss = 0.16554462758358568\n",
      "            Epoch 100 loss = 0.13122055954590905\n",
      "            Epoch 110 loss = 0.108059216523543\n",
      "            Epoch 120 loss = 0.09146791165403556\n",
      "            Epoch 130 loss = 0.0790589493335574\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.30878913402557\n",
      "            Epoch 10 loss = 102.08372813463211\n",
      "            Epoch 20 loss = 80.65448519587517\n",
      "            Epoch 30 loss = 48.44690761715174\n",
      "            Epoch 40 loss = 20.499981524422765\n",
      "            Epoch 50 loss = 6.192693277262151\n",
      "            Epoch 60 loss = 2.396822136361152\n",
      "            Epoch 70 loss = 1.1333172344602644\n",
      "            Epoch 80 loss = 0.6427888437174261\n",
      "            Epoch 90 loss = 0.4261807586881332\n",
      "            Epoch 100 loss = 0.3116036104038358\n",
      "            Epoch 110 loss = 0.24239587536430918\n",
      "            Epoch 120 loss = 0.19676272990182042\n",
      "            Epoch 130 loss = 0.16471893312700558\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.99359983205795\n",
      "            Epoch 10 loss = 107.7861071228981\n",
      "            Epoch 20 loss = 104.65491300821304\n",
      "            Epoch 30 loss = 96.15924471616745\n",
      "            Epoch 40 loss = 81.40198436379433\n",
      "            Epoch 50 loss = 68.05208578705788\n",
      "            Epoch 60 loss = 49.8225220143795\n",
      "            Epoch 70 loss = 32.44438426569104\n",
      "            Epoch 80 loss = 19.371986677870154\n",
      "            Epoch 90 loss = 10.22395038139075\n",
      "            Epoch 100 loss = 5.262157975696027\n",
      "            Epoch 110 loss = 2.9042248574551195\n",
      "            Epoch 120 loss = 1.76291148934979\n",
      "            Epoch 130 loss = 1.192128456896171\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.5404104590416\n",
      "            Epoch 10 loss = 108.82304710149765\n",
      "            Epoch 20 loss = 108.22047001123428\n",
      "            Epoch 30 loss = 107.34112256765366\n",
      "            Epoch 40 loss = 105.98184084892273\n",
      "            Epoch 50 loss = 103.97549426555634\n",
      "            Epoch 60 loss = 100.95294708013535\n",
      "            Epoch 70 loss = 95.72450625896454\n",
      "            Epoch 80 loss = 88.79349592328072\n",
      "            Epoch 90 loss = 83.01047411561012\n",
      "            Epoch 100 loss = 77.9922603070736\n",
      "            Epoch 110 loss = 72.6971181333065\n",
      "            Epoch 120 loss = 66.64023360610008\n",
      "            Epoch 130 loss = 59.675648018717766\n",
      "      For 140 epochs : [75.8, 72.1, 69.9, 69.6, 73.3]\n",
      "     150 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.33948373794556\n",
      "            Epoch 10 loss = 89.88141241669655\n",
      "            Epoch 20 loss = 34.782953694462776\n",
      "            Epoch 30 loss = 4.03553219139576\n",
      "            Epoch 40 loss = 0.817672505392693\n",
      "            Epoch 50 loss = 0.36063702509272844\n",
      "            Epoch 60 loss = 0.21731765918957535\n",
      "            Epoch 70 loss = 0.15219226067711134\n",
      "            Epoch 80 loss = 0.11574236159503926\n",
      "            Epoch 90 loss = 0.0927008978324011\n",
      "            Epoch 100 loss = 0.076922493371967\n",
      "            Epoch 110 loss = 0.065478203650855\n",
      "            Epoch 120 loss = 0.0568279410181276\n",
      "            Epoch 130 loss = 0.05008160170473275\n",
      "            Epoch 140 loss = 0.0446804152270488\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.3920766711235\n",
      "            Epoch 10 loss = 97.17600718140602\n",
      "            Epoch 20 loss = 53.97906935214996\n",
      "            Epoch 30 loss = 14.408036476001143\n",
      "            Epoch 40 loss = 2.8318090490065515\n",
      "            Epoch 50 loss = 0.9132919303374365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 60 loss = 0.45351790863787755\n",
      "            Epoch 70 loss = 0.28474878732231446\n",
      "            Epoch 80 loss = 0.2026986279815901\n",
      "            Epoch 90 loss = 0.15548856518580578\n",
      "            Epoch 100 loss = 0.12516894747386687\n",
      "            Epoch 110 loss = 0.10419405820721295\n",
      "            Epoch 120 loss = 0.0889097872786806\n",
      "            Epoch 130 loss = 0.07731400805641897\n",
      "            Epoch 140 loss = 0.06822999032010557\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.05028355121613\n",
      "            Epoch 10 loss = 99.27646064758301\n",
      "            Epoch 20 loss = 74.3832175731659\n",
      "            Epoch 30 loss = 39.40646857023239\n",
      "            Epoch 40 loss = 13.405586401000619\n",
      "            Epoch 50 loss = 3.786835875827819\n",
      "            Epoch 60 loss = 1.5202864434104413\n",
      "            Epoch 70 loss = 0.7924504596740007\n",
      "            Epoch 80 loss = 0.4942314198706299\n",
      "            Epoch 90 loss = 0.34665462566772476\n",
      "            Epoch 100 loss = 0.2624839688069187\n",
      "            Epoch 110 loss = 0.20923993701580912\n",
      "            Epoch 120 loss = 0.17287390466663055\n",
      "            Epoch 130 loss = 0.1466309676907258\n",
      "            Epoch 140 loss = 0.12686985211621504\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.870445728302\n",
      "            Epoch 10 loss = 108.68421161174774\n",
      "            Epoch 20 loss = 107.00756520032883\n",
      "            Epoch 30 loss = 103.70374804735184\n",
      "            Epoch 40 loss = 94.73455563187599\n",
      "            Epoch 50 loss = 81.4477334022522\n",
      "            Epoch 60 loss = 68.14416135847569\n",
      "            Epoch 70 loss = 49.71288953721523\n",
      "            Epoch 80 loss = 31.734343506395817\n",
      "            Epoch 90 loss = 17.634276516735554\n",
      "            Epoch 100 loss = 8.784079026430845\n",
      "            Epoch 110 loss = 4.596821471583098\n",
      "            Epoch 120 loss = 2.6425928063690662\n",
      "            Epoch 130 loss = 1.6660601207986474\n",
      "            Epoch 140 loss = 1.136125419754535\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.6305639743805\n",
      "            Epoch 10 loss = 108.78804099559784\n",
      "            Epoch 20 loss = 107.94434261322021\n",
      "            Epoch 30 loss = 106.82901656627655\n",
      "            Epoch 40 loss = 105.29316902160645\n",
      "            Epoch 50 loss = 103.0724425315857\n",
      "            Epoch 60 loss = 99.35205614566803\n",
      "            Epoch 70 loss = 93.01483535766602\n",
      "            Epoch 80 loss = 85.4832651913166\n",
      "            Epoch 90 loss = 79.40976476669312\n",
      "            Epoch 100 loss = 73.96738460659981\n",
      "            Epoch 110 loss = 68.5034609735012\n",
      "            Epoch 120 loss = 62.68701893091202\n",
      "            Epoch 130 loss = 56.291101060807705\n",
      "            Epoch 140 loss = 49.23604739457369\n",
      "      For 150 epochs : [75.8, 71.89999999999999, 72.1, 73.6, 69.0]\n",
      "  With minibatches of size 20 : [[76.8, 73.8, 74.7, 73.4, 69.8], [73.8, 72.2, 71.6, 71.7, 71.50000000000001], [72.7, 73.8, 72.1, 76.3, 70.7], [73.6, 73.5, 76.7, 74.5, 70.30000000000001], [75.8, 72.1, 69.9, 69.6, 73.3], [75.8, 71.89999999999999, 72.1, 73.6, 69.0]]\n",
      "Batch size 40\n",
      "==============\n",
      "     100 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.66754561662674\n",
      "            Epoch 10 loss = 52.17578774690628\n",
      "            Epoch 20 loss = 42.83812612295151\n",
      "            Epoch 30 loss = 27.779195055365562\n",
      "            Epoch 40 loss = 14.3041006103158\n",
      "            Epoch 50 loss = 4.951415354385972\n",
      "            Epoch 60 loss = 1.4111246773973107\n",
      "            Epoch 70 loss = 0.5795675660483539\n",
      "            Epoch 80 loss = 0.3254280890105292\n",
      "            Epoch 90 loss = 0.21800972754135728\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.735715329647064\n",
      "            Epoch 10 loss = 53.20639383792877\n",
      "            Epoch 20 loss = 48.08678317070007\n",
      "            Epoch 30 loss = 39.754870891571045\n",
      "            Epoch 40 loss = 27.90760374069214\n",
      "            Epoch 50 loss = 15.525358900427818\n",
      "            Epoch 60 loss = 6.288658898323774\n",
      "            Epoch 70 loss = 2.2170936223119497\n",
      "            Epoch 80 loss = 0.9962198711000383\n",
      "            Epoch 90 loss = 0.5573346591554582\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.82963848114014\n",
      "            Epoch 10 loss = 54.0178416967392\n",
      "            Epoch 20 loss = 52.66964244842529\n",
      "            Epoch 30 loss = 49.33854812383652\n",
      "            Epoch 40 loss = 42.12191525101662\n",
      "            Epoch 50 loss = 36.25872269272804\n",
      "            Epoch 60 loss = 27.597199216485023\n",
      "            Epoch 70 loss = 19.213457487523556\n",
      "            Epoch 80 loss = 12.956521198153496\n",
      "            Epoch 90 loss = 7.604127366095781\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 54.64776599407196\n",
      "            Epoch 10 loss = 54.06315577030182\n",
      "            Epoch 20 loss = 53.37432771921158\n",
      "            Epoch 30 loss = 52.284022092819214\n",
      "            Epoch 40 loss = 50.46629011631012\n",
      "            Epoch 50 loss = 46.948627173900604\n",
      "            Epoch 60 loss = 42.345121175050735\n",
      "            Epoch 70 loss = 38.79358607530594\n",
      "            Epoch 80 loss = 35.3970368206501\n",
      "            Epoch 90 loss = 31.46934276819229\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 54.789090394973755\n",
      "            Epoch 10 loss = 54.562498331069946\n",
      "            Epoch 20 loss = 54.38715171813965\n",
      "            Epoch 30 loss = 54.19892704486847\n",
      "            Epoch 40 loss = 53.980597257614136\n",
      "            Epoch 50 loss = 53.718206107616425\n",
      "            Epoch 60 loss = 53.39903849363327\n",
      "            Epoch 70 loss = 53.013598918914795\n",
      "            Epoch 80 loss = 52.554875791072845\n",
      "            Epoch 90 loss = 52.00979828834534\n",
      "      For 100 epochs : [73.8, 74.8, 73.5, 72.1, 48.8]\n",
      "     110 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.99114793539047\n",
      "            Epoch 10 loss = 52.624113619327545\n",
      "            Epoch 20 loss = 44.06384891271591\n",
      "            Epoch 30 loss = 30.340543657541275\n",
      "            Epoch 40 loss = 15.756405740976334\n",
      "            Epoch 50 loss = 5.452044663950801\n",
      "            Epoch 60 loss = 1.5673028687015176\n",
      "            Epoch 70 loss = 0.6613102934788913\n",
      "            Epoch 80 loss = 0.36432035616599023\n",
      "            Epoch 90 loss = 0.23890407790895551\n",
      "            Epoch 100 loss = 0.17374686046969146\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.81189823150635\n",
      "            Epoch 10 loss = 53.188522934913635\n",
      "            Epoch 20 loss = 47.57511180639267\n",
      "            Epoch 30 loss = 38.92222410440445\n",
      "            Epoch 40 loss = 26.223040282726288\n",
      "            Epoch 50 loss = 14.89664851129055\n",
      "            Epoch 60 loss = 6.454238381236792\n",
      "            Epoch 70 loss = 2.413956381380558\n",
      "            Epoch 80 loss = 1.056009391322732\n",
      "            Epoch 90 loss = 0.5833419489208609\n",
      "            Epoch 100 loss = 0.3686164702521637\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.66680073738098\n",
      "            Epoch 10 loss = 53.57331305742264\n",
      "            Epoch 20 loss = 51.44367969036102\n",
      "            Epoch 30 loss = 45.11773216724396\n",
      "            Epoch 40 loss = 38.76580247282982\n",
      "            Epoch 50 loss = 31.162300288677216\n",
      "            Epoch 60 loss = 22.00041927397251\n",
      "            Epoch 70 loss = 15.184137307107449\n",
      "            Epoch 80 loss = 9.153197392821312\n",
      "            Epoch 90 loss = 4.561474008485675\n",
      "            Epoch 100 loss = 2.213690150529146\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 54.82099133729935\n",
      "            Epoch 10 loss = 54.3742059469223\n",
      "            Epoch 20 loss = 53.883790016174316\n",
      "            Epoch 30 loss = 53.16403645277023\n",
      "            Epoch 40 loss = 52.08939790725708\n",
      "            Epoch 50 loss = 50.24830347299576\n",
      "            Epoch 60 loss = 46.603442311286926\n",
      "            Epoch 70 loss = 42.20140612125397\n",
      "            Epoch 80 loss = 38.785076320171356\n",
      "            Epoch 90 loss = 35.40237218141556\n",
      "            Epoch 100 loss = 31.372729003429413\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 55.182909309864044\n",
      "            Epoch 10 loss = 54.52859181165695\n",
      "            Epoch 20 loss = 54.34437143802643\n",
      "            Epoch 30 loss = 54.146613121032715\n",
      "            Epoch 40 loss = 53.920466244220734\n",
      "            Epoch 50 loss = 53.65575164556503\n",
      "            Epoch 60 loss = 53.34422588348389\n",
      "            Epoch 70 loss = 52.979461669921875\n",
      "            Epoch 80 loss = 52.55214697122574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 90 loss = 52.04386931657791\n",
      "            Epoch 100 loss = 51.41359078884125\n",
      "      For 110 epochs : [76.0, 73.3, 76.0, 71.30000000000001, 50.4]\n",
      "     120 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.97470462322235\n",
      "            Epoch 10 loss = 52.39678418636322\n",
      "            Epoch 20 loss = 42.784855753183365\n",
      "            Epoch 30 loss = 28.444043949246407\n",
      "            Epoch 40 loss = 13.125697661191225\n",
      "            Epoch 50 loss = 3.6763510555028915\n",
      "            Epoch 60 loss = 1.1308350977487862\n",
      "            Epoch 70 loss = 0.51169493352063\n",
      "            Epoch 80 loss = 0.30108465324155986\n",
      "            Epoch 90 loss = 0.2061525417957455\n",
      "            Epoch 100 loss = 0.15410713135497645\n",
      "            Epoch 110 loss = 0.121761717076879\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.79721987247467\n",
      "            Epoch 10 loss = 52.45394325256348\n",
      "            Epoch 20 loss = 43.68911558389664\n",
      "            Epoch 30 loss = 33.635463923215866\n",
      "            Epoch 40 loss = 20.267937764525414\n",
      "            Epoch 50 loss = 10.81536839529872\n",
      "            Epoch 60 loss = 4.317954961210489\n",
      "            Epoch 70 loss = 1.729014277458191\n",
      "            Epoch 80 loss = 0.8298705508932471\n",
      "            Epoch 90 loss = 0.4743201616220176\n",
      "            Epoch 100 loss = 0.31465316528920084\n",
      "            Epoch 110 loss = 0.22976338805165142\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.84059250354767\n",
      "            Epoch 10 loss = 53.83105719089508\n",
      "            Epoch 20 loss = 51.76884424686432\n",
      "            Epoch 30 loss = 45.25156119465828\n",
      "            Epoch 40 loss = 39.476033091545105\n",
      "            Epoch 50 loss = 33.045206010341644\n",
      "            Epoch 60 loss = 23.872888788580894\n",
      "            Epoch 70 loss = 15.887348338961601\n",
      "            Epoch 80 loss = 9.421953741461039\n",
      "            Epoch 90 loss = 4.765824591740966\n",
      "            Epoch 100 loss = 2.353211272507906\n",
      "            Epoch 110 loss = 1.2904391041956842\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 54.77375012636185\n",
      "            Epoch 10 loss = 54.39390289783478\n",
      "            Epoch 20 loss = 53.938524305820465\n",
      "            Epoch 30 loss = 53.26983588933945\n",
      "            Epoch 40 loss = 52.26481330394745\n",
      "            Epoch 50 loss = 50.6056302189827\n",
      "            Epoch 60 loss = 47.36139702796936\n",
      "            Epoch 70 loss = 42.87279549241066\n",
      "            Epoch 80 loss = 39.3280993103981\n",
      "            Epoch 90 loss = 36.12770280241966\n",
      "            Epoch 100 loss = 32.64522647857666\n",
      "            Epoch 110 loss = 28.536820888519287\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 55.004267036914825\n",
      "            Epoch 10 loss = 54.69224292039871\n",
      "            Epoch 20 loss = 54.52435439825058\n",
      "            Epoch 30 loss = 54.36496818065643\n",
      "            Epoch 40 loss = 54.194056272506714\n",
      "            Epoch 50 loss = 54.000950396060944\n",
      "            Epoch 60 loss = 53.77789753675461\n",
      "            Epoch 70 loss = 53.51873731613159\n",
      "            Epoch 80 loss = 53.220074355602264\n",
      "            Epoch 90 loss = 52.88147991895676\n",
      "            Epoch 100 loss = 52.500760316848755\n",
      "            Epoch 110 loss = 52.06730276346207\n",
      "      For 120 epochs : [72.6, 74.0, 76.9, 68.6, 47.699999999999996]\n",
      "     130 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 55.006598234176636\n",
      "            Epoch 10 loss = 51.78282827138901\n",
      "            Epoch 20 loss = 42.536252617836\n",
      "            Epoch 30 loss = 27.037459209561348\n",
      "            Epoch 40 loss = 12.979643918573856\n",
      "            Epoch 50 loss = 3.9964003786444664\n",
      "            Epoch 60 loss = 1.0807534735649824\n",
      "            Epoch 70 loss = 0.47833989653736353\n",
      "            Epoch 80 loss = 0.2804392884718254\n",
      "            Epoch 90 loss = 0.19204920146148652\n",
      "            Epoch 100 loss = 0.1436459799297154\n",
      "            Epoch 110 loss = 0.1135464133694768\n",
      "            Epoch 120 loss = 0.09319474038784392\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.777180671691895\n",
      "            Epoch 10 loss = 53.15110844373703\n",
      "            Epoch 20 loss = 47.75750434398651\n",
      "            Epoch 30 loss = 38.94899433851242\n",
      "            Epoch 40 loss = 26.6340029835701\n",
      "            Epoch 50 loss = 15.662989042699337\n",
      "            Epoch 60 loss = 7.397490777075291\n",
      "            Epoch 70 loss = 2.6175115620717406\n",
      "            Epoch 80 loss = 1.1056039473041892\n",
      "            Epoch 90 loss = 0.5979774950537831\n",
      "            Epoch 100 loss = 0.3829023251309991\n",
      "            Epoch 110 loss = 0.27228992455638945\n",
      "            Epoch 120 loss = 0.2073502178536728\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.89448547363281\n",
      "            Epoch 10 loss = 53.80050837993622\n",
      "            Epoch 20 loss = 51.794042229652405\n",
      "            Epoch 30 loss = 46.52611416578293\n",
      "            Epoch 40 loss = 40.35341250896454\n",
      "            Epoch 50 loss = 34.29915902018547\n",
      "            Epoch 60 loss = 25.138756811618805\n",
      "            Epoch 70 loss = 17.002617590129375\n",
      "            Epoch 80 loss = 10.612093292176723\n",
      "            Epoch 90 loss = 5.517472978681326\n",
      "            Epoch 100 loss = 2.6348052201792598\n",
      "            Epoch 110 loss = 1.4383708937093616\n",
      "            Epoch 120 loss = 0.8774522850289941\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 55.00401175022125\n",
      "            Epoch 10 loss = 54.378345370292664\n",
      "            Epoch 20 loss = 54.01057493686676\n",
      "            Epoch 30 loss = 53.45912957191467\n",
      "            Epoch 40 loss = 52.608040392398834\n",
      "            Epoch 50 loss = 51.29152715206146\n",
      "            Epoch 60 loss = 48.813499093055725\n",
      "            Epoch 70 loss = 44.611000418663025\n",
      "            Epoch 80 loss = 40.652277529239655\n",
      "            Epoch 90 loss = 37.49709552526474\n",
      "            Epoch 100 loss = 34.20795840024948\n",
      "            Epoch 110 loss = 30.38519185781479\n",
      "            Epoch 120 loss = 25.826048478484154\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 55.05037760734558\n",
      "            Epoch 10 loss = 54.51166749000549\n",
      "            Epoch 20 loss = 54.32361578941345\n",
      "            Epoch 30 loss = 54.12968283891678\n",
      "            Epoch 40 loss = 53.905211329460144\n",
      "            Epoch 50 loss = 53.63668870925903\n",
      "            Epoch 60 loss = 53.31091898679733\n",
      "            Epoch 70 loss = 52.915276288986206\n",
      "            Epoch 80 loss = 52.437257409095764\n",
      "            Epoch 90 loss = 51.85525196790695\n",
      "            Epoch 100 loss = 51.126911640167236\n",
      "            Epoch 110 loss = 50.17719501256943\n",
      "            Epoch 120 loss = 48.91055208444595\n",
      "      For 130 epochs : [72.7, 74.4, 73.9, 73.2, 57.3]\n",
      "     140 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.769703567028046\n",
      "            Epoch 10 loss = 51.12030255794525\n",
      "            Epoch 20 loss = 39.559786438941956\n",
      "            Epoch 30 loss = 22.70552448928356\n",
      "            Epoch 40 loss = 10.236005309969187\n",
      "            Epoch 50 loss = 2.724784357473254\n",
      "            Epoch 60 loss = 0.8804652122780681\n",
      "            Epoch 70 loss = 0.4263364910148084\n",
      "            Epoch 80 loss = 0.26129920152015984\n",
      "            Epoch 90 loss = 0.18340027355588973\n",
      "            Epoch 100 loss = 0.13913226983277127\n",
      "            Epoch 110 loss = 0.11095665825996548\n",
      "            Epoch 120 loss = 0.09162205207394436\n",
      "            Epoch 130 loss = 0.07760268772835843\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.97760850191116\n",
      "            Epoch 10 loss = 53.44898933172226\n",
      "            Epoch 20 loss = 50.54381674528122\n",
      "            Epoch 30 loss = 42.18211683630943\n",
      "            Epoch 40 loss = 31.539533883333206\n",
      "            Epoch 50 loss = 19.298571303486824\n",
      "            Epoch 60 loss = 10.410389345139265\n",
      "            Epoch 70 loss = 4.081314915791154\n",
      "            Epoch 80 loss = 1.5268641719594598\n",
      "            Epoch 90 loss = 0.7569278310984373\n",
      "            Epoch 100 loss = 0.4557512227911502\n",
      "            Epoch 110 loss = 0.3115565123734996\n",
      "            Epoch 120 loss = 0.2313867323100567\n",
      "            Epoch 130 loss = 0.18166035693138838\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.76911520957947\n",
      "            Epoch 10 loss = 53.82147318124771\n",
      "            Epoch 20 loss = 51.776063203811646\n",
      "            Epoch 30 loss = 45.65087455511093\n",
      "            Epoch 40 loss = 38.72139695286751\n",
      "            Epoch 50 loss = 30.918205499649048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 60 loss = 21.77935630083084\n",
      "            Epoch 70 loss = 15.155114434659481\n",
      "            Epoch 80 loss = 9.387870378792286\n",
      "            Epoch 90 loss = 4.896859247237444\n",
      "            Epoch 100 loss = 2.426760943606496\n",
      "            Epoch 110 loss = 1.3154400973580778\n",
      "            Epoch 120 loss = 0.7958167036995292\n",
      "            Epoch 130 loss = 0.5358434673398733\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 54.71114772558212\n",
      "            Epoch 10 loss = 54.425776302814484\n",
      "            Epoch 20 loss = 54.0970014333725\n",
      "            Epoch 30 loss = 53.601517260074615\n",
      "            Epoch 40 loss = 52.83477580547333\n",
      "            Epoch 50 loss = 51.60841566324234\n",
      "            Epoch 60 loss = 49.16741096973419\n",
      "            Epoch 70 loss = 44.63211044669151\n",
      "            Epoch 80 loss = 40.15624663233757\n",
      "            Epoch 90 loss = 36.55785861611366\n",
      "            Epoch 100 loss = 32.687134593725204\n",
      "            Epoch 110 loss = 28.083193972706795\n",
      "            Epoch 120 loss = 23.119685024023056\n",
      "            Epoch 130 loss = 18.650906533002853\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 54.89372169971466\n",
      "            Epoch 10 loss = 54.58433675765991\n",
      "            Epoch 20 loss = 54.36311739683151\n",
      "            Epoch 30 loss = 54.13249331712723\n",
      "            Epoch 40 loss = 53.87733495235443\n",
      "            Epoch 50 loss = 53.58671796321869\n",
      "            Epoch 60 loss = 53.25131368637085\n",
      "            Epoch 70 loss = 52.865584552288055\n",
      "            Epoch 80 loss = 52.42429792881012\n",
      "            Epoch 90 loss = 51.91383081674576\n",
      "            Epoch 100 loss = 51.30327498912811\n",
      "            Epoch 110 loss = 50.54018872976303\n",
      "            Epoch 120 loss = 49.55833750963211\n",
      "            Epoch 130 loss = 48.295898377895355\n",
      "      For 140 epochs : [73.8, 71.6, 73.9, 73.4, 60.9]\n",
      "     150 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.68795830011368\n",
      "            Epoch 10 loss = 52.194209814071655\n",
      "            Epoch 20 loss = 43.08055913448334\n",
      "            Epoch 30 loss = 27.847311109304428\n",
      "            Epoch 40 loss = 14.302263721823692\n",
      "            Epoch 50 loss = 4.390429727733135\n",
      "            Epoch 60 loss = 1.21917623328045\n",
      "            Epoch 70 loss = 0.5235659331083298\n",
      "            Epoch 80 loss = 0.3034535350743681\n",
      "            Epoch 90 loss = 0.20626391132827848\n",
      "            Epoch 100 loss = 0.15348196076229215\n",
      "            Epoch 110 loss = 0.12088342645438388\n",
      "            Epoch 120 loss = 0.09897654806263745\n",
      "            Epoch 130 loss = 0.0833329466113355\n",
      "            Epoch 140 loss = 0.07168021364486776\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.77105134725571\n",
      "            Epoch 10 loss = 53.53048688173294\n",
      "            Epoch 20 loss = 49.50888645648956\n",
      "            Epoch 30 loss = 40.25131168961525\n",
      "            Epoch 40 loss = 27.598665669560432\n",
      "            Epoch 50 loss = 16.6984051913023\n",
      "            Epoch 60 loss = 8.022907147184014\n",
      "            Epoch 70 loss = 2.990978012792766\n",
      "            Epoch 80 loss = 1.2008131272159517\n",
      "            Epoch 90 loss = 0.6299593772273511\n",
      "            Epoch 100 loss = 0.39841008570510894\n",
      "            Epoch 110 loss = 0.2812212856952101\n",
      "            Epoch 120 loss = 0.21312323363963515\n",
      "            Epoch 130 loss = 0.16950440441723913\n",
      "            Epoch 140 loss = 0.13954783766530454\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.72742861509323\n",
      "            Epoch 10 loss = 53.38663446903229\n",
      "            Epoch 20 loss = 50.53505688905716\n",
      "            Epoch 30 loss = 43.42550548911095\n",
      "            Epoch 40 loss = 37.88986724615097\n",
      "            Epoch 50 loss = 30.195675641298294\n",
      "            Epoch 60 loss = 21.697554230690002\n",
      "            Epoch 70 loss = 14.874081142246723\n",
      "            Epoch 80 loss = 8.842218365520239\n",
      "            Epoch 90 loss = 4.5216663889586926\n",
      "            Epoch 100 loss = 2.323651684448123\n",
      "            Epoch 110 loss = 1.323018483351916\n",
      "            Epoch 120 loss = 0.8325624358840287\n",
      "            Epoch 130 loss = 0.571109163807705\n",
      "            Epoch 140 loss = 0.4201448066160083\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 55.17585498094559\n",
      "            Epoch 10 loss = 54.36979407072067\n",
      "            Epoch 20 loss = 53.9248086810112\n",
      "            Epoch 30 loss = 53.23407542705536\n",
      "            Epoch 40 loss = 52.11975234746933\n",
      "            Epoch 50 loss = 50.2425742149353\n",
      "            Epoch 60 loss = 46.65105164051056\n",
      "            Epoch 70 loss = 42.047994405031204\n",
      "            Epoch 80 loss = 38.28564888238907\n",
      "            Epoch 90 loss = 34.49957099556923\n",
      "            Epoch 100 loss = 30.120719075202942\n",
      "            Epoch 110 loss = 25.09438280761242\n",
      "            Epoch 120 loss = 20.165710777044296\n",
      "            Epoch 130 loss = 15.974449090659618\n",
      "            Epoch 140 loss = 12.46282709017396\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 54.926306784152985\n",
      "            Epoch 10 loss = 54.59055608510971\n",
      "            Epoch 20 loss = 54.41056847572327\n",
      "            Epoch 30 loss = 54.22239601612091\n",
      "            Epoch 40 loss = 54.009112894535065\n",
      "            Epoch 50 loss = 53.759326100349426\n",
      "            Epoch 60 loss = 53.46080952882767\n",
      "            Epoch 70 loss = 53.101615607738495\n",
      "            Epoch 80 loss = 52.66638767719269\n",
      "            Epoch 90 loss = 52.13139981031418\n",
      "            Epoch 100 loss = 51.449551701545715\n",
      "            Epoch 110 loss = 50.54446029663086\n",
      "            Epoch 120 loss = 49.31231904029846\n",
      "            Epoch 130 loss = 47.666195333004\n",
      "            Epoch 140 loss = 45.64242082834244\n",
      "      For 150 epochs : [74.4, 74.9, 73.9, 74.1, 62.4]\n",
      "  With minibatches of size 40 : [[73.8, 74.8, 73.5, 72.1, 48.8], [76.0, 73.3, 76.0, 71.30000000000001, 50.4], [72.6, 74.0, 76.9, 68.6, 47.699999999999996], [72.7, 74.4, 73.9, 73.2, 57.3], [73.8, 71.6, 73.9, 73.4, 60.9], [74.4, 74.9, 73.9, 74.1, 62.4]]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"WITHOUT DROPOUT\")\n",
    "print(\"\")\n",
    "no_dropout = []\n",
    "for b_size in batch_sizes:\n",
    "    print(\"Batch size\", b_size)\n",
    "    print(\"=\"*14)\n",
    "    epoch_acc = []\n",
    "    for e in nb_epochs: \n",
    "        print(\"    \", e,\"epochs\")\n",
    "        print(\"  \", \"-\"*15)\n",
    "        lr_acc =[]\n",
    "        for lr in learning_rates:\n",
    "            print(\"        Learning rate\", lr, \":\")\n",
    "            model = CNN_1D()\n",
    "            train_model(model, Variable(train_input), Variable(train_target), b_size, nb_epochs=e, learning_rate=lr)\n",
    "            test_error = compute_nb_errors(model, Variable(test_input), Variable(test_target))\n",
    "            lr_acc.append(100*(1-(test_error/test_input.size(0))))\n",
    "        epoch_acc.append(lr_acc)\n",
    "        print(\"      For\", e, \"epochs :\", lr_acc)\n",
    "    print(\"  With minibatches of size\", b_size, \":\", epoch_acc)\n",
    "    no_dropout.append(epoch_acc)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_results = {\"batch_sizes\":batch_sizes, \"learning_rates\":learning_rates, \"nb_epochs\":nb_epochs, \"results\":no_dropout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(normal_results,open('results_no_dropout.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_sizes': [20, 40],\n",
       " 'learning_rates': [0.01, 0.0075, 0.005, 0.0025, 0.001],\n",
       " 'nb_epochs': [100, 110, 120, 130, 140, 150],\n",
       " 'results': [[[76.8, 73.8, 74.7, 73.4, 69.8],\n",
       "   [73.8, 72.2, 71.6, 71.7, 71.50000000000001],\n",
       "   [72.7, 73.8, 72.1, 76.3, 70.7],\n",
       "   [73.6, 73.5, 76.7, 74.5, 70.30000000000001],\n",
       "   [75.8, 72.1, 69.9, 69.6, 73.3],\n",
       "   [75.8, 71.89999999999999, 72.1, 73.6, 69.0]],\n",
       "  [[73.8, 74.8, 73.5, 72.1, 48.8],\n",
       "   [76.0, 73.3, 76.0, 71.30000000000001, 50.4],\n",
       "   [72.6, 74.0, 76.9, 68.6, 47.699999999999996],\n",
       "   [72.7, 74.4, 73.9, 73.2, 57.3],\n",
       "   [73.8, 71.6, 73.9, 73.4, 60.9],\n",
       "   [74.4, 74.9, 73.9, 74.1, 62.4]]]}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(open('results_no_dropout.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH DROPOUT\n",
      "\n",
      "Batch size 20\n",
      "==============\n",
      "    Learning rate 0.01 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.56861990690231\n",
      "            Epoch 10 loss = 97.68512445688248\n",
      "            Epoch 20 loss = 44.219020545482635\n",
      "            Epoch 30 loss = 5.120638834312558\n",
      "            Epoch 40 loss = 1.2338459701277316\n",
      "            Epoch 50 loss = 0.5521879402222112\n",
      "            Epoch 60 loss = 0.34920062322635204\n",
      "            Epoch 70 loss = 0.2755592145549599\n",
      "            Epoch 80 loss = 0.19016350763558876\n",
      "            Epoch 90 loss = 0.15994437631161418\n",
      "            Epoch 100 loss = 0.12974347618728643\n",
      "            Epoch 1 loss = 0.13672240971209249\n",
      "            Epoch 10 loss = 0.12727110175183043\n",
      "            Epoch 1 loss = 0.10125427930688602\n",
      "            Epoch 10 loss = 0.0968204613745911\n",
      "            Epoch 1 loss = 0.10534073887538398\n",
      "            Epoch 10 loss = 0.08921499579082592\n",
      "            Epoch 1 loss = 0.0914028889455949\n",
      "            Epoch 10 loss = 0.07541410003614146\n",
      "            Epoch 1 loss = 0.07375158538343385\n",
      "            Epoch 10 loss = 0.08126419101972715\n",
      "      For a learning rate of 0.01 : [73.0, 73.1, 73.5, 72.3, 72.39999999999999, 73.0]\n",
      "    Learning rate 0.0075 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.62843090295792\n",
      "            Epoch 10 loss = 98.21608650684357\n",
      "            Epoch 20 loss = 63.43432921171188\n",
      "            Epoch 30 loss = 22.24169982969761\n",
      "            Epoch 40 loss = 4.341281499247998\n",
      "            Epoch 50 loss = 1.3381346956593916\n",
      "            Epoch 60 loss = 0.6578830982325599\n",
      "            Epoch 70 loss = 0.4369636698102113\n",
      "            Epoch 80 loss = 0.3122253053879831\n",
      "            Epoch 90 loss = 0.23963762546190992\n",
      "            Epoch 100 loss = 0.20411846942442935\n",
      "            Epoch 1 loss = 0.19837641302729025\n",
      "            Epoch 10 loss = 0.1728045970085077\n",
      "            Epoch 1 loss = 0.15700763886707136\n",
      "            Epoch 10 loss = 0.1435071559244534\n",
      "            Epoch 1 loss = 0.1388901480968343\n",
      "            Epoch 10 loss = 0.12629083030697075\n",
      "            Epoch 1 loss = 0.11654621600609971\n",
      "            Epoch 10 loss = 0.09773171833512606\n",
      "            Epoch 1 loss = 0.11018532399612013\n",
      "            Epoch 10 loss = 0.1102513519799686\n",
      "      For a learning rate of 0.0075 : [75.2, 75.1, 75.4, 75.4, 75.6, 75.3]\n",
      "    Learning rate 0.005 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.57152128219604\n",
      "            Epoch 10 loss = 104.8253173828125\n",
      "            Epoch 20 loss = 86.22930234670639\n",
      "            Epoch 30 loss = 56.883259519934654\n",
      "            Epoch 40 loss = 26.098600275814533\n",
      "            Epoch 50 loss = 8.068019286729395\n",
      "            Epoch 60 loss = 2.9940146636217833\n",
      "            Epoch 70 loss = 1.4079134740168229\n",
      "            Epoch 80 loss = 0.8640981721691787\n",
      "            Epoch 90 loss = 0.6214228210155852\n",
      "            Epoch 100 loss = 0.47480609541526064\n",
      "            Epoch 1 loss = 0.42907167854718864\n",
      "            Epoch 10 loss = 0.3684646670008078\n",
      "            Epoch 1 loss = 0.35554498550482094\n",
      "            Epoch 10 loss = 0.2901668788981624\n",
      "            Epoch 1 loss = 0.2853577405330725\n",
      "            Epoch 10 loss = 0.2576447843457572\n",
      "            Epoch 1 loss = 0.24716310511576012\n",
      "            Epoch 10 loss = 0.22386534267570823\n",
      "            Epoch 1 loss = 0.22069361874309834\n",
      "            Epoch 10 loss = 0.2124225073494017\n",
      "      For a learning rate of 0.005 : [73.8, 73.8, 73.5, 73.6, 73.4, 73.7]\n",
      "    Learning rate 0.0025 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.3594121336937\n",
      "            Epoch 10 loss = 108.4105435013771\n",
      "            Epoch 20 loss = 105.75545698404312\n",
      "            Epoch 30 loss = 100.24230909347534\n",
      "            Epoch 40 loss = 88.01185545325279\n",
      "            Epoch 50 loss = 75.54851604998112\n",
      "            Epoch 60 loss = 59.67797887325287\n",
      "            Epoch 70 loss = 42.26284524798393\n",
      "            Epoch 80 loss = 26.826030373573303\n",
      "            Epoch 90 loss = 14.796065475791693\n",
      "            Epoch 100 loss = 7.5477120243012905\n",
      "            Epoch 1 loss = 7.137507160194218\n",
      "            Epoch 10 loss = 4.207017409149557\n",
      "            Epoch 1 loss = 4.0747702326625586\n",
      "            Epoch 10 loss = 2.5371380450669676\n",
      "            Epoch 1 loss = 2.446350328391418\n",
      "            Epoch 10 loss = 1.70310002239421\n",
      "            Epoch 1 loss = 1.677784666302614\n",
      "            Epoch 10 loss = 1.3118308016564697\n",
      "            Epoch 1 loss = 1.2329265616135672\n",
      "            Epoch 10 loss = 0.9859189314302057\n",
      "      For a learning rate of 0.0025 : [75.1, 75.5, 75.4, 74.8, 74.1, 73.1]\n",
      "    Learning rate 0.001 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.69096857309341\n",
      "            Epoch 10 loss = 108.99123287200928\n",
      "            Epoch 20 loss = 108.61261177062988\n",
      "            Epoch 30 loss = 108.04281640052795\n",
      "            Epoch 40 loss = 107.16600161790848\n",
      "            Epoch 50 loss = 105.9531564116478\n",
      "            Epoch 60 loss = 104.44089043140411\n",
      "            Epoch 70 loss = 101.91724252700806\n",
      "            Epoch 80 loss = 97.78526335954666\n",
      "            Epoch 90 loss = 92.06609332561493\n",
      "            Epoch 100 loss = 85.72497540712357\n",
      "            Epoch 1 loss = 85.1369411945343\n",
      "            Epoch 10 loss = 80.47107920050621\n",
      "            Epoch 1 loss = 79.68020674586296\n",
      "            Epoch 10 loss = 75.22263269126415\n",
      "            Epoch 1 loss = 74.47514486312866\n",
      "            Epoch 10 loss = 70.10273890197277\n",
      "            Epoch 1 loss = 69.723218947649\n",
      "            Epoch 10 loss = 63.98162969946861\n",
      "            Epoch 1 loss = 63.00163885951042\n",
      "            Epoch 10 loss = 55.89687018096447\n",
      "      For a learning rate of 0.001 : [67.30000000000001, 70.8, 70.6, 72.8, 70.30000000000001, 69.39999999999999]\n",
      "  With minibatches of size 20 : [[73.0, 73.1, 73.5, 72.3, 72.39999999999999, 73.0], [75.2, 75.1, 75.4, 75.4, 75.6, 75.3], [73.8, 73.8, 73.5, 73.6, 73.4, 73.7], [75.1, 75.5, 75.4, 74.8, 74.1, 73.1], [67.30000000000001, 70.8, 70.6, 72.8, 70.30000000000001, 69.39999999999999]]\n",
      "Batch size 40\n",
      "==============\n",
      "    Learning rate 0.01 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 54.66406261920929\n",
      "            Epoch 10 loss = 52.695162534713745\n",
      "            Epoch 20 loss = 44.93166735768318\n",
      "            Epoch 30 loss = 34.12722697854042\n",
      "            Epoch 40 loss = 18.3684239089489\n",
      "            Epoch 50 loss = 7.36179362423718\n",
      "            Epoch 60 loss = 2.1382580287754536\n",
      "            Epoch 70 loss = 0.897298735100776\n",
      "            Epoch 80 loss = 0.4998171986080706\n",
      "            Epoch 90 loss = 0.3381415222538635\n",
      "            Epoch 100 loss = 0.24739446869352832\n",
      "            Epoch 1 loss = 0.24304704036330804\n",
      "            Epoch 10 loss = 0.19185999414185062\n",
      "            Epoch 1 loss = 0.19869344122707844\n",
      "            Epoch 10 loss = 0.17098108417121693\n",
      "            Epoch 1 loss = 0.1536108668660745\n",
      "            Epoch 10 loss = 0.14005059521878138\n",
      "            Epoch 1 loss = 0.1329002336715348\n",
      "            Epoch 10 loss = 0.1318695903464686\n",
      "            Epoch 1 loss = 0.13340372551465407\n",
      "            Epoch 10 loss = 0.09913914924254641\n",
      "      For a learning rate of 0.01 : [73.2, 73.1, 73.1, 72.6, 72.39999999999999, 72.2]\n",
      "    Learning rate 0.0075 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 54.81049567461014\n",
      "            Epoch 10 loss = 52.81171107292175\n",
      "            Epoch 20 loss = 47.105011343955994\n",
      "            Epoch 30 loss = 39.14561042189598\n",
      "            Epoch 40 loss = 26.74927717447281\n",
      "            Epoch 50 loss = 15.877388007938862\n",
      "            Epoch 60 loss = 7.6357070207595825\n",
      "            Epoch 70 loss = 3.161790208891034\n",
      "            Epoch 80 loss = 1.3640930019319057\n",
      "            Epoch 90 loss = 0.7406394677236676\n",
      "            Epoch 100 loss = 0.5193929544184357\n",
      "            Epoch 1 loss = 0.4563080398365855\n",
      "            Epoch 10 loss = 0.35685411910526454\n",
      "            Epoch 1 loss = 0.33944230689667165\n",
      "            Epoch 10 loss = 0.29535032174317166\n",
      "            Epoch 1 loss = 0.28001729876268655\n",
      "            Epoch 10 loss = 0.2341560226632282\n",
      "            Epoch 1 loss = 0.2297601808095351\n",
      "            Epoch 10 loss = 0.18885271868202835\n",
      "            Epoch 1 loss = 0.19422286713961512\n",
      "            Epoch 10 loss = 0.16072780184913427\n",
      "      For a learning rate of 0.0075 : [76.8, 76.2, 75.9, 75.1, 74.6, 74.7]\n",
      "    Learning rate 0.005 :\n",
      "   ----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 1 loss = 54.91450089216232\n",
      "            Epoch 10 loss = 54.06467169523239\n",
      "            Epoch 20 loss = 52.71694880723953\n",
      "            Epoch 30 loss = 49.13769429922104\n",
      "            Epoch 40 loss = 41.97347775101662\n",
      "            Epoch 50 loss = 35.981355518102646\n",
      "            Epoch 60 loss = 27.21285429596901\n",
      "            Epoch 70 loss = 19.208084240555763\n",
      "            Epoch 80 loss = 12.621800161898136\n",
      "            Epoch 90 loss = 7.263875424861908\n",
      "            Epoch 100 loss = 3.752448473125696\n",
      "            Epoch 1 loss = 3.4975507725030184\n",
      "            Epoch 10 loss = 2.073324886150658\n",
      "            Epoch 1 loss = 1.9309103712439537\n",
      "            Epoch 10 loss = 1.257086329627782\n",
      "            Epoch 1 loss = 1.1806313153356314\n",
      "            Epoch 10 loss = 0.8548443848267198\n",
      "            Epoch 1 loss = 0.8137015693355352\n",
      "            Epoch 10 loss = 0.5689654373563826\n",
      "            Epoch 1 loss = 0.5970331872813404\n",
      "            Epoch 10 loss = 0.47645941376686096\n",
      "      For a learning rate of 0.005 : [76.0, 76.3, 76.2, 77.0, 77.0, 76.8]\n",
      "    Learning rate 0.0025 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 54.95566213130951\n",
      "            Epoch 10 loss = 54.31988149881363\n",
      "            Epoch 20 loss = 53.68212467432022\n",
      "            Epoch 30 loss = 52.749579071998596\n",
      "            Epoch 40 loss = 51.421384036540985\n",
      "            Epoch 50 loss = 49.19163703918457\n",
      "            Epoch 60 loss = 45.31329184770584\n",
      "            Epoch 70 loss = 41.40261700749397\n",
      "            Epoch 80 loss = 38.284474700689316\n",
      "            Epoch 90 loss = 34.926138788461685\n",
      "            Epoch 100 loss = 31.0321643948555\n",
      "            Epoch 1 loss = 30.302784517407417\n",
      "            Epoch 10 loss = 26.365234196186066\n",
      "            Epoch 1 loss = 26.033069029450417\n",
      "            Epoch 10 loss = 22.147339925169945\n",
      "            Epoch 1 loss = 21.53264446556568\n",
      "            Epoch 10 loss = 17.8727028593421\n",
      "            Epoch 1 loss = 17.737405739724636\n",
      "            Epoch 10 loss = 14.785745188593864\n",
      "            Epoch 1 loss = 14.256498351693153\n",
      "            Epoch 10 loss = 11.106331713497639\n",
      "      For a learning rate of 0.0025 : [70.7, 70.7, 69.39999999999999, 71.10000000000001, 71.50000000000001, 73.3]\n",
      "    Learning rate 0.001 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 54.83039230108261\n",
      "            Epoch 10 loss = 54.67936599254608\n",
      "            Epoch 20 loss = 54.49523842334747\n",
      "            Epoch 30 loss = 54.27069890499115\n",
      "            Epoch 40 loss = 54.07359677553177\n",
      "            Epoch 50 loss = 53.767672300338745\n",
      "            Epoch 60 loss = 53.34742248058319\n",
      "            Epoch 70 loss = 52.96017634868622\n",
      "            Epoch 80 loss = 52.46834188699722\n",
      "            Epoch 90 loss = 51.913032591342926\n",
      "            Epoch 100 loss = 51.10638552904129\n",
      "            Epoch 1 loss = 50.95891749858856\n",
      "            Epoch 10 loss = 50.22554123401642\n",
      "            Epoch 1 loss = 50.2536124587059\n",
      "            Epoch 10 loss = 49.254887998104095\n",
      "            Epoch 1 loss = 48.92675894498825\n",
      "            Epoch 10 loss = 47.60741853713989\n",
      "            Epoch 1 loss = 47.44680052995682\n",
      "            Epoch 10 loss = 45.85034269094467\n",
      "            Epoch 1 loss = 45.822577238082886\n",
      "            Epoch 10 loss = 44.126912385225296\n",
      "      For a learning rate of 0.001 : [50.8, 51.4, 53.6, 56.89999999999999, 57.00000000000001, 60.6]\n",
      "  With minibatches of size 40 : [[73.2, 73.1, 73.1, 72.6, 72.39999999999999, 72.2], [76.8, 76.2, 75.9, 75.1, 74.6, 74.7], [76.0, 76.3, 76.2, 77.0, 77.0, 76.8], [70.7, 70.7, 69.39999999999999, 71.10000000000001, 71.50000000000001, 73.3], [50.8, 51.4, 53.6, 56.89999999999999, 57.00000000000001, 60.6]]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"WITH DROPOUT\")\n",
    "print(\"\")\n",
    "with_dropout = []\n",
    "for b_size in batch_sizes:\n",
    "    print(\"Batch size\", b_size)\n",
    "    print(\"=\"*14)\n",
    "    lr_acc = []\n",
    "    for lr in learning_rates: \n",
    "        print(\"    Learning rate\", lr, \":\")\n",
    "        print(\"  \", \"-\"*22)\n",
    "        epoch_acc = []\n",
    "        model = CNN_dropout()\n",
    "        train_model(model, Variable(train_input), Variable(train_target), b_size, learning_rate=lr)\n",
    "        test_error = compute_nb_errors(model, Variable(test_input), Variable(test_target))\n",
    "        epoch_acc.append(100*(1-(test_error/test_input.size(0))))\n",
    "        for i in range(5):\n",
    "            train_model(model, Variable(train_input), Variable(train_target), b_size, nb_epochs=10, learning_rate=lr)\n",
    "            test_error = compute_nb_errors(model, Variable(test_input), Variable(test_target))\n",
    "            epoch_acc.append(100*(1-(test_error/test_input.size(0))))\n",
    "        lr_acc.append(epoch_acc)\n",
    "        print(\"      For a learning rate of\", lr, \":\", epoch_acc)\n",
    "    print(\"  With minibatches of size\", b_size, \":\", lr_acc)\n",
    "    with_dropout.append(lr_acc)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_results = {\"batch_sizes\":batch_sizes, \"learning_rates\":learning_rates, \"nb_epochs\":nb_epochs, \"results\":with_dropout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(dropout_results,open('results_with_dropout.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_sizes': [20, 40],\n",
       " 'learning_rates': [0.01, 0.0075, 0.005, 0.0025, 0.001],\n",
       " 'nb_epochs': [100, 110, 120, 130, 140, 150],\n",
       " 'results': [[[73.0, 73.1, 73.5, 72.3, 72.39999999999999, 73.0],\n",
       "   [75.2, 75.1, 75.4, 75.4, 75.6, 75.3],\n",
       "   [73.8, 73.8, 73.5, 73.6, 73.4, 73.7],\n",
       "   [75.1, 75.5, 75.4, 74.8, 74.1, 73.1],\n",
       "   [67.30000000000001,\n",
       "    70.8,\n",
       "    70.6,\n",
       "    72.8,\n",
       "    70.30000000000001,\n",
       "    69.39999999999999]],\n",
       "  [[73.2, 73.1, 73.1, 72.6, 72.39999999999999, 72.2],\n",
       "   [76.8, 76.2, 75.9, 75.1, 74.6, 74.7],\n",
       "   [76.0, 76.3, 76.2, 77.0, 77.0, 76.8],\n",
       "   [70.7, 70.7, 69.39999999999999, 71.10000000000001, 71.50000000000001, 73.3],\n",
       "   [50.8, 51.4, 53.6, 56.89999999999999, 57.00000000000001, 60.6]]]}"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(open('results_with_dropout.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "list indices must be integers or slices, not str",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-264-37cd6fb32411>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mnormal_xticks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnormal_results\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'results'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nb_epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnormal_xticks\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m221\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: list indices must be integers or slices, not str"
     ]
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "normal_xticks = normal_results['results']['nb_epochs']\n",
    "print(normal_xticks)\n",
    "plt.subplot(221)\n",
    "plt.imshow(np.array(normal_results['results'][0]).T, cmap='Reds')\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(normal_xticks)), normal_xticks)\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(np.array(normal_results['results'][1]).T, cmap='Reds')\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(dropout_results['results'][0], cmap='Reds')\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.colorbar()\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(dropout_results['results'][1], cmap='Reds')\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAWAAAADxCAYAAAD1LG0eAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10db038d0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axes = plt.subplots(nrows=2, ncols=2)\n",
    "\n",
    "im = axes[0][0].imshow(normal_results['results'][0], vmin=50, vmax=80)\n",
    "im = axes[0][1].imshow(normal_results['results'][1], vmin=50, vmax=80)\n",
    "im = axes[1][0].imshow(dropout_results['results'][0], vmin=50, vmax=80)\n",
    "im = axes[1][1].imshow(dropout_results['results'][1], vmin=50, vmax=80)\n",
    "\n",
    "fig.colorbar(im, ax=axes.ravel().tolist())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 5, 30)"
      ]
     },
     "execution_count": 249,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.asarray(dropout_results['results']).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
