{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 279,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pickle \n",
    "import json\n",
    "\n",
    "import dlc_bci as bci\n",
    "from helpers import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (10,10)\n",
    "mpl.rcParams['image.cmap'] = 'viridis'\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input : <class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "Train target : <class 'torch.LongTensor'> torch.Size([316])\n",
      "Test input : <class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "Test target : <class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Subset of data sampled at 100Hz\n",
    "train_input , train_target = bci.load(root = './data_bci')\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input : <class 'torch.FloatTensor'> torch.Size([316, 28, 500])\n",
      "Train target : <class 'torch.LongTensor'> torch.Size([316])\n",
      "Test input : <class 'torch.FloatTensor'> torch.Size([100, 28, 500])\n",
      "Test target : <class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Full data sampled at 1Khz\n",
    "train_input , train_target = bci.load(root = './data_bci', one_khz = True)\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False, one_khz = True)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  41.8000   44.8000   47.1000  ...    69.8000   72.6000   76.1000\n",
       " -10.3000   -5.9000   -3.3000  ...    12.6000   24.0000   26.5000\n",
       "  38.1000   25.2000   46.0000  ...    45.1000   74.1000   64.8000\n",
       "             ...                â‹±                ...             \n",
       "   7.9000   11.2000   14.3000  ...    32.7000   43.4000   45.5000\n",
       "  19.2000   33.6000   33.8000  ...    46.7000   53.7000   43.4000\n",
       "  -0.4000   12.7000   12.0000  ...    30.7000   40.6000   33.1000\n",
       "[torch.FloatTensor of size 28x50]"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.LongTensor of size 316]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# normalize mean to 0\n",
    "train_input.sub_(train_input.mean())\n",
    "test_input.sub_(test_input.mean())\n",
    "\n",
    "# normalize variance to 1\n",
    "train_input.div_(train_input.std())\n",
    "test_input.div_(test_input.std())\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq = []\n",
    "for i in range(10):\n",
    "    seq.append(train_input[:,:,i::10])\n",
    "train_input = torch.cat(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = torch.cat([train_target]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "seq = []\n",
    "for i in range(10):\n",
    "    seq.append(test_input[:,:,i::10])\n",
    "test_input = torch.cat(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_target = torch.cat([test_target]*10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_pred_train = discrete_predictions(torch.FloatTensor(linear_reg.predict(train_input.view(train_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % train error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(linear_pred_train) - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_pred_test = discrete_predictions(torch.FloatTensor(linear_reg.predict(test_input.view(test_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0 % test error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(linear_pred_test) - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_pred_train = logistic_reg.predict(train_input.view(train_input.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % train error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(logistic_pred_train) - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_pred_test = logistic_reg.predict(test_input.view(test_input.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.000000000000004 % test error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(logistic_pred_test) - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert targets to one hot labels\n",
    "#train_target = convert_to_one_hot_labels(train_input, train_target)\n",
    "#test_target = convert_to_one_hot_labels(test_input, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP_big = nn.Sequential(\n",
    "        nn.Linear(14000, 1400),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(1400, 280),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(280, 28),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(28, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP = nn.Sequential(\n",
    "        nn.Linear(50, 280),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(280, 140),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(140, 28),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(28, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "invalid argument 3: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1 at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/THNN/generic/SpatialClassNLLCriterion.c:60",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-967537cad33d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m28\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Cours/Deep Learning/DL_projects/miniproject_1/helpers.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, train_input, train_target, mini_batch_size, nb_epochs, learning_rate)\u001b[0m\n\u001b[1;32m     28\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mb\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m                         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m                         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnarrow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mb\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m                         \u001b[0msum_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/modules/loss.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m    677\u001b[0m         \u001b[0m_assert_no_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m         return F.cross_entropy(input, target, self.weight, self.size_average,\n\u001b[0;32m--> 679\u001b[0;31m                                self.ignore_index, self.reduce)\n\u001b[0m\u001b[1;32m    680\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    681\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0;34m>>\u001b[0m\u001b[0;34m>\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m     \"\"\"\n\u001b[0;32m-> 1161\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_softmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mnll_loss\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce)\u001b[0m\n\u001b[1;32m   1052\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1054\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnll_loss2d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize_average\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mignore_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1055\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m3\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mdim\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1056\u001b[0m         \u001b[0mn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: invalid argument 3: only batches of spatial targets supported (3D tensors) but got targets of dimension: 1 at /Users/soumith/code/builder/wheel/pytorch-src/torch/lib/THNN/generic/SpatialClassNLLCriterion.c:60"
     ]
    }
   ],
   "source": [
    "train_model(MLP, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 55.861808478832245\n",
      "Epoch 1 loss = 54.59076404571533\n",
      "Epoch 2 loss = 53.797380566596985\n",
      "Epoch 3 loss = 53.191094636917114\n",
      "Epoch 4 loss = 52.66807159781456\n",
      "Epoch 5 loss = 52.18098449707031\n",
      "Epoch 6 loss = 51.69751679897308\n",
      "Epoch 7 loss = 51.194580405950546\n",
      "Epoch 8 loss = 50.65688365697861\n",
      "Epoch 9 loss = 50.074628949165344\n",
      "Epoch 10 loss = 49.44200122356415\n",
      "Epoch 11 loss = 48.756370306015015\n",
      "Epoch 12 loss = 48.019282549619675\n",
      "Epoch 13 loss = 47.23957505822182\n",
      "Epoch 14 loss = 46.418880090117455\n",
      "Epoch 15 loss = 45.549498587846756\n",
      "Epoch 16 loss = 44.65137520432472\n",
      "Epoch 17 loss = 43.72418200969696\n",
      "Epoch 18 loss = 42.76705865561962\n",
      "Epoch 19 loss = 41.78091523051262\n",
      "Epoch 20 loss = 40.76593078672886\n",
      "Epoch 21 loss = 39.72083881497383\n",
      "Epoch 22 loss = 38.644645400345325\n",
      "Epoch 23 loss = 37.53784937411547\n",
      "Epoch 24 loss = 36.39436551183462\n",
      "Epoch 25 loss = 35.19167825952172\n",
      "Epoch 26 loss = 33.98767836764455\n",
      "Epoch 27 loss = 32.73168759420514\n",
      "Epoch 28 loss = 31.445499220862985\n",
      "Epoch 29 loss = 30.16045959852636\n",
      "Epoch 30 loss = 28.89358583278954\n",
      "Epoch 31 loss = 27.635679287835956\n",
      "Epoch 32 loss = 26.451356172561646\n",
      "Epoch 33 loss = 24.993193933740258\n",
      "Epoch 34 loss = 22.329513754695654\n",
      "Epoch 35 loss = 21.300788438413292\n",
      "Epoch 36 loss = 21.69328035786748\n",
      "Epoch 37 loss = 19.40192669071257\n",
      "Epoch 38 loss = 21.74633002281189\n",
      "Epoch 39 loss = 22.328185795107856\n",
      "Epoch 40 loss = 19.27624101890251\n",
      "Epoch 41 loss = 21.494215379934758\n",
      "Epoch 42 loss = 18.539624228607863\n",
      "Epoch 43 loss = 14.740391871193424\n",
      "Epoch 44 loss = 12.809250887483358\n",
      "Epoch 45 loss = 23.509267386049032\n",
      "Epoch 46 loss = 21.448826832696795\n",
      "Epoch 47 loss = 13.998251458164304\n",
      "Epoch 48 loss = 12.360645934939384\n",
      "Epoch 49 loss = 19.829397476278245\n",
      "Epoch 50 loss = 12.925528926309198\n",
      "Epoch 51 loss = 9.004871571436524\n",
      "Epoch 52 loss = 12.712735519977286\n",
      "Epoch 53 loss = 15.814191723242402\n",
      "Epoch 54 loss = 8.620287388330325\n",
      "Epoch 55 loss = 7.815711344475858\n",
      "Epoch 56 loss = 19.2838870389387\n",
      "Epoch 57 loss = 7.382501360960305\n",
      "Epoch 58 loss = 10.198833256727085\n",
      "Epoch 59 loss = 13.357606679695891\n",
      "Epoch 60 loss = 6.52746866340749\n",
      "Epoch 61 loss = 12.122200927638914\n",
      "Epoch 62 loss = 9.533251075306907\n",
      "Epoch 63 loss = 13.620927825162653\n",
      "Epoch 64 loss = 19.769558089785278\n",
      "Epoch 65 loss = 8.031806251266971\n",
      "Epoch 66 loss = 6.8842518598539755\n",
      "Epoch 67 loss = 6.117898587137461\n",
      "Epoch 68 loss = 5.8591331308707595\n",
      "Epoch 69 loss = 3.542390557762701\n",
      "Epoch 70 loss = 14.470347541675437\n",
      "Epoch 71 loss = 8.51938004483236\n",
      "Epoch 72 loss = 4.182998319272883\n",
      "Epoch 73 loss = 2.651830873102881\n",
      "Epoch 74 loss = 1.7161602322594263\n",
      "Epoch 75 loss = 16.01654776475334\n",
      "Epoch 76 loss = 5.953169251966756\n",
      "Epoch 77 loss = 2.997427246242296\n",
      "Epoch 78 loss = 9.255936907837167\n",
      "Epoch 79 loss = 14.021566861658357\n",
      "Epoch 80 loss = 7.792811411316507\n",
      "Epoch 81 loss = 2.29853199608624\n",
      "Epoch 82 loss = 3.015809479460586\n",
      "Epoch 83 loss = 1.5350954779423773\n",
      "Epoch 84 loss = 0.8139297832094599\n",
      "Epoch 85 loss = 7.46518522471888\n",
      "Epoch 86 loss = 18.022797207639087\n",
      "Epoch 87 loss = 16.096968480589567\n",
      "Epoch 88 loss = 4.842176103615202\n",
      "Epoch 89 loss = 11.231128733430523\n",
      "Epoch 90 loss = 6.885814970242791\n",
      "Epoch 91 loss = 3.763526126218494\n",
      "Epoch 92 loss = 4.0822326296474785\n",
      "Epoch 93 loss = 8.046630994882435\n",
      "Epoch 94 loss = 3.4680433393805288\n",
      "Epoch 95 loss = 1.4004087385255843\n",
      "Epoch 96 loss = 1.8394939813879319\n",
      "Epoch 97 loss = 0.8679366677533835\n",
      "Epoch 98 loss = 0.4531381952401716\n",
      "Epoch 99 loss = 0.32211697948514484\n"
     ]
    }
   ],
   "source": [
    "train_model(MLP, Variable(train_input.view(train_input.size(0),-1)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.455696202531644 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_mlp = compute_nb_errors(MLP, Variable(test_input.view(test_input.size(0),-1)), Variable(train_target), 4)\n",
    "print(100*(train_error_mlp/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.999999999999996 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_mlp = compute_nb_errors(MLP, Variable(test_input.view(test_input.size(0),-1)), Variable(test_target), 4)\n",
    "print(100*(test_error_mlp/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Basic_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Basic_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(640, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.fc1(x.view(-1, 640)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = Basic_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 54.498005986213684\n",
      "Epoch 1 loss = 53.519576370716095\n",
      "Epoch 2 loss = 52.730925261974335\n",
      "Epoch 3 loss = 52.011470079422\n",
      "Epoch 4 loss = 51.306885719299316\n",
      "Epoch 5 loss = 50.57355135679245\n",
      "Epoch 6 loss = 49.77597540616989\n",
      "Epoch 7 loss = 48.90287631750107\n",
      "Epoch 8 loss = 47.96535176038742\n",
      "Epoch 9 loss = 46.977213621139526\n",
      "Epoch 10 loss = 45.9560324549675\n",
      "Epoch 11 loss = 44.91541266441345\n",
      "Epoch 12 loss = 43.863889276981354\n",
      "Epoch 13 loss = 42.80109569430351\n",
      "Epoch 14 loss = 41.72726050019264\n",
      "Epoch 15 loss = 40.64559432864189\n",
      "Epoch 16 loss = 39.55902707576752\n",
      "Epoch 17 loss = 38.47445222735405\n",
      "Epoch 18 loss = 37.40180164575577\n",
      "Epoch 19 loss = 36.34577456116676\n",
      "Epoch 20 loss = 35.31309676170349\n",
      "Epoch 21 loss = 34.30517861247063\n",
      "Epoch 22 loss = 33.31542217731476\n",
      "Epoch 23 loss = 32.36064922809601\n",
      "Epoch 24 loss = 31.455952614545822\n",
      "Epoch 25 loss = 30.621112316846848\n",
      "Epoch 26 loss = 29.83599954843521\n",
      "Epoch 27 loss = 29.091609865427017\n",
      "Epoch 28 loss = 28.385593682527542\n",
      "Epoch 29 loss = 27.71784856915474\n",
      "Epoch 30 loss = 27.08013343811035\n",
      "Epoch 31 loss = 26.471470445394516\n",
      "Epoch 32 loss = 25.892269998788834\n",
      "Epoch 33 loss = 25.33688971400261\n",
      "Epoch 34 loss = 24.80695317685604\n",
      "Epoch 35 loss = 24.297536432743073\n",
      "Epoch 36 loss = 23.8085807710886\n",
      "Epoch 37 loss = 23.3407344520092\n",
      "Epoch 38 loss = 22.890871182084084\n",
      "Epoch 39 loss = 22.462048381567\n",
      "Epoch 40 loss = 22.04953905940056\n",
      "Epoch 41 loss = 21.65613017976284\n",
      "Epoch 42 loss = 21.275663554668427\n",
      "Epoch 43 loss = 20.906475737690926\n",
      "Epoch 44 loss = 20.54767796397209\n",
      "Epoch 45 loss = 20.202588871121407\n",
      "Epoch 46 loss = 19.865014672279358\n",
      "Epoch 47 loss = 19.53286224603653\n",
      "Epoch 48 loss = 19.20836329460144\n",
      "Epoch 49 loss = 18.891260266304016\n",
      "Epoch 50 loss = 18.583666563034058\n",
      "Epoch 51 loss = 18.287342324852943\n",
      "Epoch 52 loss = 18.00163134932518\n",
      "Epoch 53 loss = 17.72761470079422\n",
      "Epoch 54 loss = 17.46428184211254\n",
      "Epoch 55 loss = 17.210159450769424\n",
      "Epoch 56 loss = 16.9658125936985\n",
      "Epoch 57 loss = 16.72996048629284\n",
      "Epoch 58 loss = 16.501679465174675\n",
      "Epoch 59 loss = 16.28134447336197\n",
      "Epoch 60 loss = 16.06744134426117\n",
      "Epoch 61 loss = 15.863640859723091\n",
      "Epoch 62 loss = 15.66768342256546\n",
      "Epoch 63 loss = 15.479718893766403\n",
      "Epoch 64 loss = 15.301828876137733\n",
      "Epoch 65 loss = 15.132765635848045\n",
      "Epoch 66 loss = 14.971958667039871\n",
      "Epoch 67 loss = 14.817156955599785\n",
      "Epoch 68 loss = 14.671055093407631\n",
      "Epoch 69 loss = 14.533242762088776\n",
      "Epoch 70 loss = 14.402028396725655\n",
      "Epoch 71 loss = 14.277263030409813\n",
      "Epoch 72 loss = 14.158192783594131\n",
      "Epoch 73 loss = 14.04104134440422\n",
      "Epoch 74 loss = 13.922865718603134\n",
      "Epoch 75 loss = 13.79942874610424\n",
      "Epoch 76 loss = 13.673272088170052\n",
      "Epoch 77 loss = 13.547573789954185\n",
      "Epoch 78 loss = 13.423323675990105\n",
      "Epoch 79 loss = 13.303480252623558\n",
      "Epoch 80 loss = 13.190399900078773\n",
      "Epoch 81 loss = 13.084183022379875\n",
      "Epoch 82 loss = 12.985377386212349\n",
      "Epoch 83 loss = 12.893768697977066\n",
      "Epoch 84 loss = 12.808810248970985\n",
      "Epoch 85 loss = 12.729156851768494\n",
      "Epoch 86 loss = 12.655004635453224\n",
      "Epoch 87 loss = 12.585136085748672\n",
      "Epoch 88 loss = 12.519690230488777\n",
      "Epoch 89 loss = 12.458024248480797\n",
      "Epoch 90 loss = 12.399600893259048\n",
      "Epoch 91 loss = 12.344199568033218\n",
      "Epoch 92 loss = 12.291459187865257\n",
      "Epoch 93 loss = 12.241384372115135\n",
      "Epoch 94 loss = 12.193539321422577\n",
      "Epoch 95 loss = 12.148138523101807\n",
      "Epoch 96 loss = 12.104768931865692\n",
      "Epoch 97 loss = 12.063279837369919\n",
      "Epoch 98 loss = 12.023726433515549\n",
      "Epoch 99 loss = 11.985645055770874\n"
     ]
    }
   ],
   "source": [
    "train_model(cnn, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.31645569620253167 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn = compute_nb_errors(cnn, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "print(100*(train_error_cnn/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32.7 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn = compute_nb_errors(cnn, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)\n",
    "print(100*(test_error_cnn/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN w 1D filters\n",
    "\n",
    "v1:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_1D_conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_1D_conv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1,5))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1,3))\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(1,5))\n",
    "        self.fc1 = nn.Linear(896*2, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=(1,2), stride=(1,2)))\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=(1,3), stride=(1,3)))\n",
    "        x = F.tanh(F.max_pool2d(self.conv3(x), kernel_size=(1,3), stride=(1,3)))\n",
    "        x = F.tanh(self.fc1(x.view(-1, 896*2)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 55.29474902153015\n",
      "Epoch 1 loss = 55.05597680807114\n",
      "Epoch 2 loss = 54.855808675289154\n",
      "Epoch 3 loss = 54.682802975177765\n",
      "Epoch 4 loss = 54.524170219898224\n",
      "Epoch 5 loss = 54.372238516807556\n",
      "Epoch 6 loss = 54.2225626707077\n",
      "Epoch 7 loss = 54.07223069667816\n",
      "Epoch 8 loss = 53.92045372724533\n",
      "Epoch 9 loss = 53.7664595246315\n",
      "Epoch 10 loss = 53.6088582277298\n",
      "Epoch 11 loss = 53.44771748781204\n",
      "Epoch 12 loss = 53.28130882978439\n",
      "Epoch 13 loss = 53.10787773132324\n",
      "Epoch 14 loss = 52.924775540828705\n",
      "Epoch 15 loss = 52.73032104969025\n",
      "Epoch 16 loss = 52.52050358057022\n",
      "Epoch 17 loss = 52.2923142015934\n",
      "Epoch 18 loss = 52.04249811172485\n",
      "Epoch 19 loss = 51.76665839552879\n",
      "Epoch 20 loss = 51.46092280745506\n",
      "Epoch 21 loss = 51.12171161174774\n",
      "Epoch 22 loss = 50.74609687924385\n",
      "Epoch 23 loss = 50.327125549316406\n",
      "Epoch 24 loss = 49.860064417123795\n",
      "Epoch 25 loss = 49.33736914396286\n",
      "Epoch 26 loss = 48.7524134516716\n",
      "Epoch 27 loss = 48.09661191701889\n",
      "Epoch 28 loss = 47.363799035549164\n",
      "Epoch 29 loss = 46.548118352890015\n",
      "Epoch 30 loss = 45.64110191166401\n",
      "Epoch 31 loss = 44.62833517789841\n",
      "Epoch 32 loss = 43.5095302015543\n",
      "Epoch 33 loss = 42.288945361971855\n",
      "Epoch 34 loss = 40.98556010425091\n",
      "Epoch 35 loss = 39.624012991786\n",
      "Epoch 36 loss = 38.224144004285336\n",
      "Epoch 37 loss = 36.83618910610676\n",
      "Epoch 38 loss = 35.50140971690416\n",
      "Epoch 39 loss = 34.220379054546356\n",
      "Epoch 40 loss = 32.987858686596155\n",
      "Epoch 41 loss = 31.78924137726426\n",
      "Epoch 42 loss = 30.60187002643943\n",
      "Epoch 43 loss = 29.414099875837564\n",
      "Epoch 44 loss = 28.20890563353896\n",
      "Epoch 45 loss = 26.9710533618927\n",
      "Epoch 46 loss = 25.724096946418285\n",
      "Epoch 47 loss = 24.45225503668189\n",
      "Epoch 48 loss = 23.141171157360077\n",
      "Epoch 49 loss = 21.80590709671378\n",
      "Epoch 50 loss = 20.482658119872212\n",
      "Epoch 51 loss = 19.184796921908855\n",
      "Epoch 52 loss = 17.91424261406064\n",
      "Epoch 53 loss = 16.68403071537614\n",
      "Epoch 54 loss = 15.473992392420769\n",
      "Epoch 55 loss = 14.307200957089663\n",
      "Epoch 56 loss = 13.158826867118478\n",
      "Epoch 57 loss = 12.065468588843942\n",
      "Epoch 58 loss = 11.004322789609432\n",
      "Epoch 59 loss = 10.01800771523267\n",
      "Epoch 60 loss = 9.092921589501202\n",
      "Epoch 61 loss = 8.246936591342092\n",
      "Epoch 62 loss = 7.473065987229347\n",
      "Epoch 63 loss = 6.765459856018424\n",
      "Epoch 64 loss = 6.132833236362785\n",
      "Epoch 65 loss = 5.56078403769061\n",
      "Epoch 66 loss = 5.051177802029997\n",
      "Epoch 67 loss = 4.5967433960177\n",
      "Epoch 68 loss = 4.186151412315667\n",
      "Epoch 69 loss = 3.8177803866565228\n",
      "Epoch 70 loss = 3.485889774048701\n",
      "Epoch 71 loss = 3.189264864195138\n",
      "Epoch 72 loss = 2.920124291209504\n",
      "Epoch 73 loss = 2.6755923714954406\n",
      "Epoch 74 loss = 2.4577202387154102\n",
      "Epoch 75 loss = 2.259887665626593\n",
      "Epoch 76 loss = 2.082024419447407\n",
      "Epoch 77 loss = 1.9214594420045614\n",
      "Epoch 78 loss = 1.7766964577604085\n",
      "Epoch 79 loss = 1.6465725870220922\n",
      "Epoch 80 loss = 1.5289256813121028\n",
      "Epoch 81 loss = 1.422422197414562\n",
      "Epoch 82 loss = 1.3253437340026721\n",
      "Epoch 83 loss = 1.238339542527683\n",
      "Epoch 84 loss = 1.1580220417818055\n",
      "Epoch 85 loss = 1.085460348345805\n",
      "Epoch 86 loss = 1.019526947668055\n",
      "Epoch 87 loss = 0.9589260969660245\n",
      "Epoch 88 loss = 0.9038629011774901\n",
      "Epoch 89 loss = 0.8529104291810654\n",
      "Epoch 90 loss = 0.8064452380058356\n",
      "Epoch 91 loss = 0.7638936454895884\n",
      "Epoch 92 loss = 0.7245852873311378\n",
      "Epoch 93 loss = 0.6883455333882011\n",
      "Epoch 94 loss = 0.6548019158944953\n",
      "Epoch 95 loss = 0.6237556178239174\n",
      "Epoch 96 loss = 0.5950728290190455\n",
      "Epoch 97 loss = 0.5683711224410217\n",
      "Epoch 98 loss = 0.5436698693956714\n",
      "Epoch 99 loss = 0.5205931734817568\n"
     ]
    }
   ],
   "source": [
    "train_model(cnn2, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn2 = compute_nb_errors(cnn2, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "print(100*(train_error_cnn2/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn2 = compute_nb_errors(cnn2, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)\n",
    "print(100*(test_error_cnn2/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "v2 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 56, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(56, 112, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(112, 112, kernel_size=5)\n",
    "        #self.dropout = nn.Dropout(0.5)\n",
    "        self.fc1 = nn.Linear(112, 56)\n",
    "        self.fc2 = nn.Linear(56, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.conv2(x), kernel_size=3, stride=3))\n",
    "        x = F.tanh(F.max_pool1d(self.conv3(x), kernel_size=3, stride=3))\n",
    "        #x = self.dropout(x)\n",
    "        x = F.tanh(self.fc1(x.view(-1, 112)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 54.80717808008194\n",
      "Epoch 1 loss = 54.70321995019913\n",
      "Epoch 2 loss = 54.62144333124161\n",
      "Epoch 3 loss = 54.54609006643295\n",
      "Epoch 4 loss = 54.47215002775192\n",
      "Epoch 5 loss = 54.397279500961304\n",
      "Epoch 6 loss = 54.31979596614838\n",
      "Epoch 7 loss = 54.23857867717743\n",
      "Epoch 8 loss = 54.152719497680664\n",
      "Epoch 9 loss = 54.06127589941025\n",
      "Epoch 10 loss = 53.963149666786194\n",
      "Epoch 11 loss = 53.85764080286026\n",
      "Epoch 12 loss = 53.74411725997925\n",
      "Epoch 13 loss = 53.621960520744324\n",
      "Epoch 14 loss = 53.490806221961975\n",
      "Epoch 15 loss = 53.3504199385643\n",
      "Epoch 16 loss = 53.20103591680527\n",
      "Epoch 17 loss = 53.042898178100586\n",
      "Epoch 18 loss = 52.87592488527298\n",
      "Epoch 19 loss = 52.69968330860138\n",
      "Epoch 20 loss = 52.512857258319855\n",
      "Epoch 21 loss = 52.313249707221985\n",
      "Epoch 22 loss = 52.09775102138519\n",
      "Epoch 23 loss = 51.862221360206604\n",
      "Epoch 24 loss = 51.600734174251556\n",
      "Epoch 25 loss = 51.30662363767624\n",
      "Epoch 26 loss = 50.97217923402786\n",
      "Epoch 27 loss = 50.58827191591263\n",
      "Epoch 28 loss = 50.145546674728394\n",
      "Epoch 29 loss = 49.635470151901245\n",
      "Epoch 30 loss = 49.052971839904785\n",
      "Epoch 31 loss = 48.39720916748047\n",
      "Epoch 32 loss = 47.67743617296219\n",
      "Epoch 33 loss = 46.91091322898865\n",
      "Epoch 34 loss = 46.12178260087967\n",
      "Epoch 35 loss = 45.33747237920761\n",
      "Epoch 36 loss = 44.585057497024536\n",
      "Epoch 37 loss = 43.88276046514511\n",
      "Epoch 38 loss = 43.23992794752121\n",
      "Epoch 39 loss = 42.65469256043434\n",
      "Epoch 40 loss = 42.11176699399948\n",
      "Epoch 41 loss = 41.59381613135338\n",
      "Epoch 42 loss = 41.08444428443909\n",
      "Epoch 43 loss = 40.574191838502884\n",
      "Epoch 44 loss = 40.05659916996956\n",
      "Epoch 45 loss = 39.525228440761566\n",
      "Epoch 46 loss = 38.97643005847931\n",
      "Epoch 47 loss = 38.40751928091049\n",
      "Epoch 48 loss = 37.81886240839958\n",
      "Epoch 49 loss = 37.20732679963112\n",
      "Epoch 50 loss = 36.56616833806038\n",
      "Epoch 51 loss = 35.89628407359123\n",
      "Epoch 52 loss = 35.19750505685806\n",
      "Epoch 53 loss = 34.467127710580826\n",
      "Epoch 54 loss = 33.70280182361603\n",
      "Epoch 55 loss = 32.90799552202225\n",
      "Epoch 56 loss = 32.079257011413574\n",
      "Epoch 57 loss = 31.2220561504364\n",
      "Epoch 58 loss = 30.336877077817917\n",
      "Epoch 59 loss = 29.43371231853962\n",
      "Epoch 60 loss = 28.521252885460854\n",
      "Epoch 61 loss = 27.609381422400475\n",
      "Epoch 62 loss = 26.705064982175827\n",
      "Epoch 63 loss = 25.81715965270996\n",
      "Epoch 64 loss = 24.946798160672188\n",
      "Epoch 65 loss = 24.0969417989254\n",
      "Epoch 66 loss = 23.268715247511864\n",
      "Epoch 67 loss = 22.465265810489655\n",
      "Epoch 68 loss = 21.68551456928253\n",
      "Epoch 69 loss = 20.929760172963142\n",
      "Epoch 70 loss = 20.197027780115604\n",
      "Epoch 71 loss = 19.48685085773468\n",
      "Epoch 72 loss = 18.79366946965456\n",
      "Epoch 73 loss = 18.11371350288391\n",
      "Epoch 74 loss = 17.448977544903755\n",
      "Epoch 75 loss = 16.79688136279583\n",
      "Epoch 76 loss = 16.153860576450825\n",
      "Epoch 77 loss = 15.521179631352425\n",
      "Epoch 78 loss = 14.890969168394804\n",
      "Epoch 79 loss = 14.270723462104797\n",
      "Epoch 80 loss = 13.655808195471764\n",
      "Epoch 81 loss = 13.047254204750061\n",
      "Epoch 82 loss = 12.443563140928745\n",
      "Epoch 83 loss = 11.85163190215826\n",
      "Epoch 84 loss = 11.260812729597092\n",
      "Epoch 85 loss = 10.682115264236927\n",
      "Epoch 86 loss = 10.107771836221218\n",
      "Epoch 87 loss = 9.542313646525145\n",
      "Epoch 88 loss = 8.990537386387587\n",
      "Epoch 89 loss = 8.456313788890839\n",
      "Epoch 90 loss = 7.941506223753095\n",
      "Epoch 91 loss = 7.44299029186368\n",
      "Epoch 92 loss = 6.962817462161183\n",
      "Epoch 93 loss = 6.505012100562453\n",
      "Epoch 94 loss = 6.072084203362465\n",
      "Epoch 95 loss = 5.656321510672569\n",
      "Epoch 96 loss = 5.265139950439334\n",
      "Epoch 97 loss = 4.898181421682239\n",
      "Epoch 98 loss = 4.551967371255159\n",
      "Epoch 99 loss = 4.229025145992637\n",
      "Epoch 100 loss = 3.929819437675178\n",
      "Epoch 101 loss = 3.6509207440540195\n",
      "Epoch 102 loss = 3.3982019387185574\n",
      "Epoch 103 loss = 3.1606581825762987\n",
      "Epoch 104 loss = 2.943011302500963\n",
      "Epoch 105 loss = 2.744522529654205\n",
      "Epoch 106 loss = 2.5625202348455787\n",
      "Epoch 107 loss = 2.3965079355984926\n",
      "Epoch 108 loss = 2.244069124571979\n",
      "Epoch 109 loss = 2.1043527480214834\n",
      "Epoch 110 loss = 1.9743241369724274\n",
      "Epoch 111 loss = 1.8554803794249892\n",
      "Epoch 112 loss = 1.7458152016624808\n",
      "Epoch 113 loss = 1.645016006194055\n",
      "Epoch 114 loss = 1.5512454747222364\n",
      "Epoch 115 loss = 1.465625619981438\n",
      "Epoch 116 loss = 1.3858725586906075\n",
      "Epoch 117 loss = 1.3124052952043712\n",
      "Epoch 118 loss = 1.2445298633538187\n",
      "Epoch 119 loss = 1.1818531164899468\n"
     ]
    }
   ],
   "source": [
    "cnn4 = CNN_1D()\n",
    "train_model(cnn4, Variable(train_input), Variable(train_target), 40, nb_epochs=120, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.09493670886075949 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn4 = compute_nb_errors(cnn4, Variable(train_input), Variable(train_target), 40)\n",
    "print(100*(train_error_cnn4/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73.5 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn4 = compute_nb_errors(cnn4, Variable(test_input), Variable(test_target), 40)\n",
    "print(100*(1-(test_error_cnn4/test_input.size(0))),'% test accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### previous + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_dropout, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 56, kernel_size=5)\n",
    "        self.conv2 = nn.Conv1d(56, 112, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(112, 112, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(112, 56)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(56, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.conv2(x), kernel_size=3, stride=3))\n",
    "        x = F.tanh(F.max_pool1d(self.conv3(x), kernel_size=3, stride=3))\n",
    "        x = self.fc1(x.view(-1, 112))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(F.tanh(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 54.81373769044876\n",
      "Epoch 1 loss = 54.668377578258514\n",
      "Epoch 2 loss = 54.62901657819748\n",
      "Epoch 3 loss = 54.515659153461456\n",
      "Epoch 4 loss = 54.52439606189728\n",
      "Epoch 5 loss = 54.237469136714935\n",
      "Epoch 6 loss = 54.175403356552124\n",
      "Epoch 7 loss = 53.991609275341034\n",
      "Epoch 8 loss = 54.01105797290802\n",
      "Epoch 9 loss = 53.85242795944214\n",
      "Epoch 10 loss = 53.63609594106674\n",
      "Epoch 11 loss = 53.5738622546196\n",
      "Epoch 12 loss = 53.413128554821014\n",
      "Epoch 13 loss = 53.16618990898132\n",
      "Epoch 14 loss = 53.06097322702408\n",
      "Epoch 15 loss = 52.77126497030258\n",
      "Epoch 16 loss = 52.46380579471588\n",
      "Epoch 17 loss = 52.251897275447845\n",
      "Epoch 18 loss = 52.10526090860367\n",
      "Epoch 19 loss = 51.59744983911514\n",
      "Epoch 20 loss = 51.36152005195618\n",
      "Epoch 21 loss = 51.03365182876587\n",
      "Epoch 22 loss = 50.29474097490311\n",
      "Epoch 23 loss = 49.955027401447296\n",
      "Epoch 24 loss = 49.26357388496399\n",
      "Epoch 25 loss = 48.516477048397064\n",
      "Epoch 26 loss = 47.920060098171234\n",
      "Epoch 27 loss = 47.34015101194382\n",
      "Epoch 28 loss = 46.479187071323395\n",
      "Epoch 29 loss = 45.700314462184906\n",
      "Epoch 30 loss = 44.72383964061737\n",
      "Epoch 31 loss = 44.17614609003067\n",
      "Epoch 32 loss = 43.38053569197655\n",
      "Epoch 33 loss = 42.77121365070343\n",
      "Epoch 34 loss = 42.21555379033089\n",
      "Epoch 35 loss = 41.67405363917351\n",
      "Epoch 36 loss = 41.0138064622879\n",
      "Epoch 37 loss = 40.74573215842247\n",
      "Epoch 38 loss = 39.836212903261185\n",
      "Epoch 39 loss = 39.55124592781067\n",
      "Epoch 40 loss = 38.96957564353943\n",
      "Epoch 41 loss = 38.38907963037491\n",
      "Epoch 42 loss = 37.75908660888672\n",
      "Epoch 43 loss = 37.14352938532829\n",
      "Epoch 44 loss = 36.45495381951332\n",
      "Epoch 45 loss = 35.88319593667984\n",
      "Epoch 46 loss = 34.8772489130497\n",
      "Epoch 47 loss = 34.17909014225006\n",
      "Epoch 48 loss = 33.52619290351868\n",
      "Epoch 49 loss = 32.5400710105896\n",
      "Epoch 50 loss = 31.758291870355606\n",
      "Epoch 51 loss = 31.05525651574135\n",
      "Epoch 52 loss = 30.072126001119614\n",
      "Epoch 53 loss = 29.31058633327484\n",
      "Epoch 54 loss = 28.287872210144997\n",
      "Epoch 55 loss = 27.471184015274048\n",
      "Epoch 56 loss = 26.46474976837635\n",
      "Epoch 57 loss = 25.680248767137527\n",
      "Epoch 58 loss = 24.95379027724266\n",
      "Epoch 59 loss = 24.195579141378403\n",
      "Epoch 60 loss = 23.45066736638546\n",
      "Epoch 61 loss = 22.522661462426186\n",
      "Epoch 62 loss = 21.853763937950134\n",
      "Epoch 63 loss = 20.866757094860077\n",
      "Epoch 64 loss = 20.337877228856087\n",
      "Epoch 65 loss = 19.709520764648914\n",
      "Epoch 66 loss = 18.942652329802513\n",
      "Epoch 67 loss = 18.20479527115822\n",
      "Epoch 68 loss = 17.33526001125574\n",
      "Epoch 69 loss = 16.81096263974905\n",
      "Epoch 70 loss = 16.007733650505543\n",
      "Epoch 71 loss = 15.481175780296326\n",
      "Epoch 72 loss = 15.118314653635025\n",
      "Epoch 73 loss = 14.511324524879456\n",
      "Epoch 74 loss = 13.760365635156631\n",
      "Epoch 75 loss = 13.099373191595078\n",
      "Epoch 76 loss = 12.453268885612488\n",
      "Epoch 77 loss = 11.958859086036682\n",
      "Epoch 78 loss = 11.293652899563313\n",
      "Epoch 79 loss = 10.891858391463757\n",
      "Epoch 80 loss = 10.670656442642212\n",
      "Epoch 81 loss = 10.054422814399004\n",
      "Epoch 82 loss = 9.269512955099344\n",
      "Epoch 83 loss = 8.906087685376406\n",
      "Epoch 84 loss = 8.168687107041478\n",
      "Epoch 85 loss = 7.794401986524463\n",
      "Epoch 86 loss = 7.4571825712919235\n",
      "Epoch 87 loss = 7.203381430357695\n",
      "Epoch 88 loss = 6.675417762249708\n",
      "Epoch 89 loss = 6.440882029011846\n",
      "Epoch 90 loss = 5.986491912975907\n",
      "Epoch 91 loss = 5.671619711443782\n",
      "Epoch 92 loss = 5.2231133952736855\n",
      "Epoch 93 loss = 4.983916163444519\n",
      "Epoch 94 loss = 4.549118289723992\n",
      "Epoch 95 loss = 4.518581433221698\n",
      "Epoch 96 loss = 4.184387966059148\n",
      "Epoch 97 loss = 3.8590672304853797\n",
      "Epoch 98 loss = 3.671477422118187\n",
      "Epoch 99 loss = 3.4763677520677447\n",
      "Epoch 100 loss = 3.1308714458718896\n",
      "Epoch 101 loss = 2.99355923011899\n",
      "Epoch 102 loss = 2.7842510994523764\n",
      "Epoch 103 loss = 5.583516075275838\n",
      "Epoch 104 loss = 2.4561389591544867\n",
      "Epoch 105 loss = 2.333383840508759\n",
      "Epoch 106 loss = 2.210252503864467\n",
      "Epoch 107 loss = 2.144850231707096\n",
      "Epoch 108 loss = 1.9758856864646077\n",
      "Epoch 109 loss = 1.8891642959788442\n",
      "Epoch 110 loss = 1.7669282350689173\n",
      "Epoch 111 loss = 1.61881736619398\n",
      "Epoch 112 loss = 1.5223373123444617\n",
      "Epoch 113 loss = 1.5418616137467325\n",
      "Epoch 114 loss = 1.4277844629250467\n",
      "Epoch 115 loss = 1.3539427537471056\n",
      "Epoch 116 loss = 1.3008825723081827\n",
      "Epoch 117 loss = 1.2581399232149124\n",
      "Epoch 118 loss = 1.2164256237447262\n",
      "Epoch 119 loss = 1.072476220317185\n",
      "Epoch 120 loss = 1.071033216547221\n",
      "Epoch 121 loss = 1.0725259415339679\n",
      "Epoch 122 loss = 0.9851158270612359\n",
      "Epoch 123 loss = 0.9462492645252496\n",
      "Epoch 124 loss = 0.9438748294487596\n",
      "Epoch 125 loss = 0.9186782687902451\n",
      "Epoch 126 loss = 0.8748916639015079\n",
      "Epoch 127 loss = 0.8157111203763634\n",
      "Epoch 128 loss = 0.8245638785883784\n",
      "Epoch 129 loss = 0.7964270401280373\n",
      "Epoch 130 loss = 0.7138981120660901\n",
      "Epoch 131 loss = 0.6987031803000718\n",
      "Epoch 132 loss = 0.7169414346572012\n",
      "Epoch 133 loss = 0.6530320809688419\n",
      "Epoch 134 loss = 0.6218856945633888\n",
      "Epoch 135 loss = 0.6409058675635606\n",
      "Epoch 136 loss = 0.59526975476183\n",
      "Epoch 137 loss = 0.6066255230689421\n",
      "Epoch 138 loss = 0.5718848146498203\n",
      "Epoch 139 loss = 0.5297152248676866\n"
     ]
    }
   ],
   "source": [
    "cnn3 = CNN_dropout()\n",
    "train_model(cnn3, Variable(train_input), Variable(train_target), 20, nb_epochs=140, learning_rate=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn3 = compute_nb_errors(cnn3, Variable(train_input), Variable(train_target), 40)\n",
    "print(100*(train_error_cnn3/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75.1 % test accuracy\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn3 = compute_nb_errors(cnn3, Variable(test_input), Variable(test_target))\n",
    "print(100*(1-(test_error_cnn3/test_input.size(0))),'% test accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "batch_sizes = [20, 40]\n",
    "learning_rates = [0.01, 0.0075, 0.005, 0.0025, 0.001]\n",
    "nb_epochs = [100, 110, 120, 130, 140, 150]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITHOUT DROPOUT\n",
      "Batch size 20\n",
      "==============\n",
      "     100 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.26239770650864\n",
      "            Epoch 10 loss = 87.35559633374214\n",
      "            Epoch 20 loss = 29.273580979555845\n",
      "            Epoch 30 loss = 3.4526535307522863\n",
      "            Epoch 40 loss = 0.8238349893363193\n",
      "            Epoch 50 loss = 0.36731731047620997\n",
      "            Epoch 60 loss = 0.22038837501895614\n",
      "            Epoch 70 loss = 0.1540524371812353\n",
      "            Epoch 80 loss = 0.11730458836245816\n",
      "            Epoch 90 loss = 0.09412753221113235\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.53126090765\n",
      "            Epoch 10 loss = 98.59997701644897\n",
      "            Epoch 20 loss = 60.63624921441078\n",
      "            Epoch 30 loss = 18.417776821181178\n",
      "            Epoch 40 loss = 3.300629359902814\n",
      "            Epoch 50 loss = 1.032872662995942\n",
      "            Epoch 60 loss = 0.49333338884753175\n",
      "            Epoch 70 loss = 0.29543064860627055\n",
      "            Epoch 80 loss = 0.20496040883881506\n",
      "            Epoch 90 loss = 0.15500706841703504\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.59150433540344\n",
      "            Epoch 10 loss = 103.38775956630707\n",
      "            Epoch 20 loss = 81.07911455631256\n",
      "            Epoch 30 loss = 46.00190983712673\n",
      "            Epoch 40 loss = 18.180700791999698\n",
      "            Epoch 50 loss = 5.071393890772015\n",
      "            Epoch 60 loss = 1.9003021612297744\n",
      "            Epoch 70 loss = 0.8992983910720795\n",
      "            Epoch 80 loss = 0.5457645792630501\n",
      "            Epoch 90 loss = 0.37807772649102844\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.49709230661392\n",
      "            Epoch 10 loss = 107.37256002426147\n",
      "            Epoch 20 loss = 102.59197014570236\n",
      "            Epoch 30 loss = 90.07705864310265\n",
      "            Epoch 40 loss = 78.27435711026192\n",
      "            Epoch 50 loss = 64.80661743879318\n",
      "            Epoch 60 loss = 47.093069434165955\n",
      "            Epoch 70 loss = 30.86269821226597\n",
      "            Epoch 80 loss = 17.84489591792226\n",
      "            Epoch 90 loss = 8.965906047262251\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.52510005235672\n",
      "            Epoch 10 loss = 108.94973629713058\n",
      "            Epoch 20 loss = 108.36450803279877\n",
      "            Epoch 30 loss = 107.59590619802475\n",
      "            Epoch 40 loss = 106.54223400354385\n",
      "            Epoch 50 loss = 105.13099944591522\n",
      "            Epoch 60 loss = 103.19310754537582\n",
      "            Epoch 70 loss = 100.09640657901764\n",
      "            Epoch 80 loss = 94.72407898306847\n",
      "            Epoch 90 loss = 87.48452967405319\n",
      "      For 100 epochs : [76.8, 73.8, 74.7, 73.4, 69.8]\n",
      "     110 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.51135629415512\n",
      "            Epoch 10 loss = 85.11633035540581\n",
      "            Epoch 20 loss = 27.73670467734337\n",
      "            Epoch 30 loss = 3.029438914731145\n",
      "            Epoch 40 loss = 0.7301638577482663\n",
      "            Epoch 50 loss = 0.3472374341217801\n",
      "            Epoch 60 loss = 0.2161354735144414\n",
      "            Epoch 70 loss = 0.1536087165150093\n",
      "            Epoch 80 loss = 0.11785333220905159\n",
      "            Epoch 90 loss = 0.09494049737986643\n",
      "            Epoch 100 loss = 0.07910489041387336\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.21122533082962\n",
      "            Epoch 10 loss = 90.68756893277168\n",
      "            Epoch 20 loss = 46.324748173356056\n",
      "            Epoch 30 loss = 10.60089641995728\n",
      "            Epoch 40 loss = 2.287317111855373\n",
      "            Epoch 50 loss = 0.7780623835278675\n",
      "            Epoch 60 loss = 0.39880555181298405\n",
      "            Epoch 70 loss = 0.2536020201223437\n",
      "            Epoch 80 loss = 0.18235580129839946\n",
      "            Epoch 90 loss = 0.14098363986704499\n",
      "            Epoch 100 loss = 0.11420035832270514\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.59313678741455\n",
      "            Epoch 10 loss = 103.1334639787674\n",
      "            Epoch 20 loss = 84.66455629467964\n",
      "            Epoch 30 loss = 55.275839656591415\n",
      "            Epoch 40 loss = 24.453213658183813\n",
      "            Epoch 50 loss = 7.159588501788676\n",
      "            Epoch 60 loss = 2.462039712932892\n",
      "            Epoch 70 loss = 1.2023000821936876\n",
      "            Epoch 80 loss = 0.684080587758217\n",
      "            Epoch 90 loss = 0.4453802878560964\n",
      "            Epoch 100 loss = 0.3213124443136621\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 110.26359337568283\n",
      "            Epoch 10 loss = 107.27280557155609\n",
      "            Epoch 20 loss = 103.78146809339523\n",
      "            Epoch 30 loss = 94.11041688919067\n",
      "            Epoch 40 loss = 80.10972613096237\n",
      "            Epoch 50 loss = 66.06647503376007\n",
      "            Epoch 60 loss = 48.04657384753227\n",
      "            Epoch 70 loss = 31.18941330909729\n",
      "            Epoch 80 loss = 17.922683896496892\n",
      "            Epoch 90 loss = 9.307389665395021\n",
      "            Epoch 100 loss = 4.871215416584164\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.42439287900925\n",
      "            Epoch 10 loss = 108.33290356397629\n",
      "            Epoch 20 loss = 107.26255488395691\n",
      "            Epoch 30 loss = 105.97636622190475\n",
      "            Epoch 40 loss = 104.40896564722061\n",
      "            Epoch 50 loss = 102.13122421503067\n",
      "            Epoch 60 loss = 98.1102322936058\n",
      "            Epoch 70 loss = 91.41327068209648\n",
      "            Epoch 80 loss = 84.05899116396904\n",
      "            Epoch 90 loss = 78.43948689103127\n",
      "            Epoch 100 loss = 73.28005756437778\n",
      "      For 110 epochs : [73.8, 72.2, 71.6, 71.7, 71.50000000000001]\n",
      "     120 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.33954054117203\n",
      "            Epoch 10 loss = 88.95104831457138\n",
      "            Epoch 20 loss = 29.583589512854815\n",
      "            Epoch 30 loss = 3.576523730996996\n",
      "            Epoch 40 loss = 0.8272107474622317\n",
      "            Epoch 50 loss = 0.34217565951985307\n",
      "            Epoch 60 loss = 0.20117059792391956\n",
      "            Epoch 70 loss = 0.13983733508212026\n",
      "            Epoch 80 loss = 0.10613988096883986\n",
      "            Epoch 90 loss = 0.08495210919500096\n",
      "            Epoch 100 loss = 0.07050064733630279\n",
      "            Epoch 110 loss = 0.06004626919457223\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.3211213350296\n",
      "            Epoch 10 loss = 95.12313133478165\n",
      "            Epoch 20 loss = 51.758888438344\n",
      "            Epoch 30 loss = 12.688811164349318\n",
      "            Epoch 40 loss = 2.4692856492474675\n",
      "            Epoch 50 loss = 0.8286505435826257\n",
      "            Epoch 60 loss = 0.41267726686783135\n",
      "            Epoch 70 loss = 0.25525985716376454\n",
      "            Epoch 80 loss = 0.18013736401917413\n",
      "            Epoch 90 loss = 0.13755597069393843\n",
      "            Epoch 100 loss = 0.11049488466233015\n",
      "            Epoch 110 loss = 0.09190730296541005\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.59779751300812\n",
      "            Epoch 10 loss = 103.71494746208191\n",
      "            Epoch 20 loss = 83.46086114645004\n",
      "            Epoch 30 loss = 52.33801659941673\n",
      "            Epoch 40 loss = 22.896995320916176\n",
      "            Epoch 50 loss = 6.606267931871116\n",
      "            Epoch 60 loss = 2.0875947633758187\n",
      "            Epoch 70 loss = 0.9650038053514436\n",
      "            Epoch 80 loss = 0.5670202603796497\n",
      "            Epoch 90 loss = 0.38511064572958276\n",
      "            Epoch 100 loss = 0.28549248341005296\n",
      "            Epoch 110 loss = 0.2240935679874383\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.52622425556183\n",
      "            Epoch 10 loss = 107.47157788276672\n",
      "            Epoch 20 loss = 103.63430446386337\n",
      "            Epoch 30 loss = 93.16664415597916\n",
      "            Epoch 40 loss = 79.94392976164818\n",
      "            Epoch 50 loss = 65.77411982417107\n",
      "            Epoch 60 loss = 47.02631939202547\n",
      "            Epoch 70 loss = 29.89090909808874\n",
      "            Epoch 80 loss = 16.461278945207596\n",
      "            Epoch 90 loss = 8.051386162638664\n",
      "            Epoch 100 loss = 4.1116436258889735\n",
      "            Epoch 110 loss = 2.369678360177204\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.73817306756973\n",
      "            Epoch 10 loss = 108.92421078681946\n",
      "            Epoch 20 loss = 108.15279448032379\n",
      "            Epoch 30 loss = 107.17358458042145\n",
      "            Epoch 40 loss = 105.87871193885803\n",
      "            Epoch 50 loss = 104.10844349861145\n",
      "            Epoch 60 loss = 101.31773644685745\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 70 loss = 96.2366686463356\n",
      "            Epoch 80 loss = 88.43559655547142\n",
      "            Epoch 90 loss = 81.34048029780388\n",
      "            Epoch 100 loss = 75.77934151887894\n",
      "            Epoch 110 loss = 70.32732009887695\n",
      "      For 120 epochs : [72.7, 73.8, 72.1, 76.3, 70.7]\n",
      "     130 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.41912245750427\n",
      "            Epoch 10 loss = 89.2061228454113\n",
      "            Epoch 20 loss = 29.748647063970566\n",
      "            Epoch 30 loss = 3.4543337021023035\n",
      "            Epoch 40 loss = 0.8478749920614064\n",
      "            Epoch 50 loss = 0.35630048502935097\n",
      "            Epoch 60 loss = 0.20666124668787234\n",
      "            Epoch 70 loss = 0.14177108833973762\n",
      "            Epoch 80 loss = 0.10677547605882864\n",
      "            Epoch 90 loss = 0.08511241612723097\n",
      "            Epoch 100 loss = 0.07045971160550835\n",
      "            Epoch 110 loss = 0.05992539228464011\n",
      "            Epoch 120 loss = 0.05200195697398158\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.15305829048157\n",
      "            Epoch 10 loss = 91.8991006910801\n",
      "            Epoch 20 loss = 47.86110046505928\n",
      "            Epoch 30 loss = 10.336570682935417\n",
      "            Epoch 40 loss = 2.109043473144993\n",
      "            Epoch 50 loss = 0.7552615426830016\n",
      "            Epoch 60 loss = 0.4068705244571902\n",
      "            Epoch 70 loss = 0.2659091118257493\n",
      "            Epoch 80 loss = 0.1936101824248908\n",
      "            Epoch 90 loss = 0.15059762910823338\n",
      "            Epoch 100 loss = 0.12239792483887868\n",
      "            Epoch 110 loss = 0.10262664058973314\n",
      "            Epoch 120 loss = 0.08802390179334907\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.51589125394821\n",
      "            Epoch 10 loss = 103.19141989946365\n",
      "            Epoch 20 loss = 83.09210923314095\n",
      "            Epoch 30 loss = 52.50960059463978\n",
      "            Epoch 40 loss = 22.076795529574156\n",
      "            Epoch 50 loss = 6.162675324827433\n",
      "            Epoch 60 loss = 2.302048502722755\n",
      "            Epoch 70 loss = 1.1022385370451957\n",
      "            Epoch 80 loss = 0.6438154260977171\n",
      "            Epoch 90 loss = 0.429407246701885\n",
      "            Epoch 100 loss = 0.3138679915864486\n",
      "            Epoch 110 loss = 0.24422541467356496\n",
      "            Epoch 120 loss = 0.19840114630642347\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.73865395784378\n",
      "            Epoch 10 loss = 107.27918821573257\n",
      "            Epoch 20 loss = 102.11931222677231\n",
      "            Epoch 30 loss = 88.2502712905407\n",
      "            Epoch 40 loss = 76.38900458812714\n",
      "            Epoch 50 loss = 61.9489713460207\n",
      "            Epoch 60 loss = 43.91553493589163\n",
      "            Epoch 70 loss = 28.6010176949203\n",
      "            Epoch 80 loss = 16.22612714767456\n",
      "            Epoch 90 loss = 8.412237823009491\n",
      "            Epoch 100 loss = 4.4460413423366845\n",
      "            Epoch 110 loss = 2.568923344835639\n",
      "            Epoch 120 loss = 1.6510524593759328\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.72667372226715\n",
      "            Epoch 10 loss = 109.0060288310051\n",
      "            Epoch 20 loss = 108.34453356266022\n",
      "            Epoch 30 loss = 107.51736015081406\n",
      "            Epoch 40 loss = 106.38915151357651\n",
      "            Epoch 50 loss = 104.85490638017654\n",
      "            Epoch 60 loss = 102.63380748033524\n",
      "            Epoch 70 loss = 98.86616849899292\n",
      "            Epoch 80 loss = 92.48658663034439\n",
      "            Epoch 90 loss = 84.99448281526566\n",
      "            Epoch 100 loss = 79.11528906226158\n",
      "            Epoch 110 loss = 73.92963525652885\n",
      "            Epoch 120 loss = 68.48537628352642\n",
      "      For 130 epochs : [73.6, 73.5, 76.7, 74.5, 70.30000000000001]\n",
      "     140 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.54256916046143\n",
      "            Epoch 10 loss = 92.19341304898262\n",
      "            Epoch 20 loss = 37.21160572767258\n",
      "            Epoch 30 loss = 4.623604403808713\n",
      "            Epoch 40 loss = 0.9395581346470863\n",
      "            Epoch 50 loss = 0.3794339745945763\n",
      "            Epoch 60 loss = 0.21969840244855732\n",
      "            Epoch 70 loss = 0.150348510404001\n",
      "            Epoch 80 loss = 0.11300435419252608\n",
      "            Epoch 90 loss = 0.08992603563820012\n",
      "            Epoch 100 loss = 0.07433183456305414\n",
      "            Epoch 110 loss = 0.0631312471232377\n",
      "            Epoch 120 loss = 0.05470631671778392\n",
      "            Epoch 130 loss = 0.04816185816162033\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.80230861902237\n",
      "            Epoch 10 loss = 99.76811307668686\n",
      "            Epoch 20 loss = 66.8306023478508\n",
      "            Epoch 30 loss = 23.846710227429867\n",
      "            Epoch 40 loss = 4.758076078025624\n",
      "            Epoch 50 loss = 1.3080898963380605\n",
      "            Epoch 60 loss = 0.5583072758163325\n",
      "            Epoch 70 loss = 0.32379702359321527\n",
      "            Epoch 80 loss = 0.2210900217469316\n",
      "            Epoch 90 loss = 0.16554462758358568\n",
      "            Epoch 100 loss = 0.13122055954590905\n",
      "            Epoch 110 loss = 0.108059216523543\n",
      "            Epoch 120 loss = 0.09146791165403556\n",
      "            Epoch 130 loss = 0.0790589493335574\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.30878913402557\n",
      "            Epoch 10 loss = 102.08372813463211\n",
      "            Epoch 20 loss = 80.65448519587517\n",
      "            Epoch 30 loss = 48.44690761715174\n",
      "            Epoch 40 loss = 20.499981524422765\n",
      "            Epoch 50 loss = 6.192693277262151\n",
      "            Epoch 60 loss = 2.396822136361152\n",
      "            Epoch 70 loss = 1.1333172344602644\n",
      "            Epoch 80 loss = 0.6427888437174261\n",
      "            Epoch 90 loss = 0.4261807586881332\n",
      "            Epoch 100 loss = 0.3116036104038358\n",
      "            Epoch 110 loss = 0.24239587536430918\n",
      "            Epoch 120 loss = 0.19676272990182042\n",
      "            Epoch 130 loss = 0.16471893312700558\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.99359983205795\n",
      "            Epoch 10 loss = 107.7861071228981\n",
      "            Epoch 20 loss = 104.65491300821304\n",
      "            Epoch 30 loss = 96.15924471616745\n",
      "            Epoch 40 loss = 81.40198436379433\n",
      "            Epoch 50 loss = 68.05208578705788\n",
      "            Epoch 60 loss = 49.8225220143795\n",
      "            Epoch 70 loss = 32.44438426569104\n",
      "            Epoch 80 loss = 19.371986677870154\n",
      "            Epoch 90 loss = 10.22395038139075\n",
      "            Epoch 100 loss = 5.262157975696027\n",
      "            Epoch 110 loss = 2.9042248574551195\n",
      "            Epoch 120 loss = 1.76291148934979\n",
      "            Epoch 130 loss = 1.192128456896171\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.5404104590416\n",
      "            Epoch 10 loss = 108.82304710149765\n",
      "            Epoch 20 loss = 108.22047001123428\n",
      "            Epoch 30 loss = 107.34112256765366\n",
      "            Epoch 40 loss = 105.98184084892273\n",
      "            Epoch 50 loss = 103.97549426555634\n",
      "            Epoch 60 loss = 100.95294708013535\n",
      "            Epoch 70 loss = 95.72450625896454\n",
      "            Epoch 80 loss = 88.79349592328072\n",
      "            Epoch 90 loss = 83.01047411561012\n",
      "            Epoch 100 loss = 77.9922603070736\n",
      "            Epoch 110 loss = 72.6971181333065\n",
      "            Epoch 120 loss = 66.64023360610008\n",
      "            Epoch 130 loss = 59.675648018717766\n",
      "      For 140 epochs : [75.8, 72.1, 69.9, 69.6, 73.3]\n",
      "     150 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 109.33948373794556\n",
      "            Epoch 10 loss = 89.88141241669655\n",
      "            Epoch 20 loss = 34.782953694462776\n",
      "            Epoch 30 loss = 4.03553219139576\n",
      "            Epoch 40 loss = 0.817672505392693\n",
      "            Epoch 50 loss = 0.36063702509272844\n",
      "            Epoch 60 loss = 0.21731765918957535\n",
      "            Epoch 70 loss = 0.15219226067711134\n",
      "            Epoch 80 loss = 0.11574236159503926\n",
      "            Epoch 90 loss = 0.0927008978324011\n",
      "            Epoch 100 loss = 0.076922493371967\n",
      "            Epoch 110 loss = 0.065478203650855\n",
      "            Epoch 120 loss = 0.0568279410181276\n",
      "            Epoch 130 loss = 0.05008160170473275\n",
      "            Epoch 140 loss = 0.0446804152270488\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 109.3920766711235\n",
      "            Epoch 10 loss = 97.17600718140602\n",
      "            Epoch 20 loss = 53.97906935214996\n",
      "            Epoch 30 loss = 14.408036476001143\n",
      "            Epoch 40 loss = 2.8318090490065515\n",
      "            Epoch 50 loss = 0.9132919303374365\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 60 loss = 0.45351790863787755\n",
      "            Epoch 70 loss = 0.28474878732231446\n",
      "            Epoch 80 loss = 0.2026986279815901\n",
      "            Epoch 90 loss = 0.15548856518580578\n",
      "            Epoch 100 loss = 0.12516894747386687\n",
      "            Epoch 110 loss = 0.10419405820721295\n",
      "            Epoch 120 loss = 0.0889097872786806\n",
      "            Epoch 130 loss = 0.07731400805641897\n",
      "            Epoch 140 loss = 0.06822999032010557\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 109.05028355121613\n",
      "            Epoch 10 loss = 99.27646064758301\n",
      "            Epoch 20 loss = 74.3832175731659\n",
      "            Epoch 30 loss = 39.40646857023239\n",
      "            Epoch 40 loss = 13.405586401000619\n",
      "            Epoch 50 loss = 3.786835875827819\n",
      "            Epoch 60 loss = 1.5202864434104413\n",
      "            Epoch 70 loss = 0.7924504596740007\n",
      "            Epoch 80 loss = 0.4942314198706299\n",
      "            Epoch 90 loss = 0.34665462566772476\n",
      "            Epoch 100 loss = 0.2624839688069187\n",
      "            Epoch 110 loss = 0.20923993701580912\n",
      "            Epoch 120 loss = 0.17287390466663055\n",
      "            Epoch 130 loss = 0.1466309676907258\n",
      "            Epoch 140 loss = 0.12686985211621504\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 109.870445728302\n",
      "            Epoch 10 loss = 108.68421161174774\n",
      "            Epoch 20 loss = 107.00756520032883\n",
      "            Epoch 30 loss = 103.70374804735184\n",
      "            Epoch 40 loss = 94.73455563187599\n",
      "            Epoch 50 loss = 81.4477334022522\n",
      "            Epoch 60 loss = 68.14416135847569\n",
      "            Epoch 70 loss = 49.71288953721523\n",
      "            Epoch 80 loss = 31.734343506395817\n",
      "            Epoch 90 loss = 17.634276516735554\n",
      "            Epoch 100 loss = 8.784079026430845\n",
      "            Epoch 110 loss = 4.596821471583098\n",
      "            Epoch 120 loss = 2.6425928063690662\n",
      "            Epoch 130 loss = 1.6660601207986474\n",
      "            Epoch 140 loss = 1.136125419754535\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 109.6305639743805\n",
      "            Epoch 10 loss = 108.78804099559784\n",
      "            Epoch 20 loss = 107.94434261322021\n",
      "            Epoch 30 loss = 106.82901656627655\n",
      "            Epoch 40 loss = 105.29316902160645\n",
      "            Epoch 50 loss = 103.0724425315857\n",
      "            Epoch 60 loss = 99.35205614566803\n",
      "            Epoch 70 loss = 93.01483535766602\n",
      "            Epoch 80 loss = 85.4832651913166\n",
      "            Epoch 90 loss = 79.40976476669312\n",
      "            Epoch 100 loss = 73.96738460659981\n",
      "            Epoch 110 loss = 68.5034609735012\n",
      "            Epoch 120 loss = 62.68701893091202\n",
      "            Epoch 130 loss = 56.291101060807705\n",
      "            Epoch 140 loss = 49.23604739457369\n",
      "      For 150 epochs : [75.8, 71.89999999999999, 72.1, 73.6, 69.0]\n",
      "  With minibatches of size 20 : [[76.8, 73.8, 74.7, 73.4, 69.8], [73.8, 72.2, 71.6, 71.7, 71.50000000000001], [72.7, 73.8, 72.1, 76.3, 70.7], [73.6, 73.5, 76.7, 74.5, 70.30000000000001], [75.8, 72.1, 69.9, 69.6, 73.3], [75.8, 71.89999999999999, 72.1, 73.6, 69.0]]\n",
      "Batch size 40\n",
      "==============\n",
      "     100 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.66754561662674\n",
      "            Epoch 10 loss = 52.17578774690628\n",
      "            Epoch 20 loss = 42.83812612295151\n",
      "            Epoch 30 loss = 27.779195055365562\n",
      "            Epoch 40 loss = 14.3041006103158\n",
      "            Epoch 50 loss = 4.951415354385972\n",
      "            Epoch 60 loss = 1.4111246773973107\n",
      "            Epoch 70 loss = 0.5795675660483539\n",
      "            Epoch 80 loss = 0.3254280890105292\n",
      "            Epoch 90 loss = 0.21800972754135728\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.735715329647064\n",
      "            Epoch 10 loss = 53.20639383792877\n",
      "            Epoch 20 loss = 48.08678317070007\n",
      "            Epoch 30 loss = 39.754870891571045\n",
      "            Epoch 40 loss = 27.90760374069214\n",
      "            Epoch 50 loss = 15.525358900427818\n",
      "            Epoch 60 loss = 6.288658898323774\n",
      "            Epoch 70 loss = 2.2170936223119497\n",
      "            Epoch 80 loss = 0.9962198711000383\n",
      "            Epoch 90 loss = 0.5573346591554582\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.82963848114014\n",
      "            Epoch 10 loss = 54.0178416967392\n",
      "            Epoch 20 loss = 52.66964244842529\n",
      "            Epoch 30 loss = 49.33854812383652\n",
      "            Epoch 40 loss = 42.12191525101662\n",
      "            Epoch 50 loss = 36.25872269272804\n",
      "            Epoch 60 loss = 27.597199216485023\n",
      "            Epoch 70 loss = 19.213457487523556\n",
      "            Epoch 80 loss = 12.956521198153496\n",
      "            Epoch 90 loss = 7.604127366095781\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 54.64776599407196\n",
      "            Epoch 10 loss = 54.06315577030182\n",
      "            Epoch 20 loss = 53.37432771921158\n",
      "            Epoch 30 loss = 52.284022092819214\n",
      "            Epoch 40 loss = 50.46629011631012\n",
      "            Epoch 50 loss = 46.948627173900604\n",
      "            Epoch 60 loss = 42.345121175050735\n",
      "            Epoch 70 loss = 38.79358607530594\n",
      "            Epoch 80 loss = 35.3970368206501\n",
      "            Epoch 90 loss = 31.46934276819229\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 54.789090394973755\n",
      "            Epoch 10 loss = 54.562498331069946\n",
      "            Epoch 20 loss = 54.38715171813965\n",
      "            Epoch 30 loss = 54.19892704486847\n",
      "            Epoch 40 loss = 53.980597257614136\n",
      "            Epoch 50 loss = 53.718206107616425\n",
      "            Epoch 60 loss = 53.39903849363327\n",
      "            Epoch 70 loss = 53.013598918914795\n",
      "            Epoch 80 loss = 52.554875791072845\n",
      "            Epoch 90 loss = 52.00979828834534\n",
      "      For 100 epochs : [73.8, 74.8, 73.5, 72.1, 48.8]\n",
      "     110 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.99114793539047\n",
      "            Epoch 10 loss = 52.624113619327545\n",
      "            Epoch 20 loss = 44.06384891271591\n",
      "            Epoch 30 loss = 30.340543657541275\n",
      "            Epoch 40 loss = 15.756405740976334\n",
      "            Epoch 50 loss = 5.452044663950801\n",
      "            Epoch 60 loss = 1.5673028687015176\n",
      "            Epoch 70 loss = 0.6613102934788913\n",
      "            Epoch 80 loss = 0.36432035616599023\n",
      "            Epoch 90 loss = 0.23890407790895551\n",
      "            Epoch 100 loss = 0.17374686046969146\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.81189823150635\n",
      "            Epoch 10 loss = 53.188522934913635\n",
      "            Epoch 20 loss = 47.57511180639267\n",
      "            Epoch 30 loss = 38.92222410440445\n",
      "            Epoch 40 loss = 26.223040282726288\n",
      "            Epoch 50 loss = 14.89664851129055\n",
      "            Epoch 60 loss = 6.454238381236792\n",
      "            Epoch 70 loss = 2.413956381380558\n",
      "            Epoch 80 loss = 1.056009391322732\n",
      "            Epoch 90 loss = 0.5833419489208609\n",
      "            Epoch 100 loss = 0.3686164702521637\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.66680073738098\n",
      "            Epoch 10 loss = 53.57331305742264\n",
      "            Epoch 20 loss = 51.44367969036102\n",
      "            Epoch 30 loss = 45.11773216724396\n",
      "            Epoch 40 loss = 38.76580247282982\n",
      "            Epoch 50 loss = 31.162300288677216\n",
      "            Epoch 60 loss = 22.00041927397251\n",
      "            Epoch 70 loss = 15.184137307107449\n",
      "            Epoch 80 loss = 9.153197392821312\n",
      "            Epoch 90 loss = 4.561474008485675\n",
      "            Epoch 100 loss = 2.213690150529146\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 54.82099133729935\n",
      "            Epoch 10 loss = 54.3742059469223\n",
      "            Epoch 20 loss = 53.883790016174316\n",
      "            Epoch 30 loss = 53.16403645277023\n",
      "            Epoch 40 loss = 52.08939790725708\n",
      "            Epoch 50 loss = 50.24830347299576\n",
      "            Epoch 60 loss = 46.603442311286926\n",
      "            Epoch 70 loss = 42.20140612125397\n",
      "            Epoch 80 loss = 38.785076320171356\n",
      "            Epoch 90 loss = 35.40237218141556\n",
      "            Epoch 100 loss = 31.372729003429413\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 55.182909309864044\n",
      "            Epoch 10 loss = 54.52859181165695\n",
      "            Epoch 20 loss = 54.34437143802643\n",
      "            Epoch 30 loss = 54.146613121032715\n",
      "            Epoch 40 loss = 53.920466244220734\n",
      "            Epoch 50 loss = 53.65575164556503\n",
      "            Epoch 60 loss = 53.34422588348389\n",
      "            Epoch 70 loss = 52.979461669921875\n",
      "            Epoch 80 loss = 52.55214697122574\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 90 loss = 52.04386931657791\n",
      "            Epoch 100 loss = 51.41359078884125\n",
      "      For 110 epochs : [76.0, 73.3, 76.0, 71.30000000000001, 50.4]\n",
      "     120 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.97470462322235\n",
      "            Epoch 10 loss = 52.39678418636322\n",
      "            Epoch 20 loss = 42.784855753183365\n",
      "            Epoch 30 loss = 28.444043949246407\n",
      "            Epoch 40 loss = 13.125697661191225\n",
      "            Epoch 50 loss = 3.6763510555028915\n",
      "            Epoch 60 loss = 1.1308350977487862\n",
      "            Epoch 70 loss = 0.51169493352063\n",
      "            Epoch 80 loss = 0.30108465324155986\n",
      "            Epoch 90 loss = 0.2061525417957455\n",
      "            Epoch 100 loss = 0.15410713135497645\n",
      "            Epoch 110 loss = 0.121761717076879\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.79721987247467\n",
      "            Epoch 10 loss = 52.45394325256348\n",
      "            Epoch 20 loss = 43.68911558389664\n",
      "            Epoch 30 loss = 33.635463923215866\n",
      "            Epoch 40 loss = 20.267937764525414\n",
      "            Epoch 50 loss = 10.81536839529872\n",
      "            Epoch 60 loss = 4.317954961210489\n",
      "            Epoch 70 loss = 1.729014277458191\n",
      "            Epoch 80 loss = 0.8298705508932471\n",
      "            Epoch 90 loss = 0.4743201616220176\n",
      "            Epoch 100 loss = 0.31465316528920084\n",
      "            Epoch 110 loss = 0.22976338805165142\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.84059250354767\n",
      "            Epoch 10 loss = 53.83105719089508\n",
      "            Epoch 20 loss = 51.76884424686432\n",
      "            Epoch 30 loss = 45.25156119465828\n",
      "            Epoch 40 loss = 39.476033091545105\n",
      "            Epoch 50 loss = 33.045206010341644\n",
      "            Epoch 60 loss = 23.872888788580894\n",
      "            Epoch 70 loss = 15.887348338961601\n",
      "            Epoch 80 loss = 9.421953741461039\n",
      "            Epoch 90 loss = 4.765824591740966\n",
      "            Epoch 100 loss = 2.353211272507906\n",
      "            Epoch 110 loss = 1.2904391041956842\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 54.77375012636185\n",
      "            Epoch 10 loss = 54.39390289783478\n",
      "            Epoch 20 loss = 53.938524305820465\n",
      "            Epoch 30 loss = 53.26983588933945\n",
      "            Epoch 40 loss = 52.26481330394745\n",
      "            Epoch 50 loss = 50.6056302189827\n",
      "            Epoch 60 loss = 47.36139702796936\n",
      "            Epoch 70 loss = 42.87279549241066\n",
      "            Epoch 80 loss = 39.3280993103981\n",
      "            Epoch 90 loss = 36.12770280241966\n",
      "            Epoch 100 loss = 32.64522647857666\n",
      "            Epoch 110 loss = 28.536820888519287\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 55.004267036914825\n",
      "            Epoch 10 loss = 54.69224292039871\n",
      "            Epoch 20 loss = 54.52435439825058\n",
      "            Epoch 30 loss = 54.36496818065643\n",
      "            Epoch 40 loss = 54.194056272506714\n",
      "            Epoch 50 loss = 54.000950396060944\n",
      "            Epoch 60 loss = 53.77789753675461\n",
      "            Epoch 70 loss = 53.51873731613159\n",
      "            Epoch 80 loss = 53.220074355602264\n",
      "            Epoch 90 loss = 52.88147991895676\n",
      "            Epoch 100 loss = 52.500760316848755\n",
      "            Epoch 110 loss = 52.06730276346207\n",
      "      For 120 epochs : [72.6, 74.0, 76.9, 68.6, 47.699999999999996]\n",
      "     130 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 55.006598234176636\n",
      "            Epoch 10 loss = 51.78282827138901\n",
      "            Epoch 20 loss = 42.536252617836\n",
      "            Epoch 30 loss = 27.037459209561348\n",
      "            Epoch 40 loss = 12.979643918573856\n",
      "            Epoch 50 loss = 3.9964003786444664\n",
      "            Epoch 60 loss = 1.0807534735649824\n",
      "            Epoch 70 loss = 0.47833989653736353\n",
      "            Epoch 80 loss = 0.2804392884718254\n",
      "            Epoch 90 loss = 0.19204920146148652\n",
      "            Epoch 100 loss = 0.1436459799297154\n",
      "            Epoch 110 loss = 0.1135464133694768\n",
      "            Epoch 120 loss = 0.09319474038784392\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.777180671691895\n",
      "            Epoch 10 loss = 53.15110844373703\n",
      "            Epoch 20 loss = 47.75750434398651\n",
      "            Epoch 30 loss = 38.94899433851242\n",
      "            Epoch 40 loss = 26.6340029835701\n",
      "            Epoch 50 loss = 15.662989042699337\n",
      "            Epoch 60 loss = 7.397490777075291\n",
      "            Epoch 70 loss = 2.6175115620717406\n",
      "            Epoch 80 loss = 1.1056039473041892\n",
      "            Epoch 90 loss = 0.5979774950537831\n",
      "            Epoch 100 loss = 0.3829023251309991\n",
      "            Epoch 110 loss = 0.27228992455638945\n",
      "            Epoch 120 loss = 0.2073502178536728\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.89448547363281\n",
      "            Epoch 10 loss = 53.80050837993622\n",
      "            Epoch 20 loss = 51.794042229652405\n",
      "            Epoch 30 loss = 46.52611416578293\n",
      "            Epoch 40 loss = 40.35341250896454\n",
      "            Epoch 50 loss = 34.29915902018547\n",
      "            Epoch 60 loss = 25.138756811618805\n",
      "            Epoch 70 loss = 17.002617590129375\n",
      "            Epoch 80 loss = 10.612093292176723\n",
      "            Epoch 90 loss = 5.517472978681326\n",
      "            Epoch 100 loss = 2.6348052201792598\n",
      "            Epoch 110 loss = 1.4383708937093616\n",
      "            Epoch 120 loss = 0.8774522850289941\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 55.00401175022125\n",
      "            Epoch 10 loss = 54.378345370292664\n",
      "            Epoch 20 loss = 54.01057493686676\n",
      "            Epoch 30 loss = 53.45912957191467\n",
      "            Epoch 40 loss = 52.608040392398834\n",
      "            Epoch 50 loss = 51.29152715206146\n",
      "            Epoch 60 loss = 48.813499093055725\n",
      "            Epoch 70 loss = 44.611000418663025\n",
      "            Epoch 80 loss = 40.652277529239655\n",
      "            Epoch 90 loss = 37.49709552526474\n",
      "            Epoch 100 loss = 34.20795840024948\n",
      "            Epoch 110 loss = 30.38519185781479\n",
      "            Epoch 120 loss = 25.826048478484154\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 55.05037760734558\n",
      "            Epoch 10 loss = 54.51166749000549\n",
      "            Epoch 20 loss = 54.32361578941345\n",
      "            Epoch 30 loss = 54.12968283891678\n",
      "            Epoch 40 loss = 53.905211329460144\n",
      "            Epoch 50 loss = 53.63668870925903\n",
      "            Epoch 60 loss = 53.31091898679733\n",
      "            Epoch 70 loss = 52.915276288986206\n",
      "            Epoch 80 loss = 52.437257409095764\n",
      "            Epoch 90 loss = 51.85525196790695\n",
      "            Epoch 100 loss = 51.126911640167236\n",
      "            Epoch 110 loss = 50.17719501256943\n",
      "            Epoch 120 loss = 48.91055208444595\n",
      "      For 130 epochs : [72.7, 74.4, 73.9, 73.2, 57.3]\n",
      "     140 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.769703567028046\n",
      "            Epoch 10 loss = 51.12030255794525\n",
      "            Epoch 20 loss = 39.559786438941956\n",
      "            Epoch 30 loss = 22.70552448928356\n",
      "            Epoch 40 loss = 10.236005309969187\n",
      "            Epoch 50 loss = 2.724784357473254\n",
      "            Epoch 60 loss = 0.8804652122780681\n",
      "            Epoch 70 loss = 0.4263364910148084\n",
      "            Epoch 80 loss = 0.26129920152015984\n",
      "            Epoch 90 loss = 0.18340027355588973\n",
      "            Epoch 100 loss = 0.13913226983277127\n",
      "            Epoch 110 loss = 0.11095665825996548\n",
      "            Epoch 120 loss = 0.09162205207394436\n",
      "            Epoch 130 loss = 0.07760268772835843\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.97760850191116\n",
      "            Epoch 10 loss = 53.44898933172226\n",
      "            Epoch 20 loss = 50.54381674528122\n",
      "            Epoch 30 loss = 42.18211683630943\n",
      "            Epoch 40 loss = 31.539533883333206\n",
      "            Epoch 50 loss = 19.298571303486824\n",
      "            Epoch 60 loss = 10.410389345139265\n",
      "            Epoch 70 loss = 4.081314915791154\n",
      "            Epoch 80 loss = 1.5268641719594598\n",
      "            Epoch 90 loss = 0.7569278310984373\n",
      "            Epoch 100 loss = 0.4557512227911502\n",
      "            Epoch 110 loss = 0.3115565123734996\n",
      "            Epoch 120 loss = 0.2313867323100567\n",
      "            Epoch 130 loss = 0.18166035693138838\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.76911520957947\n",
      "            Epoch 10 loss = 53.82147318124771\n",
      "            Epoch 20 loss = 51.776063203811646\n",
      "            Epoch 30 loss = 45.65087455511093\n",
      "            Epoch 40 loss = 38.72139695286751\n",
      "            Epoch 50 loss = 30.918205499649048\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 60 loss = 21.77935630083084\n",
      "            Epoch 70 loss = 15.155114434659481\n",
      "            Epoch 80 loss = 9.387870378792286\n",
      "            Epoch 90 loss = 4.896859247237444\n",
      "            Epoch 100 loss = 2.426760943606496\n",
      "            Epoch 110 loss = 1.3154400973580778\n",
      "            Epoch 120 loss = 0.7958167036995292\n",
      "            Epoch 130 loss = 0.5358434673398733\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 54.71114772558212\n",
      "            Epoch 10 loss = 54.425776302814484\n",
      "            Epoch 20 loss = 54.0970014333725\n",
      "            Epoch 30 loss = 53.601517260074615\n",
      "            Epoch 40 loss = 52.83477580547333\n",
      "            Epoch 50 loss = 51.60841566324234\n",
      "            Epoch 60 loss = 49.16741096973419\n",
      "            Epoch 70 loss = 44.63211044669151\n",
      "            Epoch 80 loss = 40.15624663233757\n",
      "            Epoch 90 loss = 36.55785861611366\n",
      "            Epoch 100 loss = 32.687134593725204\n",
      "            Epoch 110 loss = 28.083193972706795\n",
      "            Epoch 120 loss = 23.119685024023056\n",
      "            Epoch 130 loss = 18.650906533002853\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 54.89372169971466\n",
      "            Epoch 10 loss = 54.58433675765991\n",
      "            Epoch 20 loss = 54.36311739683151\n",
      "            Epoch 30 loss = 54.13249331712723\n",
      "            Epoch 40 loss = 53.87733495235443\n",
      "            Epoch 50 loss = 53.58671796321869\n",
      "            Epoch 60 loss = 53.25131368637085\n",
      "            Epoch 70 loss = 52.865584552288055\n",
      "            Epoch 80 loss = 52.42429792881012\n",
      "            Epoch 90 loss = 51.91383081674576\n",
      "            Epoch 100 loss = 51.30327498912811\n",
      "            Epoch 110 loss = 50.54018872976303\n",
      "            Epoch 120 loss = 49.55833750963211\n",
      "            Epoch 130 loss = 48.295898377895355\n",
      "      For 140 epochs : [73.8, 71.6, 73.9, 73.4, 60.9]\n",
      "     150 epochs\n",
      "   ---------------\n",
      "        Learning rate 0.01 :\n",
      "            Epoch 0 loss = 54.68795830011368\n",
      "            Epoch 10 loss = 52.194209814071655\n",
      "            Epoch 20 loss = 43.08055913448334\n",
      "            Epoch 30 loss = 27.847311109304428\n",
      "            Epoch 40 loss = 14.302263721823692\n",
      "            Epoch 50 loss = 4.390429727733135\n",
      "            Epoch 60 loss = 1.21917623328045\n",
      "            Epoch 70 loss = 0.5235659331083298\n",
      "            Epoch 80 loss = 0.3034535350743681\n",
      "            Epoch 90 loss = 0.20626391132827848\n",
      "            Epoch 100 loss = 0.15348196076229215\n",
      "            Epoch 110 loss = 0.12088342645438388\n",
      "            Epoch 120 loss = 0.09897654806263745\n",
      "            Epoch 130 loss = 0.0833329466113355\n",
      "            Epoch 140 loss = 0.07168021364486776\n",
      "        Learning rate 0.0075 :\n",
      "            Epoch 0 loss = 54.77105134725571\n",
      "            Epoch 10 loss = 53.53048688173294\n",
      "            Epoch 20 loss = 49.50888645648956\n",
      "            Epoch 30 loss = 40.25131168961525\n",
      "            Epoch 40 loss = 27.598665669560432\n",
      "            Epoch 50 loss = 16.6984051913023\n",
      "            Epoch 60 loss = 8.022907147184014\n",
      "            Epoch 70 loss = 2.990978012792766\n",
      "            Epoch 80 loss = 1.2008131272159517\n",
      "            Epoch 90 loss = 0.6299593772273511\n",
      "            Epoch 100 loss = 0.39841008570510894\n",
      "            Epoch 110 loss = 0.2812212856952101\n",
      "            Epoch 120 loss = 0.21312323363963515\n",
      "            Epoch 130 loss = 0.16950440441723913\n",
      "            Epoch 140 loss = 0.13954783766530454\n",
      "        Learning rate 0.005 :\n",
      "            Epoch 0 loss = 54.72742861509323\n",
      "            Epoch 10 loss = 53.38663446903229\n",
      "            Epoch 20 loss = 50.53505688905716\n",
      "            Epoch 30 loss = 43.42550548911095\n",
      "            Epoch 40 loss = 37.88986724615097\n",
      "            Epoch 50 loss = 30.195675641298294\n",
      "            Epoch 60 loss = 21.697554230690002\n",
      "            Epoch 70 loss = 14.874081142246723\n",
      "            Epoch 80 loss = 8.842218365520239\n",
      "            Epoch 90 loss = 4.5216663889586926\n",
      "            Epoch 100 loss = 2.323651684448123\n",
      "            Epoch 110 loss = 1.323018483351916\n",
      "            Epoch 120 loss = 0.8325624358840287\n",
      "            Epoch 130 loss = 0.571109163807705\n",
      "            Epoch 140 loss = 0.4201448066160083\n",
      "        Learning rate 0.0025 :\n",
      "            Epoch 0 loss = 55.17585498094559\n",
      "            Epoch 10 loss = 54.36979407072067\n",
      "            Epoch 20 loss = 53.9248086810112\n",
      "            Epoch 30 loss = 53.23407542705536\n",
      "            Epoch 40 loss = 52.11975234746933\n",
      "            Epoch 50 loss = 50.2425742149353\n",
      "            Epoch 60 loss = 46.65105164051056\n",
      "            Epoch 70 loss = 42.047994405031204\n",
      "            Epoch 80 loss = 38.28564888238907\n",
      "            Epoch 90 loss = 34.49957099556923\n",
      "            Epoch 100 loss = 30.120719075202942\n",
      "            Epoch 110 loss = 25.09438280761242\n",
      "            Epoch 120 loss = 20.165710777044296\n",
      "            Epoch 130 loss = 15.974449090659618\n",
      "            Epoch 140 loss = 12.46282709017396\n",
      "        Learning rate 0.001 :\n",
      "            Epoch 0 loss = 54.926306784152985\n",
      "            Epoch 10 loss = 54.59055608510971\n",
      "            Epoch 20 loss = 54.41056847572327\n",
      "            Epoch 30 loss = 54.22239601612091\n",
      "            Epoch 40 loss = 54.009112894535065\n",
      "            Epoch 50 loss = 53.759326100349426\n",
      "            Epoch 60 loss = 53.46080952882767\n",
      "            Epoch 70 loss = 53.101615607738495\n",
      "            Epoch 80 loss = 52.66638767719269\n",
      "            Epoch 90 loss = 52.13139981031418\n",
      "            Epoch 100 loss = 51.449551701545715\n",
      "            Epoch 110 loss = 50.54446029663086\n",
      "            Epoch 120 loss = 49.31231904029846\n",
      "            Epoch 130 loss = 47.666195333004\n",
      "            Epoch 140 loss = 45.64242082834244\n",
      "      For 150 epochs : [74.4, 74.9, 73.9, 74.1, 62.4]\n",
      "  With minibatches of size 40 : [[73.8, 74.8, 73.5, 72.1, 48.8], [76.0, 73.3, 76.0, 71.30000000000001, 50.4], [72.6, 74.0, 76.9, 68.6, 47.699999999999996], [72.7, 74.4, 73.9, 73.2, 57.3], [73.8, 71.6, 73.9, 73.4, 60.9], [74.4, 74.9, 73.9, 74.1, 62.4]]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"WITHOUT DROPOUT\")\n",
    "print(\"\")\n",
    "no_dropout = []\n",
    "for b_size in batch_sizes:\n",
    "    print(\"Batch size\", b_size)\n",
    "    print(\"=\"*14)\n",
    "    epoch_acc = []\n",
    "    for e in nb_epochs: \n",
    "        print(\"    \", e,\"epochs\")\n",
    "        print(\"  \", \"-\"*15)\n",
    "        lr_acc =[]\n",
    "        for lr in learning_rates:\n",
    "            print(\"        Learning rate\", lr, \":\")\n",
    "            model = CNN_1D()\n",
    "            train_model(model, Variable(train_input), Variable(train_target), b_size, nb_epochs=e, learning_rate=lr)\n",
    "            test_error = compute_nb_errors(model, Variable(test_input), Variable(test_target))\n",
    "            lr_acc.append(100*(1-(test_error/test_input.size(0))))\n",
    "        epoch_acc.append(lr_acc)\n",
    "        print(\"      For\", e, \"epochs :\", lr_acc)\n",
    "    print(\"  With minibatches of size\", b_size, \":\", epoch_acc)\n",
    "    no_dropout.append(epoch_acc)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_results = {\"batch_sizes\":batch_sizes, \"learning_rates\":learning_rates, \"nb_epochs\":nb_epochs, \"results\":no_dropout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.dump(normal_results,open('results_no_dropout.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_sizes': [20, 40],\n",
       " 'learning_rates': [0.01, 0.0075, 0.005, 0.0025, 0.001],\n",
       " 'nb_epochs': [100, 110, 120, 130, 140, 150],\n",
       " 'results': [[[76.8, 73.8, 74.7, 73.4, 69.8],\n",
       "   [73.8, 72.2, 71.6, 71.7, 71.50000000000001],\n",
       "   [72.7, 73.8, 72.1, 76.3, 70.7],\n",
       "   [73.6, 73.5, 76.7, 74.5, 70.30000000000001],\n",
       "   [75.8, 72.1, 69.9, 69.6, 73.3],\n",
       "   [75.8, 71.89999999999999, 72.1, 73.6, 69.0]],\n",
       "  [[73.8, 74.8, 73.5, 72.1, 48.8],\n",
       "   [76.0, 73.3, 76.0, 71.30000000000001, 50.4],\n",
       "   [72.6, 74.0, 76.9, 68.6, 47.699999999999996],\n",
       "   [72.7, 74.4, 73.9, 73.2, 57.3],\n",
       "   [73.8, 71.6, 73.9, 73.4, 60.9],\n",
       "   [74.4, 74.9, 73.9, 74.1, 62.4]]]}"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(open('results_no_dropout.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WITH DROPOUT\n",
      "\n",
      "Batch size 20\n",
      "==============\n",
      "    Learning rate 0.01 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.56861990690231\n",
      "            Epoch 10 loss = 97.68512445688248\n",
      "            Epoch 20 loss = 44.219020545482635\n",
      "            Epoch 30 loss = 5.120638834312558\n",
      "            Epoch 40 loss = 1.2338459701277316\n",
      "            Epoch 50 loss = 0.5521879402222112\n",
      "            Epoch 60 loss = 0.34920062322635204\n",
      "            Epoch 70 loss = 0.2755592145549599\n",
      "            Epoch 80 loss = 0.19016350763558876\n",
      "            Epoch 90 loss = 0.15994437631161418\n",
      "            Epoch 100 loss = 0.12974347618728643\n",
      "            Epoch 1 loss = 0.13672240971209249\n",
      "            Epoch 10 loss = 0.12727110175183043\n",
      "            Epoch 1 loss = 0.10125427930688602\n",
      "            Epoch 10 loss = 0.0968204613745911\n",
      "            Epoch 1 loss = 0.10534073887538398\n",
      "            Epoch 10 loss = 0.08921499579082592\n",
      "            Epoch 1 loss = 0.0914028889455949\n",
      "            Epoch 10 loss = 0.07541410003614146\n",
      "            Epoch 1 loss = 0.07375158538343385\n",
      "            Epoch 10 loss = 0.08126419101972715\n",
      "      For a learning rate of 0.01 : [73.0, 73.1, 73.5, 72.3, 72.39999999999999, 73.0]\n",
      "    Learning rate 0.0075 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.62843090295792\n",
      "            Epoch 10 loss = 98.21608650684357\n",
      "            Epoch 20 loss = 63.43432921171188\n",
      "            Epoch 30 loss = 22.24169982969761\n",
      "            Epoch 40 loss = 4.341281499247998\n",
      "            Epoch 50 loss = 1.3381346956593916\n",
      "            Epoch 60 loss = 0.6578830982325599\n",
      "            Epoch 70 loss = 0.4369636698102113\n",
      "            Epoch 80 loss = 0.3122253053879831\n",
      "            Epoch 90 loss = 0.23963762546190992\n",
      "            Epoch 100 loss = 0.20411846942442935\n",
      "            Epoch 1 loss = 0.19837641302729025\n",
      "            Epoch 10 loss = 0.1728045970085077\n",
      "            Epoch 1 loss = 0.15700763886707136\n",
      "            Epoch 10 loss = 0.1435071559244534\n",
      "            Epoch 1 loss = 0.1388901480968343\n",
      "            Epoch 10 loss = 0.12629083030697075\n",
      "            Epoch 1 loss = 0.11654621600609971\n",
      "            Epoch 10 loss = 0.09773171833512606\n",
      "            Epoch 1 loss = 0.11018532399612013\n",
      "            Epoch 10 loss = 0.1102513519799686\n",
      "      For a learning rate of 0.0075 : [75.2, 75.1, 75.4, 75.4, 75.6, 75.3]\n",
      "    Learning rate 0.005 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.57152128219604\n",
      "            Epoch 10 loss = 104.8253173828125\n",
      "            Epoch 20 loss = 86.22930234670639\n",
      "            Epoch 30 loss = 56.883259519934654\n",
      "            Epoch 40 loss = 26.098600275814533\n",
      "            Epoch 50 loss = 8.068019286729395\n",
      "            Epoch 60 loss = 2.9940146636217833\n",
      "            Epoch 70 loss = 1.4079134740168229\n",
      "            Epoch 80 loss = 0.8640981721691787\n",
      "            Epoch 90 loss = 0.6214228210155852\n",
      "            Epoch 100 loss = 0.47480609541526064\n",
      "            Epoch 1 loss = 0.42907167854718864\n",
      "            Epoch 10 loss = 0.3684646670008078\n",
      "            Epoch 1 loss = 0.35554498550482094\n",
      "            Epoch 10 loss = 0.2901668788981624\n",
      "            Epoch 1 loss = 0.2853577405330725\n",
      "            Epoch 10 loss = 0.2576447843457572\n",
      "            Epoch 1 loss = 0.24716310511576012\n",
      "            Epoch 10 loss = 0.22386534267570823\n",
      "            Epoch 1 loss = 0.22069361874309834\n",
      "            Epoch 10 loss = 0.2124225073494017\n",
      "      For a learning rate of 0.005 : [73.8, 73.8, 73.5, 73.6, 73.4, 73.7]\n",
      "    Learning rate 0.0025 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.3594121336937\n",
      "            Epoch 10 loss = 108.4105435013771\n",
      "            Epoch 20 loss = 105.75545698404312\n",
      "            Epoch 30 loss = 100.24230909347534\n",
      "            Epoch 40 loss = 88.01185545325279\n",
      "            Epoch 50 loss = 75.54851604998112\n",
      "            Epoch 60 loss = 59.67797887325287\n",
      "            Epoch 70 loss = 42.26284524798393\n",
      "            Epoch 80 loss = 26.826030373573303\n",
      "            Epoch 90 loss = 14.796065475791693\n",
      "            Epoch 100 loss = 7.5477120243012905\n",
      "            Epoch 1 loss = 7.137507160194218\n",
      "            Epoch 10 loss = 4.207017409149557\n",
      "            Epoch 1 loss = 4.0747702326625586\n",
      "            Epoch 10 loss = 2.5371380450669676\n",
      "            Epoch 1 loss = 2.446350328391418\n",
      "            Epoch 10 loss = 1.70310002239421\n",
      "            Epoch 1 loss = 1.677784666302614\n",
      "            Epoch 10 loss = 1.3118308016564697\n",
      "            Epoch 1 loss = 1.2329265616135672\n",
      "            Epoch 10 loss = 0.9859189314302057\n",
      "      For a learning rate of 0.0025 : [75.1, 75.5, 75.4, 74.8, 74.1, 73.1]\n",
      "    Learning rate 0.001 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 109.69096857309341\n",
      "            Epoch 10 loss = 108.99123287200928\n",
      "            Epoch 20 loss = 108.61261177062988\n",
      "            Epoch 30 loss = 108.04281640052795\n",
      "            Epoch 40 loss = 107.16600161790848\n",
      "            Epoch 50 loss = 105.9531564116478\n",
      "            Epoch 60 loss = 104.44089043140411\n",
      "            Epoch 70 loss = 101.91724252700806\n",
      "            Epoch 80 loss = 97.78526335954666\n",
      "            Epoch 90 loss = 92.06609332561493\n",
      "            Epoch 100 loss = 85.72497540712357\n",
      "            Epoch 1 loss = 85.1369411945343\n",
      "            Epoch 10 loss = 80.47107920050621\n",
      "            Epoch 1 loss = 79.68020674586296\n",
      "            Epoch 10 loss = 75.22263269126415\n",
      "            Epoch 1 loss = 74.47514486312866\n",
      "            Epoch 10 loss = 70.10273890197277\n",
      "            Epoch 1 loss = 69.723218947649\n",
      "            Epoch 10 loss = 63.98162969946861\n",
      "            Epoch 1 loss = 63.00163885951042\n",
      "            Epoch 10 loss = 55.89687018096447\n",
      "      For a learning rate of 0.001 : [67.30000000000001, 70.8, 70.6, 72.8, 70.30000000000001, 69.39999999999999]\n",
      "  With minibatches of size 20 : [[73.0, 73.1, 73.5, 72.3, 72.39999999999999, 73.0], [75.2, 75.1, 75.4, 75.4, 75.6, 75.3], [73.8, 73.8, 73.5, 73.6, 73.4, 73.7], [75.1, 75.5, 75.4, 74.8, 74.1, 73.1], [67.30000000000001, 70.8, 70.6, 72.8, 70.30000000000001, 69.39999999999999]]\n",
      "Batch size 40\n",
      "==============\n",
      "    Learning rate 0.01 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 54.66406261920929\n",
      "            Epoch 10 loss = 52.695162534713745\n",
      "            Epoch 20 loss = 44.93166735768318\n",
      "            Epoch 30 loss = 34.12722697854042\n",
      "            Epoch 40 loss = 18.3684239089489\n",
      "            Epoch 50 loss = 7.36179362423718\n",
      "            Epoch 60 loss = 2.1382580287754536\n",
      "            Epoch 70 loss = 0.897298735100776\n",
      "            Epoch 80 loss = 0.4998171986080706\n",
      "            Epoch 90 loss = 0.3381415222538635\n",
      "            Epoch 100 loss = 0.24739446869352832\n",
      "            Epoch 1 loss = 0.24304704036330804\n",
      "            Epoch 10 loss = 0.19185999414185062\n",
      "            Epoch 1 loss = 0.19869344122707844\n",
      "            Epoch 10 loss = 0.17098108417121693\n",
      "            Epoch 1 loss = 0.1536108668660745\n",
      "            Epoch 10 loss = 0.14005059521878138\n",
      "            Epoch 1 loss = 0.1329002336715348\n",
      "            Epoch 10 loss = 0.1318695903464686\n",
      "            Epoch 1 loss = 0.13340372551465407\n",
      "            Epoch 10 loss = 0.09913914924254641\n",
      "      For a learning rate of 0.01 : [73.2, 73.1, 73.1, 72.6, 72.39999999999999, 72.2]\n",
      "    Learning rate 0.0075 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 54.81049567461014\n",
      "            Epoch 10 loss = 52.81171107292175\n",
      "            Epoch 20 loss = 47.105011343955994\n",
      "            Epoch 30 loss = 39.14561042189598\n",
      "            Epoch 40 loss = 26.74927717447281\n",
      "            Epoch 50 loss = 15.877388007938862\n",
      "            Epoch 60 loss = 7.6357070207595825\n",
      "            Epoch 70 loss = 3.161790208891034\n",
      "            Epoch 80 loss = 1.3640930019319057\n",
      "            Epoch 90 loss = 0.7406394677236676\n",
      "            Epoch 100 loss = 0.5193929544184357\n",
      "            Epoch 1 loss = 0.4563080398365855\n",
      "            Epoch 10 loss = 0.35685411910526454\n",
      "            Epoch 1 loss = 0.33944230689667165\n",
      "            Epoch 10 loss = 0.29535032174317166\n",
      "            Epoch 1 loss = 0.28001729876268655\n",
      "            Epoch 10 loss = 0.2341560226632282\n",
      "            Epoch 1 loss = 0.2297601808095351\n",
      "            Epoch 10 loss = 0.18885271868202835\n",
      "            Epoch 1 loss = 0.19422286713961512\n",
      "            Epoch 10 loss = 0.16072780184913427\n",
      "      For a learning rate of 0.0075 : [76.8, 76.2, 75.9, 75.1, 74.6, 74.7]\n",
      "    Learning rate 0.005 :\n",
      "   ----------------------\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Epoch 1 loss = 54.91450089216232\n",
      "            Epoch 10 loss = 54.06467169523239\n",
      "            Epoch 20 loss = 52.71694880723953\n",
      "            Epoch 30 loss = 49.13769429922104\n",
      "            Epoch 40 loss = 41.97347775101662\n",
      "            Epoch 50 loss = 35.981355518102646\n",
      "            Epoch 60 loss = 27.21285429596901\n",
      "            Epoch 70 loss = 19.208084240555763\n",
      "            Epoch 80 loss = 12.621800161898136\n",
      "            Epoch 90 loss = 7.263875424861908\n",
      "            Epoch 100 loss = 3.752448473125696\n",
      "            Epoch 1 loss = 3.4975507725030184\n",
      "            Epoch 10 loss = 2.073324886150658\n",
      "            Epoch 1 loss = 1.9309103712439537\n",
      "            Epoch 10 loss = 1.257086329627782\n",
      "            Epoch 1 loss = 1.1806313153356314\n",
      "            Epoch 10 loss = 0.8548443848267198\n",
      "            Epoch 1 loss = 0.8137015693355352\n",
      "            Epoch 10 loss = 0.5689654373563826\n",
      "            Epoch 1 loss = 0.5970331872813404\n",
      "            Epoch 10 loss = 0.47645941376686096\n",
      "      For a learning rate of 0.005 : [76.0, 76.3, 76.2, 77.0, 77.0, 76.8]\n",
      "    Learning rate 0.0025 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 54.95566213130951\n",
      "            Epoch 10 loss = 54.31988149881363\n",
      "            Epoch 20 loss = 53.68212467432022\n",
      "            Epoch 30 loss = 52.749579071998596\n",
      "            Epoch 40 loss = 51.421384036540985\n",
      "            Epoch 50 loss = 49.19163703918457\n",
      "            Epoch 60 loss = 45.31329184770584\n",
      "            Epoch 70 loss = 41.40261700749397\n",
      "            Epoch 80 loss = 38.284474700689316\n",
      "            Epoch 90 loss = 34.926138788461685\n",
      "            Epoch 100 loss = 31.0321643948555\n",
      "            Epoch 1 loss = 30.302784517407417\n",
      "            Epoch 10 loss = 26.365234196186066\n",
      "            Epoch 1 loss = 26.033069029450417\n",
      "            Epoch 10 loss = 22.147339925169945\n",
      "            Epoch 1 loss = 21.53264446556568\n",
      "            Epoch 10 loss = 17.8727028593421\n",
      "            Epoch 1 loss = 17.737405739724636\n",
      "            Epoch 10 loss = 14.785745188593864\n",
      "            Epoch 1 loss = 14.256498351693153\n",
      "            Epoch 10 loss = 11.106331713497639\n",
      "      For a learning rate of 0.0025 : [70.7, 70.7, 69.39999999999999, 71.10000000000001, 71.50000000000001, 73.3]\n",
      "    Learning rate 0.001 :\n",
      "   ----------------------\n",
      "            Epoch 1 loss = 54.83039230108261\n",
      "            Epoch 10 loss = 54.67936599254608\n",
      "            Epoch 20 loss = 54.49523842334747\n",
      "            Epoch 30 loss = 54.27069890499115\n",
      "            Epoch 40 loss = 54.07359677553177\n",
      "            Epoch 50 loss = 53.767672300338745\n",
      "            Epoch 60 loss = 53.34742248058319\n",
      "            Epoch 70 loss = 52.96017634868622\n",
      "            Epoch 80 loss = 52.46834188699722\n",
      "            Epoch 90 loss = 51.913032591342926\n",
      "            Epoch 100 loss = 51.10638552904129\n",
      "            Epoch 1 loss = 50.95891749858856\n",
      "            Epoch 10 loss = 50.22554123401642\n",
      "            Epoch 1 loss = 50.2536124587059\n",
      "            Epoch 10 loss = 49.254887998104095\n",
      "            Epoch 1 loss = 48.92675894498825\n",
      "            Epoch 10 loss = 47.60741853713989\n",
      "            Epoch 1 loss = 47.44680052995682\n",
      "            Epoch 10 loss = 45.85034269094467\n",
      "            Epoch 1 loss = 45.822577238082886\n",
      "            Epoch 10 loss = 44.126912385225296\n",
      "      For a learning rate of 0.001 : [50.8, 51.4, 53.6, 56.89999999999999, 57.00000000000001, 60.6]\n",
      "  With minibatches of size 40 : [[73.2, 73.1, 73.1, 72.6, 72.39999999999999, 72.2], [76.8, 76.2, 75.9, 75.1, 74.6, 74.7], [76.0, 76.3, 76.2, 77.0, 77.0, 76.8], [70.7, 70.7, 69.39999999999999, 71.10000000000001, 71.50000000000001, 73.3], [50.8, 51.4, 53.6, 56.89999999999999, 57.00000000000001, 60.6]]\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "print(\"WITH DROPOUT\")\n",
    "print(\"\")\n",
    "with_dropout = []\n",
    "for b_size in batch_sizes:\n",
    "    print(\"Batch size\", b_size)\n",
    "    print(\"=\"*14)\n",
    "    lr_acc = []\n",
    "    for lr in learning_rates: \n",
    "        print(\"    Learning rate\", lr, \":\")\n",
    "        print(\"  \", \"-\"*22)\n",
    "        epoch_acc = []\n",
    "        model = CNN_dropout()\n",
    "        train_model(model, Variable(train_input), Variable(train_target), b_size, learning_rate=lr)\n",
    "        test_error = compute_nb_errors(model, Variable(test_input), Variable(test_target))\n",
    "        epoch_acc.append(100*(1-(test_error/test_input.size(0))))\n",
    "        for i in range(5):\n",
    "            train_model(model, Variable(train_input), Variable(train_target), b_size, nb_epochs=10, learning_rate=lr)\n",
    "            test_error = compute_nb_errors(model, Variable(test_input), Variable(test_target))\n",
    "            epoch_acc.append(100*(1-(test_error/test_input.size(0))))\n",
    "        lr_acc.append(epoch_acc)\n",
    "        print(\"      For a learning rate of\", lr, \":\", epoch_acc)\n",
    "    print(\"  With minibatches of size\", b_size, \":\", lr_acc)\n",
    "    with_dropout.append(lr_acc)\n",
    "print(\"Done.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dropout_results = {\"batch_sizes\":batch_sizes, \"learning_rates\":learning_rates, \"nb_epochs\":nb_epochs, \"results\":with_dropout}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "json.dump(dropout_results,open('results_with_dropout.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_sizes': [20, 40],\n",
       " 'learning_rates': [0.01, 0.0075, 0.005, 0.0025, 0.001],\n",
       " 'nb_epochs': [100, 110, 120, 130, 140, 150],\n",
       " 'results': [[[73.0, 73.1, 73.5, 72.3, 72.39999999999999, 73.0],\n",
       "   [75.2, 75.1, 75.4, 75.4, 75.6, 75.3],\n",
       "   [73.8, 73.8, 73.5, 73.6, 73.4, 73.7],\n",
       "   [75.1, 75.5, 75.4, 74.8, 74.1, 73.1],\n",
       "   [67.30000000000001,\n",
       "    70.8,\n",
       "    70.6,\n",
       "    72.8,\n",
       "    70.30000000000001,\n",
       "    69.39999999999999]],\n",
       "  [[73.2, 73.1, 73.1, 72.6, 72.39999999999999, 72.2],\n",
       "   [76.8, 76.2, 75.9, 75.1, 74.6, 74.7],\n",
       "   [76.0, 76.3, 76.2, 77.0, 77.0, 76.8],\n",
       "   [70.7, 70.7, 69.39999999999999, 71.10000000000001, 71.50000000000001, 73.3],\n",
       "   [50.8, 51.4, 53.6, 56.89999999999999, 57.00000000000001, 60.6]]]}"
      ]
     },
     "execution_count": 253,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json.load(open('results_with_dropout.json','r'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsgAAAJzCAYAAAASpuXAAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzKzg/wNAIMUiMJcGygAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x10e3ddac8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "colormap = 'Blues'\n",
    "normal_xticks = normal_results['nb_epochs']\n",
    "normal_yticks = normal_results['learning_rates']\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(np.array(normal_results['results'][0]).T, cmap=colormap)\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(normal_xticks)), normal_xticks)\n",
    "plt.yticks(range(len(normal_yticks)), normal_yticks)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.title('Without dropout layer and batch size 20')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(np.array(normal_results['results'][1]).T, cmap=colormap)\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(normal_xticks)), normal_xticks)\n",
    "plt.yticks(range(len(normal_yticks)), normal_yticks)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.title('Without dropout layer and batch size 40')\n",
    "\n",
    "dropout_xticks = dropout_results['nb_epochs']\n",
    "dropout_yticks = dropout_results['learning_rates']\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(dropout_results['results'][0], cmap=colormap)\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(dropout_xticks)), dropout_xticks)\n",
    "plt.yticks(range(len(dropout_yticks)), dropout_yticks)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.title('With dropout layer and batch size 20')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(dropout_results['results'][1], cmap=colormap)\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(dropout_xticks)), dropout_xticks)\n",
    "plt.yticks(range(len(dropout_yticks)), dropout_yticks)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.title('With dropout layer and batch size 40')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results_1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
