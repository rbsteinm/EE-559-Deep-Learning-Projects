{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import pickle \n",
    "import json\n",
    "\n",
    "import dlc_bci as bci\n",
    "from helpers import *\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib as mpl\n",
    "mpl.rcParams['figure.figsize'] = (10,10)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.manual_seed(1)\n",
    "torch.set_num_threads(4)\n",
    "np.random.seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input : <class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "Train target : <class 'torch.LongTensor'> torch.Size([316])\n",
      "Test input : <class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "Test target : <class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Subset of data sampled at 100Hz\n",
    "train_input , train_target = bci.load(root = './data_bci')\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input : <class 'torch.FloatTensor'> torch.Size([316, 28, 500])\n",
      "Train target : <class 'torch.LongTensor'> torch.Size([316])\n",
      "Test input : <class 'torch.FloatTensor'> torch.Size([100, 28, 500])\n",
      "Test target : <class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Full data sampled at 1Khz\n",
    "train_input , train_target = bci.load(root = './data_bci', one_khz = True)\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False, one_khz = True)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  30.8000   33.1000   37.9000  ...    76.9000   75.9000   72.3000\n",
       " -22.6000  -19.2000  -11.8000  ...    26.8000   22.9000   16.4000\n",
       "  11.2000   16.7000   26.4000  ...    64.1000   62.0000   55.9000\n",
       "             ...                ⋱                ...             \n",
       "   0.5000    0.5000    0.4000  ...    45.1000   46.2000   46.6000\n",
       "  11.0000   10.6000    9.8000  ...    41.7000   41.5000   40.9000\n",
       "  -9.9000  -10.0000  -10.3000  ...    32.9000   33.8000   32.8000\n",
       "[torch.FloatTensor of size 28x500]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.LongTensor of size 316]"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data\n",
    "Normalizing :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_mean = train_input.mean(2).mean(0).unsqueeze(1).expand(-1,train_input.size(2))\n",
    "train_std = train_input.std(2).std(0).unsqueeze(1).expand(-1,train_input.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "test_mean = test_input.mean(2).mean(0).unsqueeze(1).expand(-1,test_input.size(2))\n",
    "test_std = test_input.std(2).std(0).unsqueeze(1).expand(-1,test_input.size(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# normalize mean to 0\n",
    "train_input.sub_(train_mean)\n",
    "test_input.sub_(test_mean)\n",
    "\n",
    "# normalize variance to 1\n",
    "train_input.div_(train_input.std())\n",
    "test_input.div_(test_input.std())\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Window slicing for data augmentation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [ train_input[:,:,i::10] for i in range(10) ]\n",
    "train_input = torch.cat(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = torch.cat([train_target]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq = [ test_input[:,:,i::10] for i in range(10) ]\n",
    "test_input = torch.cat(seq)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_target = torch.cat([test_target]*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0  ,.,.) = \n",
       " -1.1184e+00 -6.6277e-01 -1.1995e+00  ...   4.5434e-01 -2.1655e-01  3.7009e-01\n",
       " -6.8330e-01  7.6446e-05 -7.4883e-01  ...   1.0049e+00  8.7448e-02  1.0891e+00\n",
       " -1.1344e+00 -6.4085e-02 -1.1812e+00  ...   9.9686e-01 -3.5741e-01  1.2122e+00\n",
       "                 ...                   ⋱                   ...                \n",
       " -3.4267e-01 -2.2721e-01 -7.1193e-02  ...   8.4621e-01  8.3997e-01  7.6820e-01\n",
       " -3.7571e-01 -1.5416e-01  2.5774e-01  ...   9.5359e-01  9.5359e-01  6.4467e-01\n",
       " -5.9022e-01 -3.2186e-01 -5.3502e-02  ...   8.7327e-01  8.9511e-01  6.1739e-01\n",
       "\n",
       "( 1  ,.,.) = \n",
       "  2.1550e+00  2.1269e+00  2.4670e+00  ...   3.4825e-01  7.7262e-01  5.7604e-01\n",
       "  5.4927e-01  4.3762e-02  6.2416e-01  ...  -1.2356e+00 -3.5877e-01 -1.0234e+00\n",
       " -7.3810e-01 -1.1999e+00 -7.4434e-01  ...  -2.0424e+00 -1.2623e+00 -1.8958e+00\n",
       "                 ...                   ⋱                   ...                \n",
       " -9.4491e-01 -8.7938e-01 -1.2351e+00  ...  -1.3225e+00 -9.8548e-01 -8.4194e-01\n",
       " -5.2861e-01 -5.3797e-01 -8.6874e-01  ...  -9.2802e-01 -4.1939e-01 -1.2607e-01\n",
       " -1.1394e+00 -8.5857e-01 -1.2205e+00  ...  -1.6543e+00 -1.2892e+00 -8.3673e-01\n",
       "\n",
       "( 2  ,.,.) = \n",
       "  8.6130e-02 -3.0080e-01  3.0456e-01  ...  -3.4449e-01  3.3082e-02 -2.3839e-01\n",
       "  4.3382e-01 -4.7735e-01  6.2728e-01  ...  -2.7452e-01  4.9935e-01 -2.5580e-01\n",
       "  4.9447e-01 -8.0675e-01  6.2553e-01  ...  -6.1640e-01  5.1007e-01 -5.7584e-01\n",
       "                 ...                   ⋱                   ...                \n",
       " -2.5842e-01  1.2851e-01  1.5036e-01  ...   2.2420e-02 -3.4579e-01  2.6893e-01\n",
       " -5.5669e-01  3.5447e-01  3.2951e-01  ...   5.1361e-01 -1.8536e-01  4.8670e-02\n",
       " -9.0226e-01 -6.9104e-02  4.1144e-01  ...   2.1486e-01 -2.1264e-01  4.1456e-01\n",
       " ... \n",
       "\n",
       "(3157,.,.) = \n",
       "  7.2270e-01  5.1675e-01  1.6102e-01  ...   9.2371e-02  3.3082e-02  1.1733e-01\n",
       "  3.9325e-01  1.5610e-01 -6.8573e-02  ...  -2.9948e-01 -3.3381e-01 -3.1197e-01\n",
       "  1.8555e-01  4.1646e-01 -2.2323e-01  ...  -5.4724e-02 -5.8832e-01 -1.4210e-01\n",
       "                 ...                   ⋱                   ...                \n",
       "  2.3773e-01 -3.0627e-02 -5.5486e-01  ...  -2.8026e-01 -5.3302e-01 -4.0508e-01\n",
       " -7.3027e-02 -8.3753e-01 -1.4710e+00  ...   7.3634e-02 -4.1822e-02 -4.4943e-02\n",
       "  1.1500e-01 -5.5589e-01 -1.3890e+00  ...   1.4309e-01 -1.4087e-01 -4.7261e-02\n",
       "\n",
       "(3158,.,.) = \n",
       " -1.6844e-02  7.5078e-01  5.4795e-01  ...   1.2357e-01 -3.6945e-01 -9.1735e-02\n",
       "  3.8701e-01  1.4043e+00  1.1359e+00  ...   6.4288e-01  1.5679e-02  4.6814e-01\n",
       "  6.7546e-01  1.8331e+00  1.3027e+00  ...   1.1622e+00  1.8243e-01  9.8438e-01\n",
       "                 ...                   ⋱                   ...                \n",
       "  5.9970e-01  8.7118e-01  8.1501e-01  ...   2.5333e-01  1.0979e-01  3.1781e-02\n",
       "  5.5418e-01  6.3843e-01  1.3916e-01  ...   2.9518e-01  1.7661e-01  2.6827e-02\n",
       "  5.6434e-01  6.7980e-01  2.8663e-01  ...   3.6776e-01  3.3031e-01  5.2593e-02\n",
       "\n",
       "(3159,.,.) = \n",
       " -1.6844e-02  5.0739e-01  1.5790e-01  ...   5.6356e-01  1.0797e-01  5.2611e-01\n",
       "  5.2119e-01  1.5291e+00  4.6190e-01  ...   1.4074e+00  5.0871e-01  1.2857e+00\n",
       "  1.2715e+00  2.3542e+00  1.0967e+00  ...   2.3605e+00  1.1498e+00  2.1545e+00\n",
       "                 ...                   ⋱                   ...                \n",
       "  2.3253e+00  2.2473e+00  2.4688e+00  ...   2.1630e+00  2.4688e+00  2.3409e+00\n",
       "  2.4888e+00  2.5949e+00  2.8321e+00  ...   2.0145e+00  2.3172e+00  2.1362e+00\n",
       "  2.9078e+00  2.8579e+00  3.2354e+00  ...   2.0746e+00  2.7362e+00  2.9078e+00\n",
       "[torch.FloatTensor of size 3160x28x50]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Splitting training and validation sets :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "split = 2500"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "indices = np.arange(train_input.size(0))\n",
    "np.random.shuffle(indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_input = train_input.clone()\n",
    "train_input = full_input[indices[:split].tolist()]\n",
    "validation_input = full_input[indices[split:].tolist()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_target = train_target.clone()\n",
    "train_target = full_target[indices[:split].tolist()]\n",
    "validation_target = full_target[indices[split:].tolist()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_pred_train = discrete_predictions(torch.FloatTensor(linear_reg.predict(train_input.view(train_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*((linear_pred_train - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_pred_test = discrete_predictions(torch.FloatTensor(linear_reg.predict(test_input.view(test_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*((linear_pred_test - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ridge Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_reg = Ridge()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ridge_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_pred_train = discrete_predictions(torch.FloatTensor(ridge_reg.predict(train_input.view(train_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*((ridge_pred_train - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ridge_pred_test = discrete_predictions(torch.FloatTensor(ridge_reg.predict(test_input.view(test_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*((ridge_pred_test - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_pred_train = torch.LongTensor(logistic_reg.predict(train_input.view(train_input.size(0),-1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*((logistic_pred_train - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_pred_test = logistic_reg.predict(test_input.view(test_input.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(100*((logistic_pred_test - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP = nn.Sequential(\n",
    "        nn.Linear(1400, 140),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(140, 28),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(28, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(MLP, Variable(full_input.view(full_input.size(0),-1)), Variable(full_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_mlp = compute_nb_errors(MLP, Variable(full_input.view(full_input.size(0),-1)), Variable(full_target))\n",
    "print(100*(train_error_mlp/full_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_mlp = compute_nb_errors(MLP, Variable(test_input.view(test_input.size(0),-1)), Variable(test_target))\n",
    "print(100*(test_error_mlp/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Basic_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Basic_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, kernel_size=3)\n",
    "        self.conv2 = nn.Conv2d(8, 16, kernel_size=5)\n",
    "        self.conv3 = nn.Conv2d(16, 16, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(128, 28)\n",
    "        self.fc2 = nn.Linear(28, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=(3,2), stride=(3,2)))\n",
    "        x = F.tanh(self.conv3(x))\n",
    "        x = F.tanh(self.fc1(x.view(x.size(0),-1)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = Basic_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_model(cnn, Variable(full_input.unsqueeze(1)), Variable(full_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_cnn = compute_nb_errors(cnn, Variable(full_input.unsqueeze(1)), Variable(full_target))\n",
    "print(100*(train_error_cnn/full_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_cnn = compute_nb_errors(cnn, Variable(test_input.unsqueeze(1)), Variable(test_target))\n",
    "print(100*(test_error_cnn/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN w 1D filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_1D(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_1D, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 28, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(28, 14, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(14, 1, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(9, 36)\n",
    "        self.fc2 = nn.Linear(36, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.conv3(x))\n",
    "        x = self.fc1(x.squeeze(1))\n",
    "        x = self.fc2(F.tanh(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_1D = CNN_1D()\n",
    "train_model(cnn_1D, Variable(full_input), Variable(full_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_cnn_1D = compute_nb_errors(cnn_1D, Variable(full_input), Variable(full_target))\n",
    "print(100*(train_error_cnn_1D/full_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_cnn_1D = compute_nb_errors(cnn_1D, Variable(test_input), Variable(test_target))\n",
    "print(100*(test_error_cnn_1D/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### previous + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_dropout(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(CNN_dropout, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 28, kernel_size=3)\n",
    "        self.conv2 = nn.Conv1d(28, 14, kernel_size=3)\n",
    "        self.conv3 = nn.Conv1d(14, 1, kernel_size=3)\n",
    "        self.fc1 = nn.Linear(9, 30)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(30, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.conv1(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.conv3(x))\n",
    "        x = self.fc1(x.squeeze(1))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(F.tanh(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "cnn_drop = CNN_dropout()\n",
    "train_model(cnn_drop, Variable(full_input), Variable(full_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_cnn_drop = compute_nb_errors(cnn_drop, Variable(full_input), Variable(full_target))\n",
    "print(100*(train_error_cnn_drop/full_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_cnn_drop = compute_nb_errors(cnn_drop, Variable(test_input), Variable(test_target))\n",
    "print(100*(test_error_cnn_drop/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_batchnorm(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_batchnorm, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 28, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(28)\n",
    "        self.conv2 = nn.Conv1d(28, 14, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(14)\n",
    "        self.conv3 = nn.Conv1d(14, 1, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(1)\n",
    "        self.fc1 = nn.Linear(9, 30)\n",
    "        self.bn4 = nn.BatchNorm1d(30)\n",
    "        self.fc2 = nn.Linear(30, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.bn1(self.conv1(x)), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.bn2(self.conv2(x)), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.bn3(self.conv3(x)))\n",
    "        x = self.fc1(x.squeeze(1))\n",
    "        x = self.fc2(F.tanh(self.bn4(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_norm = CNN_batchnorm()\n",
    "train_model(cnn_norm, Variable(full_input), Variable(full_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_cnn_norm = compute_nb_errors(cnn_norm, Variable(full_input), Variable(full_target))\n",
    "print(100*(train_error_cnn_norm/full_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_cnn_norm = compute_nb_errors(cnn_norm, Variable(test_input), Variable(test_target))\n",
    "print(100*(test_error_cnn_norm/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both dropout and batchnorm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_both(nn.Module):\n",
    "    def __init__(self, dropout=0.5):\n",
    "        super(CNN_both, self).__init__()\n",
    "        self.conv1 = nn.Conv1d(28, 28, kernel_size=3)\n",
    "        self.bn1 = nn.BatchNorm1d(28)\n",
    "        self.conv2 = nn.Conv1d(28, 14, kernel_size=3)\n",
    "        self.bn2 = nn.BatchNorm1d(14)\n",
    "        self.conv3 = nn.Conv1d(14, 1, kernel_size=3)\n",
    "        self.bn3 = nn.BatchNorm1d(1)\n",
    "        self.fc1 = nn.Linear(9, 30)\n",
    "        self.bn4 = nn.BatchNorm1d(30)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(30, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool1d(self.bn1(self.conv1(x)), kernel_size=2, stride=2))\n",
    "        x = F.tanh(F.max_pool1d(self.bn2(self.conv2(x)), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.bn3(self.conv3(x)))\n",
    "        x = self.fc1(x.squeeze(1))\n",
    "        x = self.dropout(self.bn4(x))\n",
    "        x = self.fc2(F.tanh(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "cnn_both = CNN_both()\n",
    "train_model(cnn_both, Variable(full_input), Variable(full_target), 40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_error_cnn_both = compute_nb_errors(cnn_both, Variable(full_input), Variable(full_target))\n",
    "print(100*(train_error_cnn_both/full_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_error_cnn_both = compute_nb_errors(cnn_both, Variable(test_input), Variable(test_target))\n",
    "print(100*(test_error_cnn_both/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_best_params(network, train_input, train_target, val_input, val_target, learning_rates, nb_epochs_total, epoch_step, mini_batch_size):\n",
    "    res = []\n",
    "    best_error = val_input.size(0)+1\n",
    "    #print(\"MLP\")\n",
    "    print(\"MODEL\", network().__class__.__name__)\n",
    "    print(\"=\"*18)\n",
    "    for lr in learning_rates: \n",
    "        print(\"  Learning rate\", lr, \":\")\n",
    "        print(\"-\"*23)\n",
    "        epoch_acc = []\n",
    "        torch.manual_seed(1)\n",
    "        model = network()\n",
    "        #model = nn.Sequential(\n",
    "        #    nn.Linear(1400, 140),\n",
    "        #    nn.Tanh(),\n",
    "        #    nn.Linear(140, 28),\n",
    "        #    nn.Tanh(),\n",
    "        #    nn.Linear(28, 2)\n",
    "        #)\n",
    "        criterion = nn.CrossEntropyLoss()\n",
    "        optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "        \n",
    "        for e in range(nb_epochs_total):\n",
    "            sum_loss = 0\n",
    "            model.train()\n",
    "            for b in range(0, train_input.size(0), mini_batch_size):\n",
    "                output = model(train_input.narrow(0, b, mini_batch_size))\n",
    "                loss = criterion(output, train_target.narrow(0, b, mini_batch_size))\n",
    "                sum_loss = sum_loss + loss.data[0]\n",
    "                model.zero_grad()\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                \n",
    "            if (e+1)%epoch_step == 0 or e==0:\n",
    "                validation_error = compute_nb_errors(model, val_input, val_target)\n",
    "                if (e+1)%(10*epoch_step) == 0 or e==0:\n",
    "                    print(\"    Epoch {:>4} : loss = {:1.8f} | validation error = {:>4}\".format(e+1, sum_loss, validation_error))\n",
    "                epoch_acc.append(100*(validation_error/val_input.size(0)))\n",
    "                if (validation_error < best_error) or ( (validation_error == best_error) and ((e+1) < best_epoch) ):\n",
    "                    best_error = validation_error\n",
    "                    best_epoch = e+1\n",
    "                    best_lr = lr\n",
    "                    \n",
    "        res.append(epoch_acc)\n",
    "    print(\"Done.\")\n",
    "    return res, best_epoch, best_lr, best_error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MLP test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.0025, 0.005, 0.0075, 0.01]\n",
    "nb_epochs_total = 500\n",
    "epoch_step = 5\n",
    "mini_batch_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "forward() missing 1 required positional argument: 'input'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-71-3adfffa643ba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mall_results\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbest_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfind_best_params\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mMLP\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mVariable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalidation_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlearning_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epochs_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmini_batch_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mjson_res\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\"MLP\"\u001b[0m\u001b[0;34m,\u001b[0m   \u001b[0;34m\"learning_rates\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlearning_rates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"nb_epochs_total\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mnb_epochs_total\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"epoch_step\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mepoch_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"mini_batch_size\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mmini_batch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best_epoch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbest_epoch\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best_lr\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbest_lr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"best_error\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mbest_error\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"results\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mall_results\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mjson\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdump\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjson_res\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results/adam/'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mjson_res\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.json'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/Documents/workspaces/MA4/DL_projects/miniproject_1/helpers.py\u001b[0m in \u001b[0;36mfind_best_params\u001b[0;34m(network, train_input, train_target, val_input, val_target, learning_rates, nb_epochs_total, epoch_step, mini_batch_size)\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mbest_error\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mval_input\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"MODEL\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"=\"\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m18\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.5/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: forward() missing 1 required positional argument: 'input'"
     ]
    }
   ],
   "source": [
    "all_results, best_epoch, best_lr, best_error = find_best_params(MLP, Variable(train_input.view(train_input.size(0),-1)), Variable(train_target), Variable(validation_input.view(validation_input.size(0),-1)), Variable(validation_target), learning_rates, nb_epochs_total, epoch_step, mini_batch_size)\n",
    "json_res = {\"model\":\"MLP\",   \"learning_rates\":learning_rates, \"nb_epochs_total\":nb_epochs_total, \"epoch_step\":epoch_step, \"mini_batch_size\":mini_batch_size, \"best_epoch\":best_epoch, \"best_lr\":best_lr, \"best_error\":best_error, \"results\":all_results}\n",
    "json.dump(json_res,open('results/adam/'+json_res[\"model\"]+'.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic CNN test :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.0025, 0.005, 0.0075, 0.01]\n",
    "nb_epochs_total = 100\n",
    "epoch_step = 5\n",
    "mini_batch_size = 250"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL Basic_CNN\n",
      "==================\n",
      "  Learning rate 0.001 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.88160986 | validation error =  293\n",
      "    Epoch   50 : loss = 0.01517187 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00309679 | validation error =    0\n",
      "  Learning rate 0.0025 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.95688170 | validation error =  331\n",
      "    Epoch   50 : loss = 0.00247078 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00063537 | validation error =    0\n",
      "  Learning rate 0.005 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 7.02459210 | validation error =  298\n",
      "    Epoch   50 : loss = 0.00054406 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00019461 | validation error =    0\n",
      "  Learning rate 0.0075 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.98603934 | validation error =  291\n",
      "    Epoch   50 : loss = 0.00021192 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00008684 | validation error =    0\n",
      "  Learning rate 0.01 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 7.01384193 | validation error =  298\n",
      "    Epoch   50 : loss = 0.00095626 | validation error =    1\n",
      "    Epoch  100 : loss = 0.00036234 | validation error =    1\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "all_results, best_epoch, best_lr, best_error = find_best_params(Basic_CNN, Variable(train_input.unsqueeze(1)), Variable(train_target), Variable(validation_input.unsqueeze(1)), Variable(validation_target), learning_rates, nb_epochs_total, epoch_step, mini_batch_size)\n",
    "json_res = {\"model\":\"Basic_CNN\",   \"learning_rates\":learning_rates, \"nb_epochs_total\":nb_epochs_total, \"epoch_step\":epoch_step, \"mini_batch_size\":mini_batch_size, \"best_epoch\":best_epoch, \"best_lr\":best_lr, \"best_error\":best_error, \"results\":all_results}\n",
    "json.dump(json_res,open('results/adam/'+json_res[\"model\"]+'.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All the 4 other models :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rates = [0.001, 0.0025, 0.005, 0.0075, 0.01]\n",
    "nb_epochs_total = 500\n",
    "epoch_step = 5\n",
    "mini_batch_size = 250\n",
    "\n",
    "models = [ CNN_1D, CNN_dropout, CNN_batchnorm, CNN_both ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MODEL CNN_1D\n",
      "==================\n",
      "  Learning rate 0.001 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.86664402 | validation error =  275\n",
      "    Epoch   50 : loss = 0.52575323 | validation error =   11\n",
      "    Epoch  100 : loss = 0.02483570 | validation error =    1\n",
      "    Epoch  150 : loss = 0.02050034 | validation error =    6\n",
      "    Epoch  200 : loss = 0.01125845 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00518089 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00303317 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00194533 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00131074 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00091051 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00064559 | validation error =    0\n",
      "  Learning rate 0.0025 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.82721412 | validation error =  244\n",
      "    Epoch   50 : loss = 0.02413524 | validation error =    1\n",
      "    Epoch  100 : loss = 0.00222192 | validation error =    1\n",
      "    Epoch  150 : loss = 0.00098169 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00053280 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00032164 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00020689 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00013898 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00009616 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00006803 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00004887 | validation error =    0\n",
      "  Learning rate 0.005 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.73812717 | validation error =  237\n",
      "    Epoch   50 : loss = 0.00250778 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00071952 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00034334 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00019511 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00012160 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00008018 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00005486 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00003853 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00002756 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00001999 | validation error =    0\n",
      "  Learning rate 0.0075 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.83330190 | validation error =  280\n",
      "    Epoch   50 : loss = 0.00056666 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00019261 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00010035 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00006017 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00003897 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00002648 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00001858 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00001333 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00000971 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00000714 | validation error =    0\n",
      "  Learning rate 0.01 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.92420095 | validation error =  307\n",
      "    Epoch   50 : loss = 0.00038641 | validation error =    1\n",
      "    Epoch  100 : loss = 0.00014067 | validation error =    1\n",
      "    Epoch  150 : loss = 0.00007142 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00004187 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00002662 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00001782 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00001233 | validation error =    1\n",
      "    Epoch  400 : loss = 0.00000873 | validation error =    1\n",
      "    Epoch  450 : loss = 0.00000630 | validation error =    1\n",
      "    Epoch  500 : loss = 0.00000460 | validation error =    1\n",
      "Done.\n",
      "\n",
      "MODEL CNN_dropout\n",
      "==================\n",
      "  Learning rate 0.001 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.94209796 | validation error =  268\n",
      "    Epoch   50 : loss = 0.77028267 | validation error =   25\n",
      "    Epoch  100 : loss = 0.04117636 | validation error =    2\n",
      "    Epoch  150 : loss = 0.58372222 | validation error =   12\n",
      "    Epoch  200 : loss = 0.01940300 | validation error =    1\n",
      "    Epoch  250 : loss = 0.01120402 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00712170 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00446643 | validation error =    1\n",
      "    Epoch  400 : loss = 0.00282518 | validation error =    1\n",
      "    Epoch  450 : loss = 0.00300164 | validation error =    1\n",
      "    Epoch  500 : loss = 0.00162680 | validation error =    1\n",
      "  Learning rate 0.0025 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.91762167 | validation error =  274\n",
      "    Epoch   50 : loss = 0.03825689 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00716908 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00273456 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00156542 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00102259 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00084024 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00086744 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00031577 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00025429 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00018012 | validation error =    0\n",
      "  Learning rate 0.005 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.97178483 | validation error =  279\n",
      "    Epoch   50 : loss = 0.00659284 | validation error =    1\n",
      "    Epoch  100 : loss = 0.00209265 | validation error =    1\n",
      "    Epoch  150 : loss = 0.00111906 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00052998 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00031068 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00048481 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00014993 | validation error =    1\n",
      "    Epoch  400 : loss = 0.00010193 | validation error =    1\n",
      "    Epoch  450 : loss = 0.00007915 | validation error =    1\n",
      "    Epoch  500 : loss = 0.00007709 | validation error =    1\n",
      "  Learning rate 0.0075 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.94934940 | validation error =  228\n",
      "    Epoch   50 : loss = 0.00450370 | validation error =    1\n",
      "    Epoch  100 : loss = 0.00108933 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00055270 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00049739 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00021792 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00016028 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00012900 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00007275 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00007330 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00007960 | validation error =    0\n",
      "  Learning rate 0.01 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 7.00631797 | validation error =  331\n",
      "    Epoch   50 : loss = 0.00240815 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00079493 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00041961 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00018362 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00031195 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00014232 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00012280 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00007912 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00006538 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00003647 | validation error =    0\n",
      "Done.\n",
      "\n",
      "MODEL CNN_batchnorm\n",
      "==================\n",
      "  Learning rate 0.001 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.90589267 | validation error =  304\n",
      "    Epoch   50 : loss = 0.02284231 | validation error =    2\n",
      "    Epoch  100 : loss = 0.00533439 | validation error =    2\n",
      "    Epoch  150 : loss = 0.00208272 | validation error =    2\n",
      "    Epoch  200 : loss = 0.00100041 | validation error =    2\n",
      "    Epoch  250 : loss = 0.00054975 | validation error =    2\n",
      "    Epoch  300 : loss = 0.00033157 | validation error =    2\n",
      "    Epoch  350 : loss = 0.00021330 | validation error =    2\n",
      "    Epoch  400 : loss = 0.00014347 | validation error =    2\n",
      "    Epoch  450 : loss = 0.00009969 | validation error =    2\n",
      "    Epoch  500 : loss = 0.00007096 | validation error =    2\n",
      "  Learning rate 0.0025 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.78612512 | validation error =  280\n",
      "    Epoch   50 : loss = 0.00808192 | validation error =    1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Epoch  100 : loss = 0.00175696 | validation error =    1\n",
      "    Epoch  150 : loss = 0.00074541 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00040033 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00024086 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00015497 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00010393 | validation error =    1\n",
      "    Epoch  400 : loss = 0.00007117 | validation error =    1\n",
      "    Epoch  450 : loss = 0.00005036 | validation error =    1\n",
      "    Epoch  500 : loss = 0.00003594 | validation error =    1\n",
      "  Learning rate 0.005 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.79651618 | validation error =  314\n",
      "    Epoch   50 : loss = 0.00135912 | validation error =    1\n",
      "    Epoch  100 : loss = 0.00040913 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00019286 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00010925 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00006809 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00004498 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00003086 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00002174 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00001560 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00001135 | validation error =    0\n",
      "  Learning rate 0.0075 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.55538821 | validation error =  248\n",
      "    Epoch   50 : loss = 0.00164568 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00037274 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00016345 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00008994 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00005550 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00003652 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00002501 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00001759 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00001261 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00000915 | validation error =    0\n",
      "  Learning rate 0.01 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.84054297 | validation error =  294\n",
      "    Epoch   50 : loss = 0.00134160 | validation error =    1\n",
      "    Epoch  100 : loss = 0.00028014 | validation error =    1\n",
      "    Epoch  150 : loss = 0.00012557 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00007001 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00004343 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00002866 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00001968 | validation error =    1\n",
      "    Epoch  400 : loss = 0.00001389 | validation error =    1\n",
      "    Epoch  450 : loss = 0.00001001 | validation error =    1\n",
      "    Epoch  500 : loss = 0.00000732 | validation error =    1\n",
      "Done.\n",
      "\n",
      "MODEL CNN_both\n",
      "==================\n",
      "  Learning rate 0.001 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 7.01547194 | validation error =  343\n",
      "    Epoch   50 : loss = 0.08809793 | validation error =    5\n",
      "    Epoch  100 : loss = 0.01784553 | validation error =    1\n",
      "    Epoch  150 : loss = 0.00952798 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00587674 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00366919 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00263967 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00179013 | validation error =    1\n",
      "    Epoch  400 : loss = 0.00129502 | validation error =    1\n",
      "    Epoch  450 : loss = 0.00104687 | validation error =    1\n",
      "    Epoch  500 : loss = 0.00061508 | validation error =    1\n",
      "  Learning rate 0.0025 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.84079564 | validation error =  273\n",
      "    Epoch   50 : loss = 0.01553098 | validation error =    4\n",
      "    Epoch  100 : loss = 0.00413315 | validation error =    2\n",
      "    Epoch  150 : loss = 0.00242577 | validation error =    3\n",
      "    Epoch  200 : loss = 0.00154194 | validation error =    2\n",
      "    Epoch  250 : loss = 0.00088714 | validation error =    2\n",
      "    Epoch  300 : loss = 0.00054436 | validation error =    2\n",
      "    Epoch  350 : loss = 0.00038477 | validation error =    2\n",
      "    Epoch  400 : loss = 0.00070354 | validation error =    2\n",
      "    Epoch  450 : loss = 0.00018230 | validation error =    2\n",
      "    Epoch  500 : loss = 0.00020320 | validation error =    2\n",
      "  Learning rate 0.005 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.36943758 | validation error =  279\n",
      "    Epoch   50 : loss = 0.00419480 | validation error =    1\n",
      "    Epoch  100 : loss = 0.00156728 | validation error =    2\n",
      "    Epoch  150 : loss = 0.00083427 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00045325 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00028339 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00022448 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00012414 | validation error =    1\n",
      "    Epoch  400 : loss = 0.27896153 | validation error =    4\n",
      "    Epoch  450 : loss = 0.01566274 | validation error =    3\n",
      "    Epoch  500 : loss = 0.80170540 | validation error =   14\n",
      "  Learning rate 0.0075 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 7.01214129 | validation error =  346\n",
      "    Epoch   50 : loss = 0.00408729 | validation error =    0\n",
      "    Epoch  100 : loss = 0.00138445 | validation error =    0\n",
      "    Epoch  150 : loss = 0.00061972 | validation error =    0\n",
      "    Epoch  200 : loss = 0.00041906 | validation error =    0\n",
      "    Epoch  250 : loss = 0.00026380 | validation error =    0\n",
      "    Epoch  300 : loss = 0.00013188 | validation error =    0\n",
      "    Epoch  350 : loss = 0.00017312 | validation error =    0\n",
      "    Epoch  400 : loss = 0.00008154 | validation error =    0\n",
      "    Epoch  450 : loss = 0.00013164 | validation error =    0\n",
      "    Epoch  500 : loss = 0.00005068 | validation error =    0\n",
      "  Learning rate 0.01 :\n",
      "-----------------------\n",
      "    Epoch    1 : loss = 6.91809332 | validation error =  306\n",
      "    Epoch   50 : loss = 0.00331867 | validation error =    1\n",
      "    Epoch  100 : loss = 0.00081139 | validation error =    1\n",
      "    Epoch  150 : loss = 0.00047012 | validation error =    1\n",
      "    Epoch  200 : loss = 0.00032575 | validation error =    1\n",
      "    Epoch  250 : loss = 0.00013112 | validation error =    1\n",
      "    Epoch  300 : loss = 0.00011913 | validation error =    1\n",
      "    Epoch  350 : loss = 0.00008636 | validation error =    1\n",
      "    Epoch  400 : loss = 0.00008441 | validation error =    1\n",
      "    Epoch  450 : loss = 0.00004268 | validation error =    1\n",
      "    Epoch  500 : loss = 0.00005698 | validation error =    1\n",
      "Done.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for m in models:\n",
    "    all_results, best_epoch, best_lr, best_error = find_best_params(m, Variable(train_input), Variable(train_target), Variable(validation_input), Variable(validation_target), learning_rates, nb_epochs_total, epoch_step, mini_batch_size)\n",
    "    print(\"\")\n",
    "    json_res = {\"model\":m().__class__.__name__,   \"learning_rates\":learning_rates, \"nb_epochs_total\":nb_epochs_total, \"epoch_step\":epoch_step, \"mini_batch_size\":mini_batch_size, \"best_epoch\":best_epoch, \"best_lr\":best_lr, \"best_error\":best_error, \"results\":all_results}\n",
    "    json.dump(json_res,open('results/adam/'+json_res[\"model\"]+'.json','w'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test results :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP\n",
      "For 25 epochs, with a learning rate of 0.001\n",
      "50.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model = nn.Sequential(\n",
    "            nn.Linear(1400, 140),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(140, 28),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(28, 2)\n",
    "        )\n",
    "json_res = json.load(open('results/adam/MLP.json','r'))\n",
    "print(\"MLP\")\n",
    "print(\"For\", json_res['best_epoch'], \"epochs, with a learning rate of\", json_res['best_lr'])\n",
    "train_model(model, Variable(train_input.view(train_input.size(0),-1)), Variable(train_target), json_res['mini_batch_size'], nb_epochs=json_res['best_epoch'], learning_rate=json_res['best_lr'], verbose=False)\n",
    "test_error = compute_nb_errors(model, Variable(test_input.view(test_input.size(0),-1)), Variable(test_target))\n",
    "print(100*(test_error/test_input.size(0)))\n",
    "json_res[\"test_error\"] = 100*(test_error/test_input.size(0))\n",
    "json.dump(json_res, open('results/adam/MLP.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Basic CNN\n",
      "For 20 epochs, with a learning rate of 0.0025\n",
      "32.0\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(1)\n",
    "model = Basic_CNN()\n",
    "json_res = json.load(open('results/adam/Basic_CNN.json','r'))\n",
    "print(\"Basic CNN\")\n",
    "print(\"For\", json_res['best_epoch'], \"epochs, with a learning rate of\", json_res['best_lr'])\n",
    "train_model(model, Variable(train_input.unsqueeze(1)), Variable(train_target), json_res['mini_batch_size'], nb_epochs=json_res['best_epoch'], learning_rate=json_res['best_lr'], verbose=False)\n",
    "test_error = compute_nb_errors(model, Variable(test_input.unsqueeze(1)), Variable(test_target))\n",
    "print(100*(test_error/test_input.size(0)))\n",
    "json_res[\"test_error\"] = 100*(test_error/test_input.size(0))\n",
    "json.dump(json_res, open('results/adam/Basic_CNN.json','w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN_1D\n",
      "For 20 epochs, with a learning rate of 0.01\n",
      "    iteration 1\n",
      "    iteration 2\n",
      "    iteration 3\n",
      "    iteration 4\n",
      "    iteration 5\n",
      "    iteration 6\n",
      "    iteration 7\n",
      "    iteration 8\n",
      "    iteration 9\n",
      "    iteration 10\n",
      "    iteration 11\n",
      "    iteration 12\n",
      "    iteration 13\n",
      "    iteration 14\n",
      "    iteration 15\n",
      "    iteration 16\n",
      "    iteration 17\n",
      "    iteration 18\n",
      "    iteration 19\n",
      "    iteration 20\n",
      "28.389999999999997\n",
      "==========\n",
      "CNN_dropout\n",
      "For 25 epochs, with a learning rate of 0.01\n",
      "    iteration 1\n",
      "    iteration 2\n",
      "    iteration 3\n",
      "    iteration 4\n",
      "    iteration 5\n",
      "    iteration 6\n",
      "    iteration 7\n",
      "    iteration 8\n",
      "    iteration 9\n",
      "    iteration 10\n",
      "    iteration 11\n",
      "    iteration 12\n",
      "    iteration 13\n",
      "    iteration 14\n",
      "    iteration 15\n",
      "    iteration 16\n",
      "    iteration 17\n",
      "    iteration 18\n",
      "    iteration 19\n",
      "    iteration 20\n",
      "31.855\n",
      "==========\n",
      "CNN_batchnorm\n",
      "For 35 epochs, with a learning rate of 0.0075\n",
      "    iteration 1\n",
      "    iteration 2\n",
      "    iteration 3\n",
      "    iteration 4\n",
      "    iteration 5\n",
      "    iteration 6\n",
      "    iteration 7\n",
      "    iteration 8\n",
      "    iteration 9\n",
      "    iteration 10\n",
      "    iteration 11\n",
      "    iteration 12\n",
      "    iteration 13\n",
      "    iteration 14\n",
      "    iteration 15\n",
      "    iteration 16\n",
      "    iteration 17\n",
      "    iteration 18\n",
      "    iteration 19\n",
      "    iteration 20\n",
      "25.369999999999997\n",
      "==========\n",
      "CNN_both\n",
      "For 30 epochs, with a learning rate of 0.0075\n",
      "    iteration 1\n",
      "    iteration 2\n",
      "    iteration 3\n",
      "    iteration 4\n",
      "    iteration 5\n",
      "    iteration 6\n",
      "    iteration 7\n",
      "    iteration 8\n",
      "    iteration 9\n",
      "    iteration 10\n",
      "    iteration 11\n",
      "    iteration 12\n",
      "    iteration 13\n",
      "    iteration 14\n",
      "    iteration 15\n",
      "    iteration 16\n",
      "    iteration 17\n",
      "    iteration 18\n",
      "    iteration 19\n",
      "    iteration 20\n",
      "26.56\n",
      "==========\n"
     ]
    }
   ],
   "source": [
    "error_rates = []\n",
    "for m in models:\n",
    "    torch.manual_seed(1)\n",
    "    json_res = json.load(open('results/adam/'+m().__class__.__name__+'.json','r'))\n",
    "    print(json_res[\"model\"])\n",
    "    print(\"For\", json_res['best_epoch'], \"epochs, with a learning rate of\", json_res['best_lr'])\n",
    "    acc_error = []\n",
    "    for i in range(20):\n",
    "        print(\"    iteration\", i+1)\n",
    "        model = m()\n",
    "        train_model(model, Variable(train_input), Variable(train_target), json_res['mini_batch_size'], nb_epochs=json_res['best_epoch'], learning_rate=json_res['best_lr'], verbose=False)\n",
    "        acc_error.append(compute_nb_errors(model, Variable(test_input), Variable(test_target)))\n",
    "    test_error = np.mean(acc_error)\n",
    "    print(100*(test_error/test_input.size(0)))\n",
    "    json_res[\"test_error\"] = 100*(test_error/test_input.size(0))\n",
    "    json.dump(json_res, open('results/adam/'+model.__class__.__name__+'.json','w'))\n",
    "    error_rates.append(100*(test_error/test_input.size(0)))\n",
    "    print(\"=\"*10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(1)\n",
    "\n",
    "colormap = 'Blues'\n",
    "normal_xticks = normal_results['nb_epochs']\n",
    "normal_yticks = normal_results['learning_rates']\n",
    "\n",
    "plt.subplot(221)\n",
    "plt.imshow(np.array(normal_results['results'][0]).T, cmap=colormap)\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(normal_xticks)), normal_xticks)\n",
    "plt.yticks(range(len(normal_yticks)), normal_yticks)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.title('Without dropout layer and batch size 20')\n",
    "\n",
    "plt.subplot(222)\n",
    "plt.imshow(np.array(normal_results['results'][1]).T, cmap=colormap)\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(normal_xticks)), normal_xticks)\n",
    "plt.yticks(range(len(normal_yticks)), normal_yticks)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.title('Without dropout layer and batch size 40')\n",
    "\n",
    "dropout_xticks = dropout_results['nb_epochs']\n",
    "dropout_yticks = dropout_results['learning_rates']\n",
    "\n",
    "plt.subplot(223)\n",
    "plt.imshow(dropout_results['results'][0], cmap=colormap)\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(dropout_xticks)), dropout_xticks)\n",
    "plt.yticks(range(len(dropout_yticks)), dropout_yticks)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.title('With dropout layer and batch size 20')\n",
    "\n",
    "plt.subplot(224)\n",
    "plt.imshow(dropout_results['results'][1], cmap=colormap)\n",
    "plt.xlabel('Nb of epochs')\n",
    "plt.ylabel('Learning rate')\n",
    "plt.xticks(range(len(dropout_xticks)), dropout_xticks)\n",
    "plt.yticks(range(len(dropout_yticks)), dropout_yticks)\n",
    "plt.colorbar(fraction=0.046, pad=0.04)\n",
    "plt.title('With dropout layer and batch size 40')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"results_1.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
