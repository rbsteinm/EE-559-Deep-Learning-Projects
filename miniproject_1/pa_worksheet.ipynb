{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlc_bci as bci\n",
    "from helpers import *\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input : <class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "Train target : <class 'torch.LongTensor'> torch.Size([316])\n",
      "Test input : <class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "Test target : <class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Subset of data sampled at 100Hz\n",
    "train_input , train_target = bci.load(root = './data_bci')\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.FloatTensor'> torch.Size([316, 28, 500])\n",
      "<class 'torch.LongTensor'> torch.Size([316])\n",
      "<class 'torch.FloatTensor'> torch.Size([100, 28, 500])\n",
      "<class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Full data sampled at 1Khz\n",
    "train_input , train_target = bci.load(root = './data_bci', one_khz = True)\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False, one_khz = True)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  30.8000   33.1000   37.9000  ...    76.9000   75.9000   72.3000\n",
       " -22.6000  -19.2000  -11.8000  ...    26.8000   22.9000   16.4000\n",
       "  11.2000   16.7000   26.4000  ...    64.1000   62.0000   55.9000\n",
       "             ...                â‹±                ...             \n",
       "   0.5000    0.5000    0.4000  ...    45.1000   46.2000   46.6000\n",
       "  11.0000   10.6000    9.8000  ...    41.7000   41.5000   40.9000\n",
       "  -9.9000  -10.0000  -10.3000  ...    32.9000   33.8000   32.8000\n",
       "[torch.FloatTensor of size 28x500]"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.LongTensor of size 316]"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# normalize mean to 0\n",
    "train_input.sub_(train_input.mean())\n",
    "test_input.sub_(test_input.mean())\n",
    "\n",
    "# normalize variance to 1\n",
    "train_input.div_(train_input.std())\n",
    "test_input.div_(test_input.std())\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_reg = LinearRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "linear_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "linear_pred_train = discrete_predictions(torch.FloatTensor(linear_reg.predict(train_input.view(train_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % train error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(linear_pred_train) - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_pred_test = discrete_predictions(torch.FloatTensor(linear_reg.predict(test_input.view(test_input.size(0),-1))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "27.0 % test error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(linear_pred_test) - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_reg = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='ovr', n_jobs=1,\n",
       "          penalty='l2', random_state=None, solver='liblinear', tol=0.0001,\n",
       "          verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 151,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logistic_reg.fit(train_input.view(train_input.size(0),-1),train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_pred_train = logistic_reg.predict(train_input.view(train_input.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % train error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(logistic_pred_train) - train_target).abs().sum()/train_input.size(0)),\"% train error\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logistic_pred_test = logistic_reg.predict(test_input.view(test_input.size(0),-1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.000000000000004 % test error\n"
     ]
    }
   ],
   "source": [
    "print(100*((torch.LongTensor(logistic_pred_test) - test_target).abs().sum()/test_input.size(0)),\"% test error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Neural Networks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#convert targets to one hot labels\n",
    "#train_target = convert_to_one_hot_labels(train_input, train_target)\n",
    "#test_target = convert_to_one_hot_labels(test_input, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic MLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MLP_big = nn.Sequential(\n",
    "        nn.Linear(14000, 1400),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(1400, 280),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(280, 28),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(28, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "MLP = nn.Sequential(\n",
    "        nn.Linear(1400, 280),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(280, 140),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(140, 28),\n",
    "        nn.Tanh(),\n",
    "        nn.Linear(28, 2)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 55.861808478832245\n",
      "Epoch 1 loss = 54.59076404571533\n",
      "Epoch 2 loss = 53.797380566596985\n",
      "Epoch 3 loss = 53.191094636917114\n",
      "Epoch 4 loss = 52.66807159781456\n",
      "Epoch 5 loss = 52.18098449707031\n",
      "Epoch 6 loss = 51.69751679897308\n",
      "Epoch 7 loss = 51.194580405950546\n",
      "Epoch 8 loss = 50.65688365697861\n",
      "Epoch 9 loss = 50.074628949165344\n",
      "Epoch 10 loss = 49.44200122356415\n",
      "Epoch 11 loss = 48.756370306015015\n",
      "Epoch 12 loss = 48.019282549619675\n",
      "Epoch 13 loss = 47.23957505822182\n",
      "Epoch 14 loss = 46.418880090117455\n",
      "Epoch 15 loss = 45.549498587846756\n",
      "Epoch 16 loss = 44.65137520432472\n",
      "Epoch 17 loss = 43.72418200969696\n",
      "Epoch 18 loss = 42.76705865561962\n",
      "Epoch 19 loss = 41.78091523051262\n",
      "Epoch 20 loss = 40.76593078672886\n",
      "Epoch 21 loss = 39.72083881497383\n",
      "Epoch 22 loss = 38.644645400345325\n",
      "Epoch 23 loss = 37.53784937411547\n",
      "Epoch 24 loss = 36.39436551183462\n",
      "Epoch 25 loss = 35.19167825952172\n",
      "Epoch 26 loss = 33.98767836764455\n",
      "Epoch 27 loss = 32.73168759420514\n",
      "Epoch 28 loss = 31.445499220862985\n",
      "Epoch 29 loss = 30.16045959852636\n",
      "Epoch 30 loss = 28.89358583278954\n",
      "Epoch 31 loss = 27.635679287835956\n",
      "Epoch 32 loss = 26.451356172561646\n",
      "Epoch 33 loss = 24.993193933740258\n",
      "Epoch 34 loss = 22.329513754695654\n",
      "Epoch 35 loss = 21.300788438413292\n",
      "Epoch 36 loss = 21.69328035786748\n",
      "Epoch 37 loss = 19.40192669071257\n",
      "Epoch 38 loss = 21.74633002281189\n",
      "Epoch 39 loss = 22.328185795107856\n",
      "Epoch 40 loss = 19.27624101890251\n",
      "Epoch 41 loss = 21.494215379934758\n",
      "Epoch 42 loss = 18.539624228607863\n",
      "Epoch 43 loss = 14.740391871193424\n",
      "Epoch 44 loss = 12.809250887483358\n",
      "Epoch 45 loss = 23.509267386049032\n",
      "Epoch 46 loss = 21.448826832696795\n",
      "Epoch 47 loss = 13.998251458164304\n",
      "Epoch 48 loss = 12.360645934939384\n",
      "Epoch 49 loss = 19.829397476278245\n",
      "Epoch 50 loss = 12.925528926309198\n",
      "Epoch 51 loss = 9.004871571436524\n",
      "Epoch 52 loss = 12.712735519977286\n",
      "Epoch 53 loss = 15.814191723242402\n",
      "Epoch 54 loss = 8.620287388330325\n",
      "Epoch 55 loss = 7.815711344475858\n",
      "Epoch 56 loss = 19.2838870389387\n",
      "Epoch 57 loss = 7.382501360960305\n",
      "Epoch 58 loss = 10.198833256727085\n",
      "Epoch 59 loss = 13.357606679695891\n",
      "Epoch 60 loss = 6.52746866340749\n",
      "Epoch 61 loss = 12.122200927638914\n",
      "Epoch 62 loss = 9.533251075306907\n",
      "Epoch 63 loss = 13.620927825162653\n",
      "Epoch 64 loss = 19.769558089785278\n",
      "Epoch 65 loss = 8.031806251266971\n",
      "Epoch 66 loss = 6.8842518598539755\n",
      "Epoch 67 loss = 6.117898587137461\n",
      "Epoch 68 loss = 5.8591331308707595\n",
      "Epoch 69 loss = 3.542390557762701\n",
      "Epoch 70 loss = 14.470347541675437\n",
      "Epoch 71 loss = 8.51938004483236\n",
      "Epoch 72 loss = 4.182998319272883\n",
      "Epoch 73 loss = 2.651830873102881\n",
      "Epoch 74 loss = 1.7161602322594263\n",
      "Epoch 75 loss = 16.01654776475334\n",
      "Epoch 76 loss = 5.953169251966756\n",
      "Epoch 77 loss = 2.997427246242296\n",
      "Epoch 78 loss = 9.255936907837167\n",
      "Epoch 79 loss = 14.021566861658357\n",
      "Epoch 80 loss = 7.792811411316507\n",
      "Epoch 81 loss = 2.29853199608624\n",
      "Epoch 82 loss = 3.015809479460586\n",
      "Epoch 83 loss = 1.5350954779423773\n",
      "Epoch 84 loss = 0.8139297832094599\n",
      "Epoch 85 loss = 7.46518522471888\n",
      "Epoch 86 loss = 18.022797207639087\n",
      "Epoch 87 loss = 16.096968480589567\n",
      "Epoch 88 loss = 4.842176103615202\n",
      "Epoch 89 loss = 11.231128733430523\n",
      "Epoch 90 loss = 6.885814970242791\n",
      "Epoch 91 loss = 3.763526126218494\n",
      "Epoch 92 loss = 4.0822326296474785\n",
      "Epoch 93 loss = 8.046630994882435\n",
      "Epoch 94 loss = 3.4680433393805288\n",
      "Epoch 95 loss = 1.4004087385255843\n",
      "Epoch 96 loss = 1.8394939813879319\n",
      "Epoch 97 loss = 0.8679366677533835\n",
      "Epoch 98 loss = 0.4531381952401716\n",
      "Epoch 99 loss = 0.32211697948514484\n"
     ]
    }
   ],
   "source": [
    "train_model(MLP, Variable(train_input.view(train_input.size(0),-1)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16.455696202531644 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_mlp = compute_nb_errors(MLP, Variable(test_input.view(test_input.size(0),-1)), Variable(train_target), 4)\n",
    "print(100*(train_error_mlp/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "28.999999999999996 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_mlp = compute_nb_errors(MLP, Variable(test_input.view(test_input.size(0),-1)), Variable(test_target), 4)\n",
    "print(100*(test_error_mlp/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic CNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Basic_CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Basic_CNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 32, kernel_size=5)\n",
    "        self.conv2 = nn.Conv2d(32, 64, kernel_size=5)\n",
    "        self.fc1 = nn.Linear(640, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=3, stride=3))\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=2, stride=2))\n",
    "        x = F.tanh(self.fc1(x.view(-1, 640)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn = Basic_CNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 56.96408760547638\n",
      "Epoch 1 loss = 55.346612334251404\n",
      "Epoch 2 loss = 54.624076545238495\n",
      "Epoch 3 loss = 54.15493232011795\n",
      "Epoch 4 loss = 53.71287339925766\n",
      "Epoch 5 loss = 53.25642216205597\n",
      "Epoch 6 loss = 52.768557608127594\n",
      "Epoch 7 loss = 52.23355823755264\n",
      "Epoch 8 loss = 51.66652154922485\n",
      "Epoch 9 loss = 51.05329883098602\n",
      "Epoch 10 loss = 50.3758901655674\n",
      "Epoch 11 loss = 49.651860892772675\n",
      "Epoch 12 loss = 48.8626429438591\n",
      "Epoch 13 loss = 48.01820057630539\n",
      "Epoch 14 loss = 47.1332391500473\n",
      "Epoch 15 loss = 46.1921281516552\n",
      "Epoch 16 loss = 45.22527086734772\n",
      "Epoch 17 loss = 44.226421266794205\n",
      "Epoch 18 loss = 43.21801918745041\n",
      "Epoch 19 loss = 42.19562944769859\n",
      "Epoch 20 loss = 41.18148720264435\n",
      "Epoch 21 loss = 40.16077479720116\n",
      "Epoch 22 loss = 39.14418172836304\n",
      "Epoch 23 loss = 38.14010426402092\n",
      "Epoch 24 loss = 37.14081862568855\n",
      "Epoch 25 loss = 36.13838002085686\n",
      "Epoch 26 loss = 35.16330511868\n",
      "Epoch 27 loss = 34.177098259329796\n",
      "Epoch 28 loss = 33.21918261051178\n",
      "Epoch 29 loss = 32.27527146041393\n",
      "Epoch 30 loss = 31.33050699532032\n",
      "Epoch 31 loss = 30.425929591059685\n",
      "Epoch 32 loss = 29.54372937977314\n",
      "Epoch 33 loss = 28.68027862906456\n",
      "Epoch 34 loss = 27.833763301372528\n",
      "Epoch 35 loss = 27.019295439124107\n",
      "Epoch 36 loss = 26.21853645145893\n",
      "Epoch 37 loss = 25.46456126868725\n",
      "Epoch 38 loss = 24.712548539042473\n",
      "Epoch 39 loss = 23.99293339252472\n",
      "Epoch 40 loss = 23.296063765883446\n",
      "Epoch 41 loss = 22.615831211209297\n",
      "Epoch 42 loss = 21.96356551349163\n",
      "Epoch 43 loss = 21.32805572450161\n",
      "Epoch 44 loss = 20.708868592977524\n",
      "Epoch 45 loss = 20.119311824440956\n",
      "Epoch 46 loss = 19.554574698209763\n",
      "Epoch 47 loss = 18.990614861249924\n",
      "Epoch 48 loss = 18.463000535964966\n",
      "Epoch 49 loss = 17.966503247618675\n",
      "Epoch 50 loss = 17.49929189682007\n",
      "Epoch 51 loss = 17.061291933059692\n",
      "Epoch 52 loss = 16.632286071777344\n",
      "Epoch 53 loss = 16.246908977627754\n",
      "Epoch 54 loss = 15.890002369880676\n",
      "Epoch 55 loss = 15.552879557013512\n",
      "Epoch 56 loss = 15.24713285267353\n",
      "Epoch 57 loss = 14.959998652338982\n",
      "Epoch 58 loss = 14.682705149054527\n",
      "Epoch 59 loss = 14.440124660730362\n",
      "Epoch 60 loss = 14.215530797839165\n",
      "Epoch 61 loss = 14.001857474446297\n",
      "Epoch 62 loss = 13.80361832678318\n",
      "Epoch 63 loss = 13.621444910764694\n",
      "Epoch 64 loss = 13.45180182158947\n",
      "Epoch 65 loss = 13.292763128876686\n",
      "Epoch 66 loss = 13.147643953561783\n",
      "Epoch 67 loss = 13.011489167809486\n",
      "Epoch 68 loss = 12.881368145346642\n",
      "Epoch 69 loss = 12.763640061020851\n",
      "Epoch 70 loss = 12.652457028627396\n",
      "Epoch 71 loss = 12.549559220671654\n",
      "Epoch 72 loss = 12.450643837451935\n",
      "Epoch 73 loss = 12.360494464635849\n",
      "Epoch 74 loss = 12.276227429509163\n",
      "Epoch 75 loss = 12.194484576582909\n",
      "Epoch 76 loss = 12.119019523262978\n",
      "Epoch 77 loss = 12.048991978168488\n",
      "Epoch 78 loss = 11.980261668562889\n",
      "Epoch 79 loss = 11.917058378458023\n",
      "Epoch 80 loss = 11.856265723705292\n",
      "Epoch 81 loss = 11.7990180850029\n",
      "Epoch 82 loss = 11.74495530128479\n",
      "Epoch 83 loss = 11.693664327263832\n",
      "Epoch 84 loss = 11.644655168056488\n",
      "Epoch 85 loss = 11.598375245928764\n",
      "Epoch 86 loss = 11.553494483232498\n",
      "Epoch 87 loss = 11.511443093419075\n",
      "Epoch 88 loss = 11.471571013331413\n",
      "Epoch 89 loss = 11.43324126303196\n",
      "Epoch 90 loss = 11.396637484431267\n",
      "Epoch 91 loss = 11.361045330762863\n",
      "Epoch 92 loss = 11.328348085284233\n",
      "Epoch 93 loss = 11.296270102262497\n",
      "Epoch 94 loss = 11.265329614281654\n",
      "Epoch 95 loss = 11.236226812005043\n",
      "Epoch 96 loss = 11.207529231905937\n",
      "Epoch 97 loss = 11.18064272403717\n",
      "Epoch 98 loss = 11.15477204322815\n",
      "Epoch 99 loss = 11.129919677972794\n"
     ]
    }
   ],
   "source": [
    "train_model(cnn, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn = compute_nb_errors(cnn, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "print(100*(train_error_cnn/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn = compute_nb_errors(cnn, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)\n",
    "print(100*(test_error_cnn/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CNN w 1D filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_1D_conv(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_1D_conv, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1,5))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1,3))\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(1,5))\n",
    "        self.fc1 = nn.Linear(896*2, 128)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=(1,2), stride=(1,2)))\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=(1,3), stride=(1,3)))\n",
    "        x = F.tanh(F.max_pool2d(self.conv3(x), kernel_size=(1,3), stride=(1,3)))\n",
    "        x = F.tanh(self.fc1(x.view(-1, 896*2)))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn2 = CNN_1D_conv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 55.29474902153015\n",
      "Epoch 1 loss = 55.05597680807114\n",
      "Epoch 2 loss = 54.855808675289154\n",
      "Epoch 3 loss = 54.682802975177765\n",
      "Epoch 4 loss = 54.524170219898224\n",
      "Epoch 5 loss = 54.372238516807556\n",
      "Epoch 6 loss = 54.2225626707077\n",
      "Epoch 7 loss = 54.07223069667816\n",
      "Epoch 8 loss = 53.92045372724533\n",
      "Epoch 9 loss = 53.7664595246315\n",
      "Epoch 10 loss = 53.6088582277298\n",
      "Epoch 11 loss = 53.44771748781204\n",
      "Epoch 12 loss = 53.28130882978439\n",
      "Epoch 13 loss = 53.10787773132324\n",
      "Epoch 14 loss = 52.924775540828705\n",
      "Epoch 15 loss = 52.73032104969025\n",
      "Epoch 16 loss = 52.52050358057022\n",
      "Epoch 17 loss = 52.2923142015934\n",
      "Epoch 18 loss = 52.04249811172485\n",
      "Epoch 19 loss = 51.76665839552879\n",
      "Epoch 20 loss = 51.46092280745506\n",
      "Epoch 21 loss = 51.12171161174774\n",
      "Epoch 22 loss = 50.74609687924385\n",
      "Epoch 23 loss = 50.327125549316406\n",
      "Epoch 24 loss = 49.860064417123795\n",
      "Epoch 25 loss = 49.33736914396286\n",
      "Epoch 26 loss = 48.7524134516716\n",
      "Epoch 27 loss = 48.09661191701889\n",
      "Epoch 28 loss = 47.363799035549164\n",
      "Epoch 29 loss = 46.548118352890015\n",
      "Epoch 30 loss = 45.64110191166401\n",
      "Epoch 31 loss = 44.62833517789841\n",
      "Epoch 32 loss = 43.5095302015543\n",
      "Epoch 33 loss = 42.288945361971855\n",
      "Epoch 34 loss = 40.98556010425091\n",
      "Epoch 35 loss = 39.624012991786\n",
      "Epoch 36 loss = 38.224144004285336\n",
      "Epoch 37 loss = 36.83618910610676\n",
      "Epoch 38 loss = 35.50140971690416\n",
      "Epoch 39 loss = 34.220379054546356\n",
      "Epoch 40 loss = 32.987858686596155\n",
      "Epoch 41 loss = 31.78924137726426\n",
      "Epoch 42 loss = 30.60187002643943\n",
      "Epoch 43 loss = 29.414099875837564\n",
      "Epoch 44 loss = 28.20890563353896\n",
      "Epoch 45 loss = 26.9710533618927\n",
      "Epoch 46 loss = 25.724096946418285\n",
      "Epoch 47 loss = 24.45225503668189\n",
      "Epoch 48 loss = 23.141171157360077\n",
      "Epoch 49 loss = 21.80590709671378\n",
      "Epoch 50 loss = 20.482658119872212\n",
      "Epoch 51 loss = 19.184796921908855\n",
      "Epoch 52 loss = 17.91424261406064\n",
      "Epoch 53 loss = 16.68403071537614\n",
      "Epoch 54 loss = 15.473992392420769\n",
      "Epoch 55 loss = 14.307200957089663\n",
      "Epoch 56 loss = 13.158826867118478\n",
      "Epoch 57 loss = 12.065468588843942\n",
      "Epoch 58 loss = 11.004322789609432\n",
      "Epoch 59 loss = 10.01800771523267\n",
      "Epoch 60 loss = 9.092921589501202\n",
      "Epoch 61 loss = 8.246936591342092\n",
      "Epoch 62 loss = 7.473065987229347\n",
      "Epoch 63 loss = 6.765459856018424\n",
      "Epoch 64 loss = 6.132833236362785\n",
      "Epoch 65 loss = 5.56078403769061\n",
      "Epoch 66 loss = 5.051177802029997\n",
      "Epoch 67 loss = 4.5967433960177\n",
      "Epoch 68 loss = 4.186151412315667\n",
      "Epoch 69 loss = 3.8177803866565228\n",
      "Epoch 70 loss = 3.485889774048701\n",
      "Epoch 71 loss = 3.189264864195138\n",
      "Epoch 72 loss = 2.920124291209504\n",
      "Epoch 73 loss = 2.6755923714954406\n",
      "Epoch 74 loss = 2.4577202387154102\n",
      "Epoch 75 loss = 2.259887665626593\n",
      "Epoch 76 loss = 2.082024419447407\n",
      "Epoch 77 loss = 1.9214594420045614\n",
      "Epoch 78 loss = 1.7766964577604085\n",
      "Epoch 79 loss = 1.6465725870220922\n",
      "Epoch 80 loss = 1.5289256813121028\n",
      "Epoch 81 loss = 1.422422197414562\n",
      "Epoch 82 loss = 1.3253437340026721\n",
      "Epoch 83 loss = 1.238339542527683\n",
      "Epoch 84 loss = 1.1580220417818055\n",
      "Epoch 85 loss = 1.085460348345805\n",
      "Epoch 86 loss = 1.019526947668055\n",
      "Epoch 87 loss = 0.9589260969660245\n",
      "Epoch 88 loss = 0.9038629011774901\n",
      "Epoch 89 loss = 0.8529104291810654\n",
      "Epoch 90 loss = 0.8064452380058356\n",
      "Epoch 91 loss = 0.7638936454895884\n",
      "Epoch 92 loss = 0.7245852873311378\n",
      "Epoch 93 loss = 0.6883455333882011\n",
      "Epoch 94 loss = 0.6548019158944953\n",
      "Epoch 95 loss = 0.6237556178239174\n",
      "Epoch 96 loss = 0.5950728290190455\n",
      "Epoch 97 loss = 0.5683711224410217\n",
      "Epoch 98 loss = 0.5436698693956714\n",
      "Epoch 99 loss = 0.5205931734817568\n"
     ]
    }
   ],
   "source": [
    "train_model(cnn2, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn2 = compute_nb_errors(cnn2, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "print(100*(train_error_cnn2/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.0 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn2 = compute_nb_errors(cnn2, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)\n",
    "print(100*(test_error_cnn2/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### previous + dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CNN_dropout(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(CNN_dropout, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 16, kernel_size=(1,5))\n",
    "        self.conv2 = nn.Conv2d(16, 32, kernel_size=(1,3))\n",
    "        self.conv3 = nn.Conv2d(32, 64, kernel_size=(1,5))\n",
    "        self.fc1 = nn.Linear(896*2, 128)\n",
    "        self.dropout = nn.Dropout(0.5)\n",
    "        self.fc2 = nn.Linear(128, 2)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.tanh(F.max_pool2d(self.conv1(x), kernel_size=(1,2), stride=(1,2)))\n",
    "        x = F.tanh(F.max_pool2d(self.conv2(x), kernel_size=(1,3), stride=(1,3)))\n",
    "        x = F.tanh(F.max_pool2d(self.conv3(x), kernel_size=(1,3), stride=(1,3)))\n",
    "        x = F.tanh(self.fc1(x.view(-1, 896*2))) \n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cnn3 = CNN_dropout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 loss = 56.019174098968506\n",
      "Epoch 1 loss = 55.4520201086998\n",
      "Epoch 2 loss = 54.7875514626503\n",
      "Epoch 3 loss = 55.15742391347885\n",
      "Epoch 4 loss = 54.50948143005371\n",
      "Epoch 5 loss = 54.30815577507019\n",
      "Epoch 6 loss = 53.86229258775711\n",
      "Epoch 7 loss = 53.462171137332916\n",
      "Epoch 8 loss = 53.096127450466156\n",
      "Epoch 9 loss = 53.54497683048248\n",
      "Epoch 10 loss = 53.712387442588806\n",
      "Epoch 11 loss = 53.26724320650101\n",
      "Epoch 12 loss = 52.391912400722504\n",
      "Epoch 13 loss = 52.46445178985596\n",
      "Epoch 14 loss = 52.34692305326462\n",
      "Epoch 15 loss = 51.58625024557114\n",
      "Epoch 16 loss = 52.29400381445885\n",
      "Epoch 17 loss = 51.25133517384529\n",
      "Epoch 18 loss = 51.64257895946503\n",
      "Epoch 19 loss = 50.183215379714966\n",
      "Epoch 20 loss = 49.77911111712456\n",
      "Epoch 21 loss = 49.276940166950226\n",
      "Epoch 22 loss = 48.689226537942886\n",
      "Epoch 23 loss = 48.142345398664474\n",
      "Epoch 24 loss = 46.38583126664162\n",
      "Epoch 25 loss = 46.18621206283569\n",
      "Epoch 26 loss = 44.67905259132385\n",
      "Epoch 27 loss = 44.67619952559471\n",
      "Epoch 28 loss = 43.307352751493454\n",
      "Epoch 29 loss = 41.84569478034973\n",
      "Epoch 30 loss = 40.8155280649662\n",
      "Epoch 31 loss = 39.68565061688423\n",
      "Epoch 32 loss = 37.75842548906803\n",
      "Epoch 33 loss = 37.33132617175579\n",
      "Epoch 34 loss = 37.62675406038761\n",
      "Epoch 35 loss = 35.663996919989586\n",
      "Epoch 36 loss = 34.00155533105135\n",
      "Epoch 37 loss = 33.99162730574608\n",
      "Epoch 38 loss = 31.923409018665552\n",
      "Epoch 39 loss = 31.01172076910734\n",
      "Epoch 40 loss = 30.645974464714527\n",
      "Epoch 41 loss = 29.25132404267788\n",
      "Epoch 42 loss = 28.108241971582174\n",
      "Epoch 43 loss = 26.433338809758425\n",
      "Epoch 44 loss = 26.899379517883062\n",
      "Epoch 45 loss = 24.882782755419612\n",
      "Epoch 46 loss = 23.410790540277958\n",
      "Epoch 47 loss = 22.529521146789193\n",
      "Epoch 48 loss = 20.49806509166956\n",
      "Epoch 49 loss = 20.221469840034842\n",
      "Epoch 50 loss = 18.527499178424478\n",
      "Epoch 51 loss = 17.405429707840085\n",
      "Epoch 52 loss = 16.13313283212483\n",
      "Epoch 53 loss = 15.333481089212\n",
      "Epoch 54 loss = 13.355516636744142\n",
      "Epoch 55 loss = 12.697945938445628\n",
      "Epoch 56 loss = 11.323693618178368\n",
      "Epoch 57 loss = 9.713399903150275\n",
      "Epoch 58 loss = 8.921685569453984\n",
      "Epoch 59 loss = 7.931075048400089\n",
      "Epoch 60 loss = 7.501569263637066\n",
      "Epoch 61 loss = 6.355410736985505\n",
      "Epoch 62 loss = 6.108370112837292\n",
      "Epoch 63 loss = 6.169687063898891\n",
      "Epoch 64 loss = 4.7262023482471704\n",
      "Epoch 65 loss = 4.435507771326229\n",
      "Epoch 66 loss = 4.1595508572645485\n",
      "Epoch 67 loss = 3.9054378874134272\n",
      "Epoch 68 loss = 3.381715140771121\n",
      "Epoch 69 loss = 2.8300233997870237\n",
      "Epoch 70 loss = 2.7562885247170925\n",
      "Epoch 71 loss = 2.200457281200215\n",
      "Epoch 72 loss = 2.47085431474261\n",
      "Epoch 73 loss = 2.2200505650253035\n",
      "Epoch 74 loss = 1.8896751843276434\n",
      "Epoch 75 loss = 1.870049441291485\n",
      "Epoch 76 loss = 1.8865614650421776\n",
      "Epoch 77 loss = 1.4823870467371307\n",
      "Epoch 78 loss = 1.401042217068607\n",
      "Epoch 79 loss = 1.3153794993413612\n",
      "Epoch 80 loss = 1.2501118758809753\n",
      "Epoch 81 loss = 1.0815581344941165\n",
      "Epoch 82 loss = 0.973528012400493\n",
      "Epoch 83 loss = 1.016819091833895\n",
      "Epoch 84 loss = 0.9837655130540952\n",
      "Epoch 85 loss = 0.7771348246315029\n",
      "Epoch 86 loss = 0.7330730964313261\n",
      "Epoch 87 loss = 0.7888818851206452\n",
      "Epoch 88 loss = 0.7571851371030789\n",
      "Epoch 89 loss = 0.6834343874797923\n",
      "Epoch 90 loss = 0.711828566971235\n",
      "Epoch 91 loss = 0.6777879584406037\n",
      "Epoch 92 loss = 0.5425377607461996\n",
      "Epoch 93 loss = 0.6252292149874847\n",
      "Epoch 94 loss = 0.5302970449702116\n",
      "Epoch 95 loss = 0.5613234615375404\n",
      "Epoch 96 loss = 0.4390529082593275\n",
      "Epoch 97 loss = 0.4941098853305448\n",
      "Epoch 98 loss = 0.42522903963254066\n",
      "Epoch 99 loss = 0.48450717332889326\n"
     ]
    }
   ],
   "source": [
    "train_model(cnn3, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0 % training error\n"
     ]
    }
   ],
   "source": [
    "train_error_cnn3 = compute_nb_errors(cnn3, Variable(train_input.view(-1, 1, 28, 50)), Variable(train_target), 4)\n",
    "print(100*(train_error_cnn3/train_input.size(0)),'% training error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "26.0 % test error\n"
     ]
    }
   ],
   "source": [
    "test_error_cnn3 = compute_nb_errors(cnn3, Variable(test_input.view(-1, 1, 28, 50)), Variable(test_target), 4)\n",
    "print(100*(test_error_cnn3/test_input.size(0)),'% test error')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
