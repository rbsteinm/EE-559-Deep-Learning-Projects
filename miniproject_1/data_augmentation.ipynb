{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "from torch.autograd import Variable\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "import dlc_bci as bci\n",
    "from helpers import *\n",
    "\n",
    "torch.manual_seed(1)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input : <class 'torch.FloatTensor'> torch.Size([316, 28, 50])\n",
      "Train target : <class 'torch.LongTensor'> torch.Size([316])\n",
      "Test input : <class 'torch.FloatTensor'> torch.Size([100, 28, 50])\n",
      "Test target : <class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Subset of data sampled at 100Hz\n",
    "train_input , train_target = bci.load(root = './data_bci')\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train input : <class 'torch.FloatTensor'> torch.Size([316, 28, 500])\n",
      "Train target : <class 'torch.LongTensor'> torch.Size([316])\n",
      "Test input : <class 'torch.FloatTensor'> torch.Size([100, 28, 500])\n",
      "Test target : <class 'torch.LongTensor'> torch.Size([100])\n"
     ]
    }
   ],
   "source": [
    "# Full data sampled at 1Khz\n",
    "train_input , train_target = bci.load(root = './data_bci', one_khz = True)\n",
    "print(\"Train input :\", str(type(train_input)), train_input.size()) \n",
    "print(\"Train target :\", str(type(train_target)), train_target.size())\n",
    "test_input , test_target = bci.load(root = './data_bci', train = False, one_khz = True)\n",
    "print(\"Test input :\", str(type(test_input)), test_input.size()) \n",
    "print(\"Test target :\", str(type(test_target)), test_target.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  30.8000   33.1000   37.9000  ...    76.9000   75.9000   72.3000\n",
       " -22.6000  -19.2000  -11.8000  ...    26.8000   22.9000   16.4000\n",
       "  11.2000   16.7000   26.4000  ...    64.1000   62.0000   55.9000\n",
       "             ...                ⋱                ...             \n",
       "   0.5000    0.5000    0.4000  ...    45.1000   46.2000   46.6000\n",
       "  11.0000   10.6000    9.8000  ...    41.7000   41.5000   40.9000\n",
       "  -9.9000  -10.0000  -10.3000  ...    32.9000   33.8000   32.8000\n",
       "[torch.FloatTensor of size 28x500]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 1\n",
       " 0\n",
       " 1\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       " 0\n",
       "[torch.LongTensor of size 316]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# normalize mean to 0\n",
    "train_input.sub_(train_input.mean())\n",
    "test_input.sub_(test_input.mean())\n",
    "\n",
    "# normalize variance to 1\n",
    "train_input.div_(train_input.std())\n",
    "test_input.div_(test_input.std())\n",
    "\n",
    "print('done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "#total = train_input.new(3160,28,50)\n",
    "seq = []\n",
    "for i in range(10):\n",
    "    seq.append(train_input[:,:,i::10])\n",
    "total = torch.cat(seq)'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "( 0  ,.,.) = \n",
       "   30.8000   45.4000   28.2000  ...    81.2000   59.7000   78.5000\n",
       "  -22.6000   -0.7000  -24.7000  ...    31.5000    2.1000   34.2000\n",
       "   11.2000   45.5000    9.7000  ...    79.5000   36.1000   86.4000\n",
       "              ...                ⋱                ...             \n",
       "    0.5000    4.2000    9.2000  ...    38.6000   38.4000   36.1000\n",
       "   11.0000   18.1000   31.3000  ...    53.6000   53.6000   43.7000\n",
       "   -9.9000   -1.3000    7.3000  ...    37.0000   37.7000   28.8000\n",
       "\n",
       "( 1  ,.,.) = \n",
       "  135.7000  134.8000  145.7000  ...    77.8000   91.4000   85.1000\n",
       "   16.9000    0.7000   19.3000  ...   -40.3000  -12.2000  -33.5000\n",
       "   23.9000    9.1000   23.7000  ...   -17.9000    7.1000  -13.2000\n",
       "              ...                ⋱                ...             \n",
       "  -18.8000  -16.7000  -28.1000  ...   -30.9000  -20.1000  -15.5000\n",
       "    6.1000    5.8000   -4.8000  ...    -6.7000    9.6000   19.0000\n",
       "  -27.5000  -18.5000  -30.1000  ...   -44.0000  -32.3000  -17.8000\n",
       "\n",
       "( 2  ,.,.) = \n",
       "   69.4000   57.0000   76.4000  ...    55.6000   67.7000   59.0000\n",
       "   13.2000  -16.0000   19.4000  ...    -9.5000   15.3000   -8.9000\n",
       "   63.4000   21.7000   67.6000  ...    27.8000   63.9000   29.1000\n",
       "              ...                ⋱                ...             \n",
       "    3.2000   15.6000   16.3000  ...    12.2000    0.4000   20.1000\n",
       "    5.2000   34.4000   33.6000  ...    39.5000   17.1000   24.6000\n",
       "  -19.9000    6.8000   22.2000  ...    15.9000    2.2000   22.3000\n",
       " ... \n",
       "\n",
       "(3157,.,.) = \n",
       "   89.8000   83.2000   71.8000  ...    69.6000   67.7000   70.4000\n",
       "   11.9000    4.3000   -2.9000  ...   -10.3000  -11.4000  -10.7000\n",
       "   53.5000   60.9000   40.4000  ...    45.8000   28.7000   43.0000\n",
       "              ...                ⋱                ...             \n",
       "   19.1000   10.5000   -6.3000  ...     2.5000   -5.6000   -1.5000\n",
       "   20.7000   -3.8000  -24.1000  ...    25.4000   21.7000   21.6000\n",
       "   12.7000   -8.8000  -35.5000  ...    13.6000    4.5000    7.5000\n",
       "\n",
       "(3158,.,.) = \n",
       "   66.1000   90.7000   84.2000  ...    70.6000   54.8000   63.7000\n",
       "   11.7000   44.3000   35.7000  ...    19.9000   -0.2000   14.3000\n",
       "   69.2000  106.3000   89.3000  ...    84.8000   53.4000   79.1000\n",
       "              ...                ⋱                ...             \n",
       "   30.7000   39.4000   37.6000  ...    19.6000   15.0000   12.5000\n",
       "   40.8000   43.5000   27.5000  ...    32.5000   28.7000   23.9000\n",
       "   27.1000   30.8000   18.2000  ...    20.8000   19.6000   10.7000\n",
       "\n",
       "(3159,.,.) = \n",
       "   66.1000   82.9000   71.7000  ...    84.7000   70.1000   83.5000\n",
       "   16.0000   48.3000   14.1000  ...    44.4000   15.6000   40.5000\n",
       "   88.3000  123.0000   82.7000  ...   123.2000   84.4000  116.6000\n",
       "              ...                ⋱                ...             \n",
       "   86.0000   83.5000   90.6000  ...    80.8000   90.6000   86.5000\n",
       "  102.8000  106.2000  113.8000  ...    87.6000   97.3000   91.5000\n",
       "  102.2000  100.6000  112.7000  ...    75.5000   96.7000  102.2000\n",
       "[torch.FloatTensor of size 3160x28x50]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       " 135.7000  134.8000  145.7000  ...    77.8000   91.4000   85.1000\n",
       "  16.9000    0.7000   19.3000  ...   -40.3000  -12.2000  -33.5000\n",
       "  23.9000    9.1000   23.7000  ...   -17.9000    7.1000  -13.2000\n",
       "             ...                ⋱                ...             \n",
       " -18.8000  -16.7000  -28.1000  ...   -30.9000  -20.1000  -15.5000\n",
       "   6.1000    5.8000   -4.8000  ...    -6.7000    9.6000   19.0000\n",
       " -27.5000  -18.5000  -30.1000  ...   -44.0000  -32.3000  -17.8000\n",
       "[torch.FloatTensor of size 28x50]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[1,:,::10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\n",
       "  41.8000\n",
       " -10.3000\n",
       "  38.1000\n",
       "  27.9000\n",
       "  13.2000\n",
       "   3.9000\n",
       "   3.2000\n",
       "  -5.8000\n",
       "  49.0000\n",
       "  59.0000\n",
       " 101.8000\n",
       " -17.1000\n",
       "  -9.1000\n",
       "  -3.5000\n",
       "  16.9000\n",
       "  13.9000\n",
       "  -0.9000\n",
       "  -4.6000\n",
       "   0.4000\n",
       "  36.9000\n",
       "  10.1000\n",
       "  24.5000\n",
       "  34.6000\n",
       "  63.4000\n",
       "  30.3000\n",
       "   7.9000\n",
       "  19.2000\n",
       "  -0.4000\n",
       "[torch.FloatTensor of size 28]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_input[0,:,6]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
